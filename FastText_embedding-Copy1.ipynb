{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fasttext_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "r_m00tKKkKNG"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahandv/science_science/blob/master/FastText_embedding-Copy1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B43KQKxsZqK2"
      },
      "source": [
        "# FASTTEXT EMBEDDING\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyakRMAwbUq2"
      },
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8klZfiWyEt98"
      },
      "source": [
        "Local OR Colab?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsEwsoJSEtKx"
      },
      "source": [
        "# datapath = '/mnt/16A4A9BCA4A99EAD/GoogleDrive/Data/' # Local\n",
        "datapath = 'drive/My Drive/Data/' # Remote"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIPuXPla6BKh"
      },
      "source": [
        "### Clone Project Git Repo"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = datapath+'Corpus/cora-classify/cora/clean/with citations/'\n",
    "file_name = 'cora deflemm'#corpus abstract-title - with n-grams'\n",
    "corpus = pd.read_csv(directory+file_name,names=['abstracts'])\n",
    "corpus_tokens = [item.lower() for sublist in corpus['abstracts'].values.tolist() for item in sublist.split()]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reference c delobel r g casey decomposition da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>object fault handling persistent programming l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distributed object management thor thor object...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>opportunistic log efficient installation read ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>performance study alternative object faulting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23139</th>\n",
       "      <td>quality service negotiation robotics environme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23140</th>\n",
       "      <td>learning abduction investigate abduction induc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23141</th>\n",
       "      <td>abstract spreadsheet comprise language metapho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23142</th>\n",
       "      <td>semigroups satisfying n n summarize recent res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23143</th>\n",
       "      <td>certification syllabus programming language co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23144 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abstracts\n",
       "0      reference c delobel r g casey decomposition da...\n",
       "1      object fault handling persistent programming l...\n",
       "2      distributed object management thor thor object...\n",
       "3      opportunistic log efficient installation read ...\n",
       "4      performance study alternative object faulting ...\n",
       "...                                                  ...\n",
       "23139  quality service negotiation robotics environme...\n",
       "23140  learning abduction investigate abduction induc...\n",
       "23141  abstract spreadsheet comprise language metapho...\n",
       "23142  semigroups satisfying n n summarize recent res...\n",
       "23143  certification syllabus programming language co...\n",
       "\n",
       "[23144 rows x 1 columns]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8crXh0Ek1FA"
      },
      "source": [
        "### Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulliMrpjkKAd",
        "outputId": "946c857b-880f-4bed-ca7e-176f57f53722"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhV7OflrnOhI"
      },
      "source": [
        "# Check files!\n",
        "!ls 'drive/My Drive/Data-Permenant/FastText-crawl-300d-2M-subword'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiBuMInGmypx"
      },
      "source": [
        "### Install requirements"
      ]
    },
    "colab_type": "code",
    "id": "F4UJAF3exSLm",
    "outputId": "65624796-56fb-468e-b923-3ff5468369f9"
   },
   "outputs": [],
   "source": [
    "# Save in a dict\n",
    "comment_embedding = 'average_manual '\n",
    "output_dict = {}\n",
    "for token in tqdm(corpus_tokens[:],total=len(corpus_tokens[:])):\n",
    "    token_split = token.split(' ')\n",
    "    if len(token_split) > 0:\n",
    "        tmp_vector_grams = []\n",
    "        for item in token_split:\n",
    "            tmp_vector_grams.append(model.get_word_vector(item))\n",
    "        output_dict[token] = str(list(np.array(tmp_vector_grams).mean(axis=0)))\n",
    "    else:\n",
    "        output_dict[token] = str(model.get_word_vector(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8nMUTU0xXo8"
   },
   "source": [
    "##### Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJk-lqjemSky"
   },
   "outputs": [],
   "source": [
    "# Save embeddings to disk\n",
    "with open(directory+'vectors/100D/FastText vector '+comment_embedding+period+'.json', 'w') as json_file:\n",
    "    json.dump(output_dict, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXkwEnj4lnJi"
   },
   "source": [
    "#### Get embeddings (alternative) : \n",
    "\n",
    "*   save to a list instead of a dicktionary and csv\n",
    "*   Will have many redundant words in it and will take lots of disk space\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GndJ3Q1OlnSy"
   },
   "outputs": [],
   "source": [
    "# Save in a list\n",
    "batches = 1000\n",
    "batch_size = len(corpus_tokens)/batches\n",
    "\n",
    "for step in tqdm(range(batches),total=batches):\n",
    "    batch_tokens = corpus_tokens[int(step*batch_size):int((step+1)*batch_size)]\n",
    "    corpus_vectors = [model.get_word_vector(x) for x in batch_tokens]\n",
    "    corpus_vectors = pd.DataFrame(corpus_vectors)\n",
    "    corpus_vectors['tokens'] = batch_tokens\n",
    "\n",
    "    # Save embeddings to disk\n",
    "    with open(directory+'vector '+period,'a') as f:\n",
    "        corpus_vectors.to_csv(f,index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "38Xcww-Mfdaj"
   },
   "source": [
    "## Gensim Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lPd9g4E4fgTi"
   },
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06depR4llpmB"
   },
   "source": [
    "Load gensim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j_UYlrcCfgxw"
   },
   "outputs": [],
   "source": [
    "gensim_model_address = datapath+'FastText Models/50D w1/fasttext-scopus-2.2-million_docs-gensim 50D-w1.model'\n",
    "model = fasttext_gensim.load(gensim_model_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gx1x_twuQCbQ"
   },
   "outputs": [],
   "source": [
    "gensim_model_address = datapath+\"Corpus/cora-classify/cora/models/fasttext cora_wos corpus 128d5w.model\"\n",
    "model = fasttext_gensim.load(gensim_model_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q18gRbrW3mvO"
   },
   "outputs": [],
   "source": [
    "gensim_model_address = 'drive/My Drive/Data-Permenant/FastText-crawl-300d-2M-subword/crawl-300d-2M-subword.bin'\n",
    "model = gensim.models.fasttext.load_facebook_model(gensim_model_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XhKlmoX6gPE-"
   },
   "source": [
    "Simple Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7oeEkj_gNLg"
   },
   "outputs": [],
   "source": [
    "print('intelligence' in model.wv.vocab)\n",
    "print(model.similarity(\"anns\", \"ann\"))\n",
    "print(model.most_similar(['eye','vision','processing'], ['computer']))\n",
    "print(model.wmdistance(['stop', 'word', 'removed', 'tokens', 'of', 'sentence 1'], ['stop word removed tokens of sentence 2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9l5YuBnkKFB"
      },
      "source": [
        "### Import Libs"
      ]
    },
    "colab_type": "code",
    "id": "iPioY7Lj7BOP",
    "outputId": "8f170cbd-4949-4325-cb12-26ecd7164c64"
   },
   "outputs": [],
   "source": [
    "model.wv['artificial intelligence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KqUSjLRy_isH"
   },
   "outputs": [],
   "source": [
    "(model.wv['artificial']+model.wv['intelligence'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0gnOxud_m4H"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.23106062, -0.29910162,  1.611154  ,  0.12580949,  0.45087183,\n",
       "        0.34442022, -0.92528075, -0.8388469 , -0.38186234,  0.01019396,\n",
       "       -0.80969816, -0.8401889 , -0.3981349 ,  0.73403674, -0.7535211 ,\n",
       "       -0.9932593 , -1.2097434 , -0.41978228, -0.25359127, -0.11182665,\n",
       "        0.04050311, -0.48496786, -0.5888825 , -0.2600431 , -0.08715916,\n",
       "        0.18177651,  0.13031416, -0.15251486, -0.80820686,  0.17925328,\n",
       "        0.6315884 ,  0.2023247 , -0.4096204 , -0.69134235,  0.14907089,\n",
       "        0.10458857, -0.51310533, -0.41936776,  0.7343195 , -1.1371127 ,\n",
       "        0.609631  , -0.712976  , -0.33220175, -0.15955056,  1.2148736 ,\n",
       "       -0.5492057 , -0.6402487 ,  1.1931036 , -0.31400204,  0.70540744,\n",
       "       -0.528428  , -0.14993478,  0.10264428,  0.70766616, -0.6580092 ,\n",
       "        0.40380043,  0.2651006 ,  1.0165303 , -0.22381835, -0.6244725 ,\n",
       "        0.09580757,  0.45569608, -0.28528696, -0.64159805, -0.20678103,\n",
       "        0.09387368,  0.35451886, -0.2465916 ,  0.0457297 ,  0.09376474,\n",
       "       -0.21294098, -0.47366217,  0.00881729,  0.68307954, -0.20610492,\n",
       "        0.37799373,  0.04772854, -0.2737389 ,  1.0039018 , -0.8023533 ,\n",
       "       -0.53350747,  0.53882676, -0.35841626, -1.2222173 , -0.00815003,\n",
       "        0.56307334, -1.1143996 ,  0.01141043, -0.10342392, -0.5954121 ,\n",
       "       -0.49160394,  0.3932491 ,  0.16835298, -0.11662421, -0.7661322 ,\n",
       "       -0.19622982, -1.6255851 ,  0.7035168 , -0.7768182 ,  0.0819238 ,\n",
       "       -0.17577833, -1.1775433 ,  0.58588725,  0.38260233,  0.01278142,\n",
       "       -1.0441288 ,  1.0680994 ,  0.3085898 , -0.7702155 ,  0.89465636,\n",
       "        0.5670864 ,  1.3593963 ,  0.8541214 ,  0.48747632,  0.9237076 ,\n",
       "       -0.5230352 ,  0.16016464,  0.27127472,  0.2697066 ,  0.05400416,\n",
       "        0.90238357,  0.74114174,  0.46529204, -0.07612032,  0.61718696,\n",
       "        1.0333123 ,  1.0007285 , -0.31818622], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['artificial_intelligence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5516KiWbelyF"
   },
   "source": [
    "#### Compare vectors to ACM categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AxNRKUidrCk"
   },
   "outputs": [],
   "source": [
    "AI_categories = [\n",
    "              'Artificial Intelligence Applications - Expert Systems',\n",
    "              'Automatic Programming',\n",
    "              'Deduction - Theorem Proving',\n",
    "              'Knowledge Representation Formalisms - Knowledge Representation Methods',\n",
    "              'Programming Languages - Software',\n",
    "              'Learning',\n",
    "              'Natural Language Processing',\n",
    "              'Problem Solving - Control Methods - Search',\n",
    "              'Robotics',\n",
    "              'Vision - Scene Understanding',\n",
    "              'Distributed Artificial Intelligence',\n",
    "              'ARTIFICIAL INTELLIGENCE'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-QeWKP-k1qpy"
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "              'Natural language processing - Information extraction - Machine translation - Discourse, dialogue  pragmatics - Natural language generation - Speech recognition - Lexical semantics - Phonology / morphology',\n",
    "              'Knowledge representation reasoning - Description logics - Semantic networks Nonmonotonic default reasoning  belief revision - Probabilistic reasoning - Vagueness fuzzy logic - Causal reasoning  diagnostics - Temporal reasoning - Cognitive robotics - Ontology engineering - Logic programming answer set programming - Spatial  physical reasoning - Reasoning about belief  knowledge',\n",
    "              'Planning  scheduling - Planning for deterministic actions - Planning under uncertainty - Multi-agent planning - Planning  abstraction  generalization - Robotic planning - Evolutionary robotics',\n",
    "              'Search methodologies - Heuristic function construction - Discrete space search - Continuous space search - Randomized search - Game tree search - Abstraction  micro-operators - Search with partial observations - ',\n",
    "              'Control methods - Robotic planning - Evolutionary robotics - Computational control theory - Motion path planning',\n",
    "              'Philosophical theoretical foundations artificial intelligence - Cognitive science - Theory mind',\n",
    "              'Distributed artificial intelligence - Multi agent systems - Intelligent agents - Mobile agents - Cooperation  coordination',\n",
    "              'Computer vision - Biometrics - Scene understanding - Activity recognition  understanding - Video summarization - Visual content based indexing  retrieval - Visual inspection - Vision for robotics - Scene anomaly detection - Image  video acquisition - Camera calibration - Epipolar geometry - Computational photography - Hyperspectral imaging - Motion capture - 3D imaging - Active vision - Image representations - Shape representations - Appearance  texture - Hierarchical representations - Computer vision problems - Interest point  salient region detections - Image segmentation  - Video segmentation - Shape inference - Object detection - Object recognition - Object identification - Tracking - Reconstruction - Matching',\n",
    "              \n",
    "              'Learning paradigms - Supervised learning - Ranking - Learning to rank - Supervised learning  classification - Supervised learning  regression - Structured outputs - Cost sensitive learning - Unsupervised learning - Cluster analysis - Anomaly detection - Mixture modeling - Topic modeling - Source separation - Motif discovery - Dimensionality reduction  manifold learning - Reinforcement learning - Sequential decision making - Inverse reinforcement learning - Apprenticeship learning - Multi-agent reinforcement learning - Adversarial learning - Multi-task learning - Transfer learning - Lifelong machine learning - Learning under covariate shift',\n",
    "              'Learning settings - Batch learning - Online learning settings - Learning from demonstrations - Learning from critiques - Learning from implicit feedback - Active learning settings - Semi supervised learning settings',\n",
    "              'Machine learning approaches - Classification  regression trees - Kernel methods - Support vector machines - Gaussian processes - Neural networks - Logical  relational learning - Inductive logic learning - Statistical relational learning - Learning in probabilistic graphical models - Maximum likelihood modeling - Maximum entropy modeling - Maximum a posteriori modeling - Mixture models - Latent variable models - Bayesian network models - Learning linear models - Perceptron algorithm - Factorization methods - Non-negative matrix factorization - Factor analysis - Principal component analysis - Canonical correlation analysis - Latent Dirichlet allocation - Rule learning - Instance-based learning - Markov decision processes -  Markov decision processes - Stochastic games - Learning latent representations - Deep belief networks - Bio inspired approaches - Artificial life - Evolvable hardware - Genetic algorithms - Genetic programming - Evolutionary robotics - Generative  developmental approaches',\n",
    "              'Machine learning algorithms - Dynamic programming Markov decision processes - Value iteration - Q learning - Policy iteration - Temporal difference learning - Approximate dynamic programming methods - Ensemble methods - Boosting - Bagging - Spectral methods - Feature selection - Regularization',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu17lpjOg7F2"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "draRfEpf8A5L",
        "outputId": "10f13f1a-f989-4a63-c6f3-585f30a126cd"
      },
      "source": [
        "stops = ['a','an','we','result','however','yet','since','previously','although','propose','proposed','e_g','method',\n",
        "         'published_elsevier','b','v','problem','paper','approach','within','with','by','via','way','t','case','issue','level','area','system',\n",
        "         'work','discussed','seen','put','usually','take','make','author','versus','enables','result','research','design','based']\n",
        "punkts = [' ','','(',')','[',']','{','}','.',',','!','?','<','>','-','_',':',';','\\\\','/','|','&','%',\"'s\",\"`s\",'#','$','@']\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = list(set(stopwords.words(\"english\")))+stops+punkts\n",
        "np.random.seed(50)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLK4cDhVkKbs"
      },
      "source": [
        "# Get embeddings from a pre-trained model\n"
      ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41443/41443 [00:01<00:00, 31648.43it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "for token in tqdm(corpus_tokens_probabilities['token'],total=corpus_tokens_probabilities.shape[0]):\n",
    "    vectors.append(model.wv[token])\n",
    "corpus_tokens_probabilities['vector'] = vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Y9rI72IxZal"
   },
   "source": [
    "Calculate weighted average vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WwJtQqABN8w"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23144/23144 [2:22:26<00:00,  2.71it/s]  \n"
     ]
    }
   ],
   "source": [
    "a = 1e-3\n",
    "embedding_size = 128\n",
    "\n",
    "doc_set = []\n",
    "for doc in tqdm(corpus['abstracts'].values.tolist(),total=len(corpus['abstracts'].values.tolist())):\n",
    "    vs = np.zeros(embedding_size)  # add all word2vec values into one vector for the sentence\n",
    "    doc_length = len(doc.split())\n",
    "#     print(doc.split())\n",
    "    for word in doc.lower().split():\n",
    "        a_value = a / (a + corpus_tokens_probabilities[corpus_tokens_probabilities['token']==word]['probability'].values.tolist()[0])  # smooth inverse frequency, SIF\n",
    "        vs = np.add(vs, np.multiply(a_value, corpus_tokens_probabilities[corpus_tokens_probabilities['token']==word]['vector'].values.tolist()[0]))  # vs += sif * word_vector\n",
    "\n",
    "    vs = np.divide(vs, doc_length)  # weighted average\n",
    "    doc_set.append(vs)  # add to our existing re-calculated set of sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(doc_set).to_csv(datapath+'Corpus/cora-classify/cora/embeddings/FastText SIF cora_wos corpus',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p3C8IQiLgBJJ"
   },
   "source": [
    "##### Wikipedia embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6XYOAVzkKsR"
      },
      "source": [
        "#### Option A - Load corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "PkWEF6DBkKzd",
        "outputId": "7482c9f8-c3ce-47d2-d05c-4ec821a88f79"
      },
      "source": [
        "directory = datapath+'Corpus/Unsupervised Training/Sentences/'\n",
        "file_name = 'sentences cora_wos training'#corpus abstract-title - with n-grams'\n",
        "corpus = pd.read_csv(directory+file_name,names=['abstracts'])\n",
        "corpus_tokens = [item.lower() for sublist in corpus['abstracts'].values.tolist() for item in sublist.split()]\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "zFwzf5Aa34G8",
        "outputId": "fc0ac909-b5dd-47eb-fa73-cd340be7b42b"
      },
      "source": [
        "corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstracts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lighting system disclosed comprising plurality...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>photographic device eliminates photometric err...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>print demand camera system camera unit incorpo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>simplified camera mechanism method allowing op...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>beam splitter digital camera split light passe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19705</th>\n",
              "      <td>display engine video graphic system includes p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19706</th>\n",
              "      <td>facilitating interaction may enabled communica...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19707</th>\n",
              "      <td>present invention generally directed system me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19708</th>\n",
              "      <td>disclosed system producing image including app...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19709</th>\n",
              "      <td>fully programmable graphic microprocessor embo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19710 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               abstracts\n",
              "0      lighting system disclosed comprising plurality...\n",
              "1      photographic device eliminates photometric err...\n",
              "2      print demand camera system camera unit incorpo...\n",
              "3      simplified camera mechanism method allowing op...\n",
              "4      beam splitter digital camera split light passe...\n",
              "...                                                  ...\n",
              "19705  display engine video graphic system includes p...\n",
              "19706  facilitating interaction may enabled communica...\n",
              "19707  present invention generally directed system me...\n",
              "19708  disclosed system producing image including app...\n",
              "19709  fully programmable graphic microprocessor embo...\n",
              "\n",
              "[19710 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG1-O4D26irL"
      },
      "source": [
        "#### Option B - Load keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7qJSz6l6mVt"
      },
      "source": [
        "directory = datapath+'LDA/'\n",
        "file_name = period+' top_90-percentile_keywords_terms.csv'\n",
        "corpus = pd.read_csv(directory+file_name)\n",
        "corpus = corpus.fillna('this_is_null')\n",
        "corpus_tokens = []\n",
        "for idx,row in tqdm(corpus.iterrows(),total=corpus.shape[0]):\n",
        "    for token in row.values.tolist():\n",
        "        if token != 'this_is_null': \n",
        "            corpus_tokens.append(token) \n",
        "del corpus\n",
        "print(\"\\nNumber of unique tokens:\",len(corpus_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YXZPTZrKjhr"
      },
      "source": [
        "#### Option C - Load author keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq2HreIeKqP0"
      },
      "source": [
        "directory = datapath+'Author keywords - 29 Oct 2019/'\n",
        "file_name = period+' keyword frequency'\n",
        "corpus = pd.read_csv(directory+file_name,names=['keyword','frequency'])\n",
        "corpus = corpus.fillna('this_is_null')\n",
        "threshold = np.percentile(corpus['frequency'].values,percentile)\n",
        "corpus = corpus[corpus['frequency']>threshold]\n",
        "\n",
        "corpus_tokens = []\n",
        "for idx,row in tqdm(corpus.iterrows(),total=corpus.shape[0]):\n",
        "    if row['keyword'] != 'this_is_null': \n",
        "        corpus_tokens.append(row['keyword']) \n",
        "print(\"\\nNumber of unique tokens:\",len(corpus_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqlZHLo1e40r"
      },
      "source": [
        "## Facebook Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeC8SkXPkKjx"
      },
      "source": [
        "#### Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv3ACHBtexEI"
      },
      "source": [
        "fb_model_address = datapath+'/FastText-crawl-300d-2M-subword/crawl-300d-2M-subword.bin'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6K74cHMkKnn"
      },
      "source": [
        "model = fasttext.load_model(fb_model_address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2648m7e8kk0n"
      },
      "source": [
        "#### Get embeddings\n",
        "\n",
        "*   Save to dictionary and json\n",
        "*   This takes much less space on disk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mq87DiZxVvY"
      },
      "source": [
        "##### No n-gram handle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiJcbZqHkkLl"
      },
      "source": [
        "# Save in a dict\n",
        "output_dict = {}\n",
        "comment_embedding = ''\n",
        "for token in tqdm(corpus_tokens[:],total=len(corpus_tokens[:])):\n",
        "    output_dict[token] = str(model.get_word_vector(token))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOMUto5AxVDy"
      },
      "source": [
        "##### Manual n-gram handle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4UJAF3exSLm"
      },
      "source": [
        "# Save in a dict\n",
        "comment_embedding = 'average_manual '\n",
        "output_dict = {}\n",
        "for token in tqdm(corpus_tokens[:],total=len(corpus_tokens[:])):\n",
        "    token_split = token.split(' ')\n",
        "    if len(token_split) > 0:\n",
        "        tmp_vector_grams = []\n",
        "        for item in token_split:\n",
        "            tmp_vector_grams.append(model.get_word_vector(item))\n",
        "        output_dict[token] = str(list(np.array(tmp_vector_grams).mean(axis=0)))\n",
        "    else:\n",
        "        output_dict[token] = str(model.get_word_vector(item))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8nMUTU0xXo8"
      },
      "source": [
        "##### Save to disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJk-lqjemSky"
      },
      "source": [
        "# Save embeddings to disk\n",
        "with open(directory+'vectors/100D/FastText vector '+comment_embedding+period+'.json', 'w') as json_file:\n",
        "    json.dump(output_dict, json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXkwEnj4lnJi"
      },
      "source": [
        "#### Get embeddings (alternative) : \n",
        "\n",
        "*   save to a list instead of a dicktionary and csv\n",
        "*   Will have many redundant words in it and will take lots of disk space\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GndJ3Q1OlnSy"
      },
      "source": [
        "# Save in a list\n",
        "batches = 1000\n",
        "batch_size = len(corpus_tokens)/batches\n",
        "\n",
        "for step in tqdm(range(batches),total=batches):\n",
        "    batch_tokens = corpus_tokens[int(step*batch_size):int((step+1)*batch_size)]\n",
        "    corpus_vectors = [model.get_word_vector(x) for x in batch_tokens]\n",
        "    corpus_vectors = pd.DataFrame(corpus_vectors)\n",
        "    corpus_vectors['tokens'] = batch_tokens\n",
        "\n",
        "    # Save embeddings to disk\n",
        "    with open(directory+'vector '+period,'a') as f:\n",
        "        corpus_vectors.to_csv(f,index=False,header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Xcww-Mfdaj"
      },
      "source": [
        "## Gensim Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPd9g4E4fgTi"
      },
      "source": [
        "#### Load model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06depR4llpmB"
      },
      "source": [
        "Load gensim model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_UYlrcCfgxw"
      },
      "source": [
        "gensim_model_address = datapath+'FastText Models/50D w1/fasttext-scopus-2.2-million_docs-gensim 50D-w1.model'\n",
        "model = fasttext_gensim.load(gensim_model_address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx1x_twuQCbQ"
      },
      "source": [
        "gensim_model_address = datapath+\"FastText Models/100D Oct2020/deflemm/patent_wos corpus.model\"\n",
        "model = fasttext_gensim.load(gensim_model_address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q18gRbrW3mvO"
      },
      "source": [
        "gensim_model_address = 'drive/My Drive/Data-Permenant/FastText-crawl-300d-2M-subword/crawl-300d-2M-subword.bin'\n",
        "model = gensim.models.fasttext.load_facebook_model(gensim_model_address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhKlmoX6gPE-"
      },
      "source": [
        "Simple Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7oeEkj_gNLg"
      },
      "source": [
        "print('intelligence' in model.wv.vocab)\n",
        "print(model.similarity(\"anns\", \"ann\"))\n",
        "print(model.most_similar(['eye','vision','processing'], ['computer']))\n",
        "print(model.wmdistance(['stop', 'word', 'removed', 'tokens', 'of', 'sentence 1'], ['stop word removed tokens of sentence 2']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCuEFwIMbND_"
      },
      "source": [
        "# get distance of two words\n",
        "from scipy import spatial,sparse,sign\n",
        "vec_a = model.wv['']\n",
        "vec_b = model.wv['fpga']\n",
        "distance_tmp = spatial.distance.cosine(vec_a, vec_b)\n",
        "similarity_tmp = 1 - distance_tmp\n",
        "similarity_tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPioY7Lj7BOP"
      },
      "source": [
        "model.wv['artificial intelligence']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqUSjLRy_isH"
      },
      "source": [
        "(model.wv['artificial']+model.wv['intelligence'])/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0gnOxud_m4H"
      },
      "source": [
        "model.wv['artificial_intelligence']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5516KiWbelyF"
      },
      "source": [
        "#### Compare vectors to ACM categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AxNRKUidrCk"
      },
      "source": [
        "AI_categories = [\n",
        "              'Artificial Intelligence Applications - Expert Systems',\n",
        "              'Automatic Programming',\n",
        "              'Deduction - Theorem Proving',\n",
        "              'Knowledge Representation Formalisms - Knowledge Representation Methods',\n",
        "              'Programming Languages - Software',\n",
        "              'Learning',\n",
        "              'Natural Language Processing',\n",
        "              'Problem Solving - Control Methods - Search',\n",
        "              'Robotics',\n",
        "              'Vision - Scene Understanding',\n",
        "              'Distributed Artificial Intelligence',\n",
        "              'ARTIFICIAL INTELLIGENCE'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QeWKP-k1qpy"
      },
      "source": [
        "categories = [\n",
        "              'Natural language processing - Information extraction - Machine translation - Discourse, dialogue  pragmatics - Natural language generation - Speech recognition - Lexical semantics - Phonology / morphology',\n",
        "              'Knowledge representation reasoning - Description logics - Semantic networks Nonmonotonic default reasoning  belief revision - Probabilistic reasoning - Vagueness fuzzy logic - Causal reasoning  diagnostics - Temporal reasoning - Cognitive robotics - Ontology engineering - Logic programming answer set programming - Spatial  physical reasoning - Reasoning about belief  knowledge',\n",
        "              'Planning  scheduling - Planning for deterministic actions - Planning under uncertainty - Multi-agent planning - Planning  abstraction  generalization - Robotic planning - Evolutionary robotics',\n",
        "              'Search methodologies - Heuristic function construction - Discrete space search - Continuous space search - Randomized search - Game tree search - Abstraction  micro-operators - Search with partial observations - ',\n",
        "              'Control methods - Robotic planning - Evolutionary robotics - Computational control theory - Motion path planning',\n",
        "              'Philosophical theoretical foundations artificial intelligence - Cognitive science - Theory mind',\n",
        "              'Distributed artificial intelligence - Multi agent systems - Intelligent agents - Mobile agents - Cooperation  coordination',\n",
        "              'Computer vision - Biometrics - Scene understanding - Activity recognition  understanding - Video summarization - Visual content based indexing  retrieval - Visual inspection - Vision for robotics - Scene anomaly detection - Image  video acquisition - Camera calibration - Epipolar geometry - Computational photography - Hyperspectral imaging - Motion capture - 3D imaging - Active vision - Image representations - Shape representations - Appearance  texture - Hierarchical representations - Computer vision problems - Interest point  salient region detections - Image segmentation  - Video segmentation - Shape inference - Object detection - Object recognition - Object identification - Tracking - Reconstruction - Matching',\n",
        "              \n",
        "              'Learning paradigms - Supervised learning - Ranking - Learning to rank - Supervised learning  classification - Supervised learning  regression - Structured outputs - Cost sensitive learning - Unsupervised learning - Cluster analysis - Anomaly detection - Mixture modeling - Topic modeling - Source separation - Motif discovery - Dimensionality reduction  manifold learning - Reinforcement learning - Sequential decision making - Inverse reinforcement learning - Apprenticeship learning - Multi-agent reinforcement learning - Adversarial learning - Multi-task learning - Transfer learning - Lifelong machine learning - Learning under covariate shift',\n",
        "              'Learning settings - Batch learning - Online learning settings - Learning from demonstrations - Learning from critiques - Learning from implicit feedback - Active learning settings - Semi supervised learning settings',\n",
        "              'Machine learning approaches - Classification  regression trees - Kernel methods - Support vector machines - Gaussian processes - Neural networks - Logical  relational learning - Inductive logic learning - Statistical relational learning - Learning in probabilistic graphical models - Maximum likelihood modeling - Maximum entropy modeling - Maximum a posteriori modeling - Mixture models - Latent variable models - Bayesian network models - Learning linear models - Perceptron algorithm - Factorization methods - Non-negative matrix factorization - Factor analysis - Principal component analysis - Canonical correlation analysis - Latent Dirichlet allocation - Rule learning - Instance-based learning - Markov decision processes -  Markov decision processes - Stochastic games - Learning latent representations - Deep belief networks - Bio inspired approaches - Artificial life - Evolvable hardware - Genetic algorithms - Genetic programming - Evolutionary robotics - Generative  developmental approaches',\n",
        "              'Machine learning algorithms - Dynamic programming Markov decision processes - Value iteration - Q learning - Policy iteration - Temporal difference learning - Approximate dynamic programming methods - Ensemble methods - Boosting - Bagging - Spectral methods - Feature selection - Regularization',\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rtaQ7KbfQ_y"
      },
      "source": [
        "AI_vectors = []\n",
        "labels = []\n",
        "for item in categories:\n",
        "    vector_tmp = []\n",
        "    label = item.split('-')[0]\n",
        "    for phrase in item.split('-'):\n",
        "        phrase = phrase.lower().strip()\n",
        "        # print(phrase)\n",
        "        vector_tmp.append(model.wv[phrase])\n",
        "    # print('---')\n",
        "    AI_vectors.append(list(np.array(vector_tmp).mean(axis=0)))\n",
        "    labels.append(label)\n",
        "print(AI_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brCodssfgEgt"
      },
      "source": [
        "pd.DataFrame(AI_vectors).to_csv(datapath+'FastText doc clusters - SIP/50D/classification/ACM_classifications_vectors')\n",
        "pd.DataFrame(labels,columns=['label']).to_csv(datapath+'FastText doc clusters - SIP/50D/classification/ACM_classifications_labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMAHWCgX6kEz"
      },
      "source": [
        "### Get Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LPLO5z4cER3"
      },
      "source": [
        "##### No n-gram handle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP8pYSjf6kiR"
      },
      "source": [
        "# Save in a dict\n",
        "comment_embedding = ''\n",
        "output_dict = {}\n",
        "for token in tqdm(corpus_tokens[:],total=len(corpus_tokens[:])):\n",
        "    output_dict[token] = str(model.wv[token])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGLF7jepcI83"
      },
      "source": [
        "##### Manual n-gram handle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9cb-KpAcPWJ"
      },
      "source": [
        "# Save in a dict\n",
        "comment_embedding = 'average_manual '\n",
        "output_dict = {}\n",
        "for token in tqdm(corpus_tokens[:],total=len(corpus_tokens[:])):\n",
        "    token_split = token.split(' ')\n",
        "    if len(token_split) > 0:\n",
        "        tmp_vector_grams = []\n",
        "        for item in token_split:\n",
        "            tmp_vector_grams.append(model.wv[item])\n",
        "        output_dict[token] = str(list(np.array(tmp_vector_grams).mean(axis=0)))\n",
        "    else:\n",
        "        output_dict[token] = str(model.wv[token])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk6CJa6pcQC-"
      },
      "source": [
        "##### Save to disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4wRDXMX6tBH"
      },
      "source": [
        "# Save embeddings to disk\n",
        "with open(directory+'/FastText vector with n-grams '+comment_embedding+period+'.json', 'w') as json_file:\n",
        "    json.dump(output_dict, json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohPXzf8mKUxQ"
      },
      "source": [
        "print(directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yYjO-JQBLE3"
      },
      "source": [
        "### Get Doc Embeddings (SIF)\n",
        "\n",
        "It is not recommended to perform this on cloud, as it is not process intesive, yet takes too long depending on the doc-count. It might take over 30 hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp1fP9z4xUGX"
      },
      "source": [
        "Make a probability dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buSIQtoWVnOz"
      },
      "source": [
        "corpus_tokens_s = pd.Series(corpus_tokens)\n",
        "corpus_tokens_probabilities = (corpus_tokens_s.groupby(corpus_tokens_s).transform('count') / len(corpus_tokens_s)).values\n",
        "corpus_tokens_probabilities = pd.DataFrame(corpus_tokens_probabilities)\n",
        "corpus_tokens_probabilities['tokens'] = corpus_tokens_s\n",
        "corpus_tokens_probabilities.columns = ['probability','token']\n",
        "corpus_tokens_probabilities = corpus_tokens_probabilities.groupby('token').mean()\n",
        "corpus_tokens_probabilities = corpus_tokens_probabilities.reset_index()\n",
        "corpus_tokens_probabilities.columns = ['token','probability']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu51jQbo9QE6"
      },
      "source": [
        "Get vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72BAU9Jm9PwE",
        "outputId": "37814d46-25b7-40cd-95fc-d260f98c16e9"
      },
      "source": [
        "vectors = []\n",
        "for token in tqdm(corpus_tokens_probabilities['token'],total=corpus_tokens_probabilities.shape[0]):\n",
        "    vectors.append(model.wv[token])\n",
        "corpus_tokens_probabilities['vector'] = vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17086/17086 [00:00<00:00, 42392.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y9rI72IxZal"
      },
      "source": [
        "Calculate weighted average vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WwJtQqABN8w",
        "outputId": "952c188e-1e42-4099-bdf2-09792292ca61"
      },
      "source": [
        "a = 1e-3\n",
        "embedding_size = 100\n",
        "\n",
        "doc_set = []\n",
        "for doc in tqdm(corpus['abstracts'].values.tolist(),total=len(corpus['abstracts'].values.tolist())):\n",
        "    vs = np.zeros(embedding_size)  # add all word2vec values into one vector for the sentence\n",
        "    doc_length = len(doc.split())\n",
        "#     print(doc.split())\n",
        "    for word in doc.lower().split():\n",
        "        a_value = a / (a + corpus_tokens_probabilities[corpus_tokens_probabilities['token']==word]['probability'].values.tolist()[0])  # smooth inverse frequency, SIF\n",
        "        vs = np.add(vs, np.multiply(a_value, corpus_tokens_probabilities[corpus_tokens_probabilities['token']==word]['vector'].values.tolist()[0]))  # vs += sif * word_vector\n",
        "\n",
        "    vs = np.divide(vs, doc_length)  # weighted average\n",
        "    doc_set.append(vs)  # add to our existing re-calculated set of sentences\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19710/19710 [50:09<00:00,  6.55it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgtn7-SFQR1j"
      },
      "source": [
        "pd.DataFrame(doc_set).to_csv(datapath+'Corpus/KPRIS/embeddings/deflemm/FastText SIF patent_wos corpus',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3C8IQiLgBJJ"
      },
      "source": [
        "##### Wikipedia embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKLjavgtgBeJ"
      },
      "source": [
        "import os\n",
        "import time\n",
        "from stat import S_ISREG, ST_CTIME, ST_MODE\n",
        "\n",
        "print(\"\\nSearching for Wiki texts...\\n\")\n",
        "dir_path = datapath+'Corpus/AI Wiki Classifications/applications/clean/'\n",
        "data = (os.path.join(dir_path, fn) for fn in os.listdir(dir_path))\n",
        "data = ((os.stat(path), path) for path in data)\n",
        "data = ((stat[ST_CTIME], path) for stat, path in data if S_ISREG(stat[ST_MODE]))\n",
        "\n",
        "names = []\n",
        "files = []\n",
        "for cdate, path in sorted(data):\n",
        "    print('   - ', time.ctime(cdate), os.path.basename(path),int(os.path.getsize(path)/1000000),'MB')\n",
        "    files.append(path)\n",
        "    names.append(os.path.basename(path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFMUH3SfgF2h"
      },
      "source": [
        "embedding_size = 50\n",
        "all = []\n",
        "for file_index,file in enumerate(files):\n",
        "    print('\\n',file,'\\n')\n",
        "    corpus = pd.read_csv(file)\n",
        "    corpus_tokens = [item.lower() for sublist in corpus['sentence'].values.tolist() if pd.notnull(sublist) for item in sublist.split()]\n",
        "    gc.collect()\n",
        "    \n",
        "    corpus_tokens_s = pd.Series(corpus_tokens)\n",
        "    corpus_tokens_probabilities = (corpus_tokens_s.groupby(corpus_tokens_s).transform('count') / len(corpus_tokens_s)).values\n",
        "    corpus_tokens_probabilities = pd.DataFrame(corpus_tokens_probabilities)\n",
        "    corpus_tokens_probabilities['tokens'] = corpus_tokens_s\n",
        "    corpus_tokens_probabilities.columns = ['probability','token']\n",
        "    corpus_tokens_probabilities = corpus_tokens_probabilities.groupby('token').mean()\n",
        "    corpus_tokens_probabilities = corpus_tokens_probabilities.reset_index()\n",
        "    corpus_tokens_probabilities.columns = ['token','probability']\n",
        "\n",
        "    vectors = []\n",
        "    for token in tqdm(corpus_tokens_probabilities['token'],total=corpus_tokens_probabilities.shape[0]):\n",
        "        vectors.append(model.wv[token])\n",
        "    corpus_tokens_probabilities['vector'] = vectors\n",
        "\n",
        "    a = 1e-3\n",
        "\n",
        "    doc_set = []\n",
        "    for doc in tqdm(corpus['sentence'].values.tolist(),total=len(corpus['sentence'].values.tolist())):\n",
        "        vs = np.zeros(embedding_size)  # add all word2vec values into one vector for the sentence\n",
        "        if pd.notnull(doc):\n",
        "            doc_length = len(doc.split())\n",
        "        #     print(doc.split())\n",
        "            for word in doc.lower().split():\n",
        "                a_value = a / (a + corpus_tokens_probabilities[corpus_tokens_probabilities['token']==word]['probability'].values.tolist()[0])  # smooth inverse frequency, SIF\n",
        "                vs = np.add(vs, np.multiply(a_value, corpus_tokens_probabilities[corpus_tokens_probabilities['token']==word]['vector'].values.tolist()[0]))  # vs += sif * word_vector\n",
        "\n",
        "            vs = np.divide(vs, doc_length)  # weighted average\n",
        "            doc_set.append(vs)  # add to our existing re-calculated set of sentences\n",
        "\n",
        "    pd.DataFrame(doc_set).to_csv(datapath+'Corpus/AI Wiki Classifications/applications/clean/vectors/'+names[file_index],index=False)\n",
        "\n",
        "    all.append(pd.DataFrame(doc_set).mean(axis=0))\n",
        "\n",
        "all_df = pd.DataFrame(all)\n",
        "all_df['clusters'] = names\n",
        "all_df.to_csv(datapath+'Corpus/AI Wiki Classifications/applications/clean/vectors/all',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_UVSpQOQR1k"
      },
      "source": [
        "### Get Doc Embedding (averaging)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG2BD96eQR1k",
        "outputId": "81ed827d-1b3e-4655-e5a3-b496992d7d55"
      },
      "source": [
        "doc_vectors = []\n",
        "for doc in tqdm(corpus['abstracts']):\n",
        "    tokens = doc.split()\n",
        "    doc_vectors.append(np.array([model.wv[token] for token in tokens]).mean(axis=0))\n",
        "pd.DataFrame(doc_vectors).to_csv(datapath+'Corpus/KPRIS/embeddings/FastText Avg large corpus',index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19710/19710 [00:09<00:00, 2073.87it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_m00tKKkKNG"
      },
      "source": [
        "# Train on a large Scopus corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz7pfl9d6R9o"
      },
      "source": [
        "#### Load Corpus Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuO0J8oxkKWz"
      },
      "source": [
        "sentence_corpus = pd.read_csv(datapath+'Corpus/Unsupervised Training/Sentences/sentences cora_wos training')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYHEYxwpEgCi"
      },
      "source": [
        "#### Preprocess and prepare corpus for FastText training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkqrGJ3cEgnX"
      },
      "source": [
        "sentences = []\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "with open(datapath+'corpus/AI ALL/1900-2019 corpus sentences abstract-title further processed.csv', 'w') as f:\n",
        "    for index,row in tqdm(sentence_corpus.iterrows(),total=sentence_corpus.shape[0]):\n",
        "        sentence = row['sentence']\n",
        "        sentence = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",sentence)\n",
        "        sentence = word_tokenize(sentence)\n",
        "        sentence = [word for word in sentence if not word in punkts] \n",
        "        sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
        "        # sentences.append(sentence)\n",
        "        f.write(\"%s\\n\" % ' '.join(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQHwYepMdKFc"
      },
      "source": [
        "#### Save sentences to disk for future use -- Not needed anymore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJZ0UwdhdOlp"
      },
      "source": [
        "pd.DataFrame([' '.join(words) for words in sentences],columns=['sentences']).to_csv(\n",
        "    datapath+'corpus/AI ALL/1900-2019 corpus sentences abstract-title further processed.csv',\n",
        "    header=True,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j6H5hsfu8p0"
      },
      "source": [
        "#### Load pre-processed sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLHxqzOPvCdV",
        "outputId": "8da0457c-3c0e-4912-8a6b-a7c7d922b491"
      },
      "source": [
        "# sentence_corpus = pd.read_csv(datapath+'Corpus/AI ALL/1900-2019 corpus sentences abstract-title further processed.csv',delimiter=\";;;\")\n",
        "# sentence_corpus.columns = [\"sentence\"]\n",
        "sentences = []\n",
        "sentence_corpus = sentence_corpus.fillna('')\n",
        "for index,row in tqdm(sentence_corpus.iterrows(),total=sentence_corpus.shape[0]):\n",
        "    sentences.append(row['sentence'].split(' '))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 305226/305226 [00:32<00:00, 9360.58it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "UA83SDYjwfrO",
        "outputId": "1b838519-375f-4189-e5e8-8d40834d0805"
      },
      "source": [
        "sentence_corpus.head(10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>reference c delobel r g casey</td>\n",
              "      <td>2010.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>decomposition data base theory boolean</td>\n",
              "      <td>2010.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>lemma</td>\n",
              "      <td>2010.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>therefore may iteratively repeat step generati...</td>\n",
              "      <td>2010.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>show g contains hamiltonneian path concludes p...</td>\n",
              "      <td>2010.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>p l</td>\n",
              "      <td>2010.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>hammer</td>\n",
              "      <td>2010.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>kogan</td>\n",
              "      <td>2010.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>horn function minimization knowledge compressi...</td>\n",
              "      <td>2010.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>rutcor research report rrr rutgers university ...</td>\n",
              "      <td>2010.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   article_index                                           sentence    year\n",
              "0              0                      reference c delobel r g casey  2010.0\n",
              "1              0             decomposition data base theory boolean  2010.0\n",
              "2              0                                              lemma  2010.0\n",
              "3              0  therefore may iteratively repeat step generati...  2010.0\n",
              "4              0  show g contains hamiltonneian path concludes p...  2010.0\n",
              "5              0                                                p l  2010.0\n",
              "6              0                                             hammer  2010.0\n",
              "7              0                                              kogan  2010.0\n",
              "8              0  horn function minimization knowledge compressi...  2010.0\n",
              "9              0  rutcor research report rrr rutgers university ...  2010.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKp8-IcEQR1l",
        "outputId": "d7379b2d-7120-4b8a-9e45-21f381f84cd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentences[:5]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['reference', 'c', 'delobel', 'r', 'g', 'casey'],\n",
              " ['decomposition', 'data', 'base', 'theory', 'boolean'],\n",
              " ['lemma'],\n",
              " ['therefore',\n",
              "  'may',\n",
              "  'iteratively',\n",
              "  'repeat',\n",
              "  'step',\n",
              "  'generating',\n",
              "  'f',\n",
              "  'etc',\n",
              "  'j',\n",
              "  'constructed',\n",
              "  'f',\n",
              "  'j',\n",
              "  'path',\n",
              "  'contains',\n",
              "  'vertex',\n",
              "  'g'],\n",
              " ['show',\n",
              "  'g',\n",
              "  'contains',\n",
              "  'hamiltonneian',\n",
              "  'path',\n",
              "  'concludes',\n",
              "  'proof',\n",
              "  'theorem']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-9A4hbD7xwV"
      },
      "source": [
        "### Train Fasttext - Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYfn-ZQfhed1"
      },
      "source": [
        "#### Load a model to continue training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bnu7_aMhu-A"
      },
      "source": [
        "* If want to continue training, run this section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7eHrQg_g2ej"
      },
      "source": [
        "model = load(gensim_model_address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s07ymd4xhtWh"
      },
      "source": [
        "model.build_vocab(sentences, update=True)\n",
        "model.train(sentences, total_examples=len(sentences), epochs=model.epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9LWfxerh2DI"
      },
      "source": [
        "* Otherwise run this section\n",
        "\n",
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsDmFFxp7yJa"
      },
      "source": [
        "model = fasttext_gensim(min_n=3, max_n=6, size=15, window=5, min_count=1, seed = 50)\n",
        "model.build_vocab(sentences=sentences)\n",
        "model.train(sentences=sentences, total_examples=len(sentences), epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqV_noKRcD4b"
      },
      "source": [
        "fname = \"datapath+Models/fasttext-scopus-2.2-million_docs-gensim 15D.model\"\n",
        "model.save(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3vrtHdqHy-K"
      },
      "source": [
        "model = fasttext_gensim(min_n=3, max_n=6, size=50, window=5, min_count=1, seed = 50)\n",
        "model.build_vocab(sentences=sentences)\n",
        "model.train(sentences=sentences, total_examples=len(sentences), epochs=10)\n",
        "fname = datapath+\"Models/fasttext-scopus-2.2-million_docs-gensim 50D.model\"\n",
        "model.save(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g75Pt__ZH4RZ"
      },
      "source": [
        "model = fasttext_gensim(min_n=3, max_n=12, size=128, window=5, min_count=5, seed = 1)\n",
        "model.build_vocab(sentences=sentences)\n",
        "model.train(sentences=sentences, total_examples=len(sentences), epochs=40,threads=15)\n",
        "fname = datapath+\"Corpus/cora-classify/cora/models/fasttext cora_wos corpus 128d5w.model\"\n",
        "model.save(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnTG0yUGiqxw"
      },
      "source": [
        "#### Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVJoU_sLiw2E"
      },
      "source": [
        "similarities = model.wv.most_similar(positive=['logic','fuzzy','expert'],negative=['deep','neural','network','cnn','ann'])\n",
        "most_similar = similarities[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaKS52YSDcup"
      },
      "source": [
        "most_similar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkokDJdHjEFz"
      },
      "source": [
        "not_matching = model.wv.doesnt_match(\"human computer interface tree\".split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fOp26djDYk7"
      },
      "source": [
        "not_matching"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7RJl89QjMhl"
      },
      "source": [
        "sim_score = model.wv.similarity('computer', 'human')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtlAXvqWDWo4"
      },
      "source": [
        "sim_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWLLBs8UBRwy"
      },
      "source": [
        "print(model.wv['artificial intelligence'])\n",
        "print(model.wv['artificial'])\n",
        "print(model.wv['intelligence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sSZfjEMUyMc"
      },
      "source": [
        "### Train Fasttext - Facebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK4N7jUqU1CJ"
      },
      "source": [
        "sentences_joined = ' '.join(sentences)\n",
        "model = fasttext.train_unsupervised(sentences_joined, \"cbow\", minn=2, maxn=5, dim=50, epoch=10,lr=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLNquk0BaywP"
      },
      "source": [
        "#### Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i6detxZVMUQ"
      },
      "source": [
        "model.words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o13SRSefVFDD"
      },
      "source": [
        "model.get_word_vector(\"the\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FvMFASYVNzs"
      },
      "source": [
        "model.get_nearest_neighbors('asparagus')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH9Wdf5WV5F8"
      },
      "source": [
        "model.get_analogies(\"intelligence\", \"math\", \"fuzzy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaDnem0pWcNw"
      },
      "source": [
        "#### Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wjWeyw-Wbpi"
      },
      "source": [
        "model.save_model(datapath+\"fasttext-scopus_wos-merged-310k_docs-facebook.ftz\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}