{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sahand/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sciosci.assets import keyword_assets as kw\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "tqdm.pandas()\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sahand/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_from = 1900\n",
    "year_to = 2020\n",
    "\n",
    "MAKE_SENTENCE_CORPUS = False\n",
    "MAKE_SENTENCE_CORPUS_ADVANCED = True\n",
    "MAKE_REGULAR_CORPUS = True\n",
    "GET_WORD_FREQ_IN_SENTENCE = False\n",
    "PROCESS_KEYWORDS = False\n",
    "\n",
    "stops = ['a','an','we','result','however','yet','since','previously','although','propose','proposed','this']\n",
    "nltk.download('stopwords')\n",
    "stop_words = list(set(stopwords.words(\"english\")))+stops\n",
    "\n",
    "\n",
    "# data_path_rel = '/home/sahand/GoogleDrive/Data/Relevant Results _ DOI duplication - scopus keywords - document types - 31 july.csv'\n",
    "data_path_rel = '/home/sahand/Data/AI ALL 1900-2019 - reformat'\n",
    "data_full_relevant = pd.read_csv(data_path_rel)\n",
    "\n",
    "root_dir = '/home/sahand/Data/Corpus/AI ALL/'\n",
    "subdir = 'AI ALL nolem stopword removed thesaurus/' # no_lemmatization_no_stopwords\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Pre-Processing :\n",
    "Following tags requires WoS format. Change them otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2118312/2118312 [01:04<00:00, 32703.67it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "100%|██████████| 2118312/2118312 [00:07<00:00, 283472.38it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "100%|██████████| 2118312/2118312 [00:06<00:00, 320108.14it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "100%|██████████| 2118312/2118312 [00:07<00:00, 302007.68it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "100%|██████████| 2118312/2118312 [00:06<00:00, 341023.33it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "100%|██████████| 2118312/2118312 [00:06<00:00, 306480.96it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 2118312/2118312 [00:06<00:00, 339841.49it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 2118312/2118312 [02:50<00:00, 12434.14it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered = data_full_relevant.copy()\n",
    "data_filtered = data_filtered[pd.notnull(data_filtered['PY'])]\n",
    "\n",
    "data_filtered = data_filtered[data_filtered['PY'].astype('int')>year_from-1]\n",
    "data_filtered = data_filtered[data_filtered['PY'].astype('int')<year_to]\n",
    "\n",
    "# Remove columns without keywords/abstract list \n",
    "data_with_keywords = data_filtered[pd.notnull(data_filtered['DE'])]\n",
    "data_with_abstract = data_filtered[pd.notnull(data_filtered['AB'])]\n",
    "\n",
    "# Remove numbers from abstracts to eliminate decimal points and other unnecessary data\n",
    "data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_c(x) if pd.notnull(x) else np.nan).str.lower()\n",
    "data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'et al.') if pd.notnull(x) else np.nan)\n",
    "data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'eg.') if pd.notnull(x) else np.nan)\n",
    "data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'ie.') if pd.notnull(x) else np.nan)\n",
    "data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'vs.') if pd.notnull(x) else np.nan)\n",
    "data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'ieee') if pd.notnull(x) else np.nan)\n",
    "data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'fig.','figure') if pd.notnull(x) else np.nan)\n",
    "\n",
    "# gc.collect()\n",
    "abstracts = []\n",
    "for abstract in tqdm(data_with_abstract['AB'].values.tolist()):\n",
    "    numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", abstract)\n",
    "    for number in numbers:\n",
    "        abstract = kw.find_and_remove_term(abstract,number)\n",
    "    abstracts.append(abstract)\n",
    "data_with_abstract['AB'] = abstracts.copy()\n",
    "del  abstracts\n",
    "\n",
    "year_list = pd.DataFrame(data_with_abstract['PY'].values.tolist(),columns=['year'])\n",
    "year_list.to_csv(root_dir+subdir+str(year_from)+'-'+str(year_to-1)+' corpus years',index=False) # Save year indices to disk for further use\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if MAKE_SENTENCE_CORPUS is True:\n",
    "    thesaurus = pd.read_csv('data/thesaurus/thesaurus_for_ai_keyword_with_().csv')\n",
    "    thesaurus = thesaurus.fillna('')\n",
    "    print(\"\\nSentence maker and thesaurus matching. \\nThis will take some time...\")\n",
    "    \n",
    "    data_with_abstract['AB_no_c'] = data_with_abstract['AB'].apply(lambda x: kw.find_and_remove_c(x) if pd.notnull(x) else np.nan)\n",
    "    sentence_corpus = []\n",
    "    \n",
    "    for index,row in tqdm(data_with_abstract.iterrows(),total=data_with_abstract.shape[0]):\n",
    "        words = re.split('( |\\\\n|\\.|\\?|!|:|;|,|_|\\[|\\])',row['AB_no_c'].lower())\n",
    "        new_words = []\n",
    "        year = row['PY']\n",
    "        flag_word_removed = False\n",
    "        for w_idx,word in enumerate(words):\n",
    "            if flag_word_removed is True:\n",
    "                if word==' ':\n",
    "                    flag_word_removed = False\n",
    "                    continue\n",
    "            if word in thesaurus['alt'].values.tolist():\n",
    "                word_old = word\n",
    "                buffer_word = word\n",
    "                word = thesaurus[thesaurus['alt']==word]['original'].values.tolist()[0]\n",
    "#                print(\"changed '\",word_old,\"' to '\",word,\"'.\")\n",
    "                \n",
    "            new_words.append(word)\n",
    "            \n",
    "        row = ''.join(new_words)\n",
    "        \n",
    "        sentences = re.split('(\\. |\\? |\\\\n)',row)\n",
    "        sentences = [i+j for i,j in zip(sentences[0::2], sentences[1::2])]\n",
    "        \n",
    "        for sentence_n in sentences:\n",
    "            sentence_corpus.append([index,sentence_n,year])\n",
    "    \n",
    "    sentence_corpus = pd.DataFrame(sentence_corpus,columns=['article_index','sentence','year'])\n",
    "    \n",
    "    sentence_corpus.to_csv(root_dir+subdir+str(year_from)+'-'+str(year_to-1)+' corpus sentences abstract-title',index=False,header=True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2118312 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2118312/2118312 [04:06<00:00, 8590.69it/s]\n",
      "  0%|          | 0/17531442 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17531442/17531442 [47:21<00:00, 6170.38it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String pre processing for abstracts: lower and strip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/17531442 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String pre processing for abstracts: lemmatize and stop word removal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17531442/17531442 [25:08:04<00:00, 193.75it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String pre processing for abstracts: null word removal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17531442/17531442 [01:49<00:00, 159943.56it/s]\n",
      "  0%|          | 0/17531442 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thesaurus matching\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17531442/17531442 [27:02<00:00, 10801.96it/s]\n",
      "  0%|          | 65799/17531442 [00:00<00:26, 657982.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stitiching words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17531442/17531442 [00:14<00:00, 1216825.97it/s]\n"
     ]
    }
   ],
   "source": [
    "if MAKE_SENTENCE_CORPUS_ADVANCED is True:    \n",
    "    data_with_abstract['TI_AB'] = data_with_abstract.TI.map(str) + \". \" + data_with_abstract.AB\n",
    "    data_fresh = data_with_abstract[['TI_AB','PY']].copy()\n",
    "    data_fresh['TI_AB'] = data_fresh['TI_AB'].str.lower()\n",
    "    \n",
    "    del data_with_abstract\n",
    "    gc.collect()\n",
    "    \n",
    "    data_tmp = data_fresh[1:10]\n",
    "    data_fresh[-2:-1]\n",
    "\n",
    "    print(\"\\nSentence extraction\")\n",
    "    sentences = []\n",
    "    years = []\n",
    "    indices = []\n",
    "    for index,row in tqdm(data_fresh.iterrows(),total=data_fresh.shape[0]):\n",
    "        abstract_str = row['TI_AB']\n",
    "        year = row['PY']\n",
    "        abstract_sentences = re.split('\\. |\\? |\\\\n',abstract_str)\n",
    "        length = len(abstract_sentences)\n",
    "        \n",
    "        sentences.extend(abstract_sentences)\n",
    "        years.extend([year for x in range(length)])\n",
    "        indices.extend([index for x in range(length)])\n",
    "        \n",
    "    print(\"\\nTokenizing\")\n",
    "    tmp = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        tmp.append(word_tokenize(sentence))\n",
    "    sentences = tmp.copy()\n",
    "    del tmp\n",
    "\n",
    "    print(\"\\nString pre processing for abstracts: lower and strip\")\n",
    "    sentences = [list(map(str.lower, x)) for x in sentences]\n",
    "    sentences = [list(map(str.strip, x)) for x in sentences]\n",
    "    \n",
    "    tmp = []\n",
    "    print(\"\\nString pre processing for abstracts: lemmatize and stop word removal\")\n",
    "    for string_list in tqdm(sentences, total=len(sentences)):\n",
    "        tmp_list = [kw.string_pre_processing(x,stemming_method='None',lemmatization=False,stop_word_removal=True,stop_words_extra=stops,verbose=False,download_nltk=False) for x in string_list]\n",
    "        tmp.append(tmp_list)\n",
    "    sentences = tmp.copy()\n",
    "    del tmp\n",
    "    \n",
    "    tmp = []\n",
    "    print(\"\\nString pre processing for abstracts: null word removal\")\n",
    "    for string_list in tqdm(sentences, total=len(sentences)):\n",
    "        tmp.append([x for x in string_list if x!=''])\n",
    "    sentences = tmp.copy()\n",
    "    del tmp\n",
    "    \n",
    "    print(\"\\nThesaurus matching\")\n",
    "    sentences = kw.thesaurus_matching(sentences)\n",
    "    \n",
    "    print(\"\\nStitiching words\")\n",
    "    tmp = []\n",
    "    for words in tqdm(sentences, total=len(sentences)):\n",
    "        tmp.append(' '.join(words))\n",
    "    sentences = tmp.copy()\n",
    "    del tmp\n",
    "    \n",
    "    sentence_df = pd.DataFrame(indices,columns=['article_index'])\n",
    "    sentence_df['sentence'] = sentences\n",
    "    sentence_df['year'] = years\n",
    "    sentence_df.to_csv(root_dir+subdir+str(year_from)+'-'+str(year_to-1)+' corpus sentences abstract-title',index=False,header=True)\n",
    "    \n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = pd.read_csv(root_dir+subdir+str(year_from)+'-'+str(year_to-1)+' corpus sentences abstract-title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17531442, 3)\n",
      "(16564332, 3)\n"
     ]
    }
   ],
   "source": [
    "print(sentence_df.shape)\n",
    "sentence_df = sentence_df[pd.notnull(sentence_df['sentence'])]\n",
    "print(sentence_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2118312/2118312 [03:11<00:00, 11042.47it/s]\n"
     ]
    }
   ],
   "source": [
    "abstracts_df = sentence_df.groupby(['article_index','year'])['sentence'].progress_apply('. '.join).reset_index()\n",
    "abstracts_df.columns = ['article_index','year','abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2118312, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_df.to_csv(root_dir+subdir+str(year_from)+'-'+str(year_to-1)+' corpus abstract-title',index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 68.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing keywords...\n",
      "\n",
      "Preparing thesaurus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_keywords = pd.read_csv(root_dir+'../n-gram author keyword taxonomy 300k.csv')\n",
    "\n",
    "wanted_grams = [2,3,4,5,6] # Statistically, 5 seems to be a proper cutting point as the frequency table suggests. Refer to : \"Get statsitic of n in n-grams of corpus\" block in drafts.\n",
    "periods = [[1990,2005],[2005,2008],[2008,2011],[2011,2014],[2014,2017],[2017,2019]]\n",
    "thesaurus = []\n",
    "\n",
    "# =============================================================================\n",
    "# Prepare keywords\n",
    "# =============================================================================\n",
    "print('\\nPreparing keywords...')\n",
    "data_keywords['grams'] = [len(x.split()) for x in data_keywords.keywords.values.tolist()]\n",
    "data_keywords = data_keywords[data_keywords['count']>1]\n",
    "\n",
    "# =============================================================================\n",
    "# Make keyword dictionary/thesaurus for all wanted gram counts\n",
    "# =============================================================================\n",
    "print('\\nPreparing thesaurus...')\n",
    "idx = 0\n",
    "for grams_count in tqdm(wanted_grams):\n",
    "    data_keywords_tmp = data_keywords[data_keywords.grams==wanted_grams[idx]].copy()\n",
    "    data_keywords_tmp['keywords'] = data_keywords_tmp.keywords.str.lower().str.strip().str.replace('  ',' ')\n",
    "    keywords_underscored = data_keywords_tmp.keywords.str.lower().str.strip().str.replace(' ','_').str.upper().values.tolist()\n",
    "    keywords_spaced = data_keywords_tmp.keywords.str.lower().str.strip().values.tolist()\n",
    "    thesaurus.append(dict(zip(keywords_spaced,keywords_underscored)))\n",
    "    idx+=1\n",
    "    \n",
    "thesaurus[2]['fpga'] = 'FIELD_PROGRAMMABLE_GATE_ARRAY'\n",
    "thesaurus[3]['anfis'] = 'ADAPTIVE_NEURO_FUZZY_INFERENCE_SYSTEM'\n",
    "thesaurus[3]['lssvm'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 934/210571 [00:00<00:22, 9338.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for period: 2005-2007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210571/210571 [00:21<00:00, 9860.79it/s] \n",
      "100%|██████████| 210571/210571 [02:03<00:00, 1705.40it/s]\n",
      "100%|██████████| 210571/210571 [11:57<00:00, 293.63it/s]\n",
      "100%|██████████| 210571/210571 [49:55<00:00, 70.29it/s] \n",
      "100%|██████████| 210571/210571 [1:58:21<00:00, 29.65it/s]  \n",
      "  0%|          | 0/296780 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for period: 2008-2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 296780/296780 [00:29<00:00, 10083.55it/s]\n",
      "100%|██████████| 296780/296780 [02:52<00:00, 1716.69it/s]\n",
      "100%|██████████| 296780/296780 [17:09<00:00, 288.27it/s]\n",
      "100%|██████████| 296780/296780 [1:11:03<00:00, 69.62it/s]\n",
      "100%|██████████| 296780/296780 [2:50:54<00:00, 28.94it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for period: 2011-2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 366163/366163 [00:38<00:00, 9503.20it/s]\n",
      "100%|██████████| 366163/366163 [03:42<00:00, 1646.25it/s]\n",
      "100%|██████████| 366163/366163 [22:03<00:00, 276.58it/s]\n",
      "100%|██████████| 366163/366163 [1:32:05<00:00, 66.26it/s]\n",
      "100%|██████████| 366163/366163 [3:40:05<00:00, 27.73it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for period: 2014-2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 420980/420980 [00:44<00:00, 9398.92it/s]\n",
      "100%|██████████| 420980/420980 [04:22<00:00, 1604.26it/s]\n",
      "100%|██████████| 420980/420980 [26:15<00:00, 267.23it/s]\n",
      "100%|██████████| 420980/420980 [1:50:29<00:00, 63.50it/s]\n",
      "100%|██████████| 420980/420980 [4:24:28<00:00, 26.53it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for period: 2017-2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332030/332030 [00:36<00:00, 9102.12it/s]\n",
      "100%|██████████| 332030/332030 [03:30<00:00, 1576.64it/s]\n",
      "100%|██████████| 332030/332030 [21:15<00:00, 260.36it/s]\n",
      "100%|██████████| 332030/332030 [1:29:45<00:00, 61.65it/s]\n",
      "100%|██████████| 332030/332030 [3:40:12<00:00, 25.13it/s]  \n"
     ]
    }
   ],
   "source": [
    "def multiple_replace(string, rep_dict):\n",
    "    pattern = re.compile(\"|\".join([re.escape(k) for k in sorted(rep_dict,key=len,reverse=True)]), flags=re.DOTALL)\n",
    "    return pattern.sub(lambda x: rep_dict[x.group(0)], string)\n",
    "\n",
    "period_names = []\n",
    "for period in periods[:]:\n",
    "    period_name = str(period[0])+'-'+str(period[1]-1)\n",
    "    print('Processing for period:',period_name)\n",
    "    period_names.append(period_name)\n",
    "    abstracts_period = abstracts_df[(abstracts_df['year']>=period[0]) & (abstracts_df['year']<period[1])].copy()\n",
    "    for thesaurus_gram in list(reversed(thesaurus)):\n",
    "        abstracts_period['abstract'] = abstracts_period['abstract'].progress_apply(lambda x: multiple_replace(x,thesaurus_gram))\n",
    "    abstracts_period.to_csv(root_dir+'n-gram by 2 repetition keywords '+period_name,index=False,header=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sentences again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sahand/Data/Corpus/AI ALL/'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1897668/1897668 [04:08<00:00, 7641.99it/s]\n"
     ]
    }
   ],
   "source": [
    "abst_data = pd.read_csv(root_dir+'n-grams/'+'1900-2019 n-gram by 2 repetition keywords',names=['article_index','year','abstract'])\n",
    "\n",
    "sentences = []\n",
    "years = []\n",
    "indices = []\n",
    "for index,row in tqdm(abst_data.iterrows(),total=abst_data.shape[0]):\n",
    "    index = row['article_index']\n",
    "    year = row['year']\n",
    "    abstract_sentences = row['abstract'].split('. ')\n",
    "    length = len(abstract_sentences)\n",
    "\n",
    "    sentences.extend(abstract_sentences)\n",
    "    years.extend([year for x in range(length)])\n",
    "    indices.extend([index for x in range(length)])\n",
    "\n",
    "sent_df = pd.DataFrame(indices,columns=['article_index'])\n",
    "sent_df['sentence'] = sentences\n",
    "sent_df['year'] = years\n",
    "sent_df.to_csv(root_dir+'n-grams/'+'1900-2019 sentences n-gram by 2 repetition keywords',index=False,header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Corpus Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_REGULAR_CORPUS is False:\n",
    "    sys.exit('Did not continue to create normal corpus. If you want a corpus, set it to True at init section.')\n",
    "# =============================================================================\n",
    "#   Get word frequency in sentence corpus -- OPTIONAL\n",
    "# =============================================================================\n",
    "if GET_WORD_FREQ_IN_SENTENCE is True:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    file = root_dir+subdir+str(year_from)+'-'+str(year_to-1)+' corpus sentences abstract-title'#'/home/sahand/GoogleDrive/Data/corpus/AI ALL/1900-2019 corpus sentences abstract-title'\n",
    "    file = pd.read_csv(file)\n",
    "    size = 500000\n",
    "    unique = []\n",
    "    for data_start_point in tqdm(np.arange(0,file.shape[0],size)):\n",
    "        if data_start_point+size<file.shape[0]:\n",
    "            end_point = data_start_point+size\n",
    "        else:\n",
    "            end_point = file.shape[0]-1\n",
    "    #    print(data_start_point,end_point)\n",
    "        str_split = list(file.sentence[data_start_point:end_point].str.split())\n",
    "        str_flat = pd.DataFrame([item for sublist in str_split for item in sublist])\n",
    "        str_flat.columns = ['words']\n",
    "        str_flat.head()\n",
    "    \n",
    "        unique = unique+list(str_flat.words.unique())\n",
    "    \n",
    "    unique = pd.DataFrame(unique)\n",
    "    unique.columns = ['words']\n",
    "    unique = list(unique.words.unique())\n",
    "    len(unique)\n",
    "# =============================================================================\n",
    "# Tokenize (Author Keywords and Abstracts+Titles)\n",
    "# =============================================================================\n",
    "abstracts = []\n",
    "keywords = []\n",
    "keywords_index = []\n",
    "abstracts_pure = []\n",
    "for index,paper in tqdm(data_with_abstract.iterrows(),total=data_with_abstract.shape[0]):\n",
    "    keywords_str = paper['DE']\n",
    "    keywords_index_str = paper['ID']\n",
    "    abstract_str = paper['AB']\n",
    "    title_str = paper['TI']\n",
    "    abstract_dic = word_tokenize(title_str+' '+abstract_str)\n",
    "    abstract_dic_pure = abstract_dic.copy()\n",
    "    if pd.notnull(paper['DE']):\n",
    "        keywords_dic = word_tokenize(keywords_str)\n",
    "        keywords.append(keywords_str.split(';'))\n",
    "        abstract_dic.extend(keywords_dic)\n",
    "    else:\n",
    "        keywords.append([])\n",
    "    if pd.notnull(paper['ID']):\n",
    "        keywords_index.append(keywords_index_str.split(';'))\n",
    "    else:\n",
    "        keywords_index.append([])\n",
    "    abstracts.append(abstract_dic)\n",
    "    abstracts_pure.append(abstract_dic_pure)\n",
    "\n",
    "# Add to main df. Not necessary\n",
    "data_with_abstract['AB_split'] = abstracts_pure \n",
    "data_with_abstract['AB_KW_split'] = abstracts\n",
    "\n",
    "# =============================================================================\n",
    "# Strip and lowe case \n",
    "# =============================================================================\n",
    "abstracts_pure = [list(map(str.strip, x)) for x in abstracts_pure]\n",
    "abstracts_pure = [list(map(str.lower, x)) for x in abstracts_pure]\n",
    "\n",
    "abstracts = [list(map(str.strip, x)) for x in abstracts]\n",
    "abstracts = [list(map(str.lower, x)) for x in abstracts]\n",
    "\n",
    "keywords = [list(map(str.strip, x)) for x in keywords]\n",
    "keywords = [list(map(str.lower, x)) for x in keywords]\n",
    "\n",
    "keywords_index = [list(map(str.strip, x)) for x in keywords_index]\n",
    "keywords_index = [list(map(str.lower, x)) for x in keywords_index]\n",
    "# =============================================================================\n",
    "# Pre Process \n",
    "# =============================================================================\n",
    "tmp_data = []\n",
    "print(\"\\nString pre processing for abstracts\")\n",
    "for string_list in tqdm(abstracts, total=len(abstracts)):\n",
    "    tmp_list = [kw.string_pre_processing(x,stemming_method='None',lemmatization=True,stop_word_removal=True,stop_words_extra=stops,verbose=False,download_nltk=False) for x in string_list]\n",
    "    tmp_data.append(tmp_list)\n",
    "abstracts = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for string_list in tqdm(abstracts_pure, total=len(abstracts_pure)):\n",
    "    tmp_list = [kw.string_pre_processing(x,stemming_method='None',lemmatization=True,stop_word_removal=True,stop_words_extra=stops,verbose=False,download_nltk=False) for x in string_list]\n",
    "    tmp_data.append(tmp_list)\n",
    "abstracts_pure = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "print(\"\\nString pre processing for keywords\")\n",
    "tmp_data = []\n",
    "for string_list in tqdm(keywords, total=len(keywords)):\n",
    "    tmp_list = []\n",
    "    for string in string_list:\n",
    "        tmp_sub_list = string.split()\n",
    "        tmp_list.append(' '.join([kw.string_pre_processing(x,stemming_method='None',lemmatization=True,stop_word_removal=True,stop_words_extra=stops,verbose=False,download_nltk=False) for x in tmp_sub_list]))\n",
    "    tmp_data.append(tmp_list)\n",
    "keywords = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for string_list in tqdm(keywords_index, total=len(keywords_index)):\n",
    "    tmp_list = []\n",
    "    for string in string_list:\n",
    "        tmp_sub_list = string.split()\n",
    "        tmp_list.append(' '.join([kw.string_pre_processing(x,stemming_method='None',lemmatization=True,stop_word_removal=True,stop_words_extra=stops,verbose=False,download_nltk=False) for x in tmp_sub_list]))\n",
    "    tmp_data.append(tmp_list)\n",
    "keywords_index = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "#tmp_data = []\n",
    "#for string_list in tqdm(keywords, total=len(keywords)):\n",
    "#    tmp_list = []\n",
    "#    for sub_string_list in string_list:\n",
    "#        tmp_list.append(' '.join(sub_string_list))\n",
    "#    tmp_data.append(tmp_list)\n",
    "#keywords = tmp_data.copy()\n",
    "#del tmp_data\n",
    "\n",
    "# =============================================================================\n",
    "# Clean-up dead words\n",
    "# =============================================================================\n",
    "tmp_data = []\n",
    "for string_list in tqdm(abstracts, total=len(abstracts)):\n",
    "    tmp_data.append([x for x in string_list if x!=''])\n",
    "abstracts = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for string_list in tqdm(abstracts_pure, total=len(abstracts_pure)):\n",
    "    tmp_data.append([x for x in string_list if x!=''])\n",
    "abstracts_pure = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for string_list in tqdm(keywords, total=len(keywords)):\n",
    "    tmp_data.append([x for x in string_list if x!=''])\n",
    "keywords = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for string_list in tqdm(keywords_index, total=len(keywords_index)):\n",
    "    tmp_data.append([x for x in string_list if x!=''])\n",
    "keywords_index = tmp_data.copy()\n",
    "del tmp_data\n",
    "# =============================================================================\n",
    "# Break-down abstracts again\n",
    "# =============================================================================\n",
    "tmp_data = []\n",
    "for abstract in tqdm(abstracts):\n",
    "    words = []\n",
    "    for word in abstract:\n",
    "        words = words+word.split()\n",
    "    tmp_data.append(words)\n",
    "abstracts = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for abstract in tqdm(abstracts_pure):\n",
    "    words = []\n",
    "    for word in abstract:\n",
    "        words = words+word.split()\n",
    "    tmp_data.append(words)\n",
    "abstracts_pure = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "# =============================================================================\n",
    "# Thesaurus matching\n",
    "# =============================================================================\n",
    "print(\"\\nThesaurus matching\")\n",
    "\n",
    "abstracts_backup = abstracts.copy()\n",
    "abstracts_pure_backup = abstracts_pure.copy()\n",
    "keywords_backup = keywords.copy()\n",
    "keywords_index_backup = keywords_index.copy()\n",
    "\n",
    "abstracts = abstracts_backup.copy()\n",
    "abstracts_pure = abstracts_pure_backup.copy()\n",
    "keywords = keywords_backup.copy()\n",
    "keywords_index = keywords_index_backup.copy()\n",
    "\n",
    "abstracts = kw.thesaurus_matching(abstracts)\n",
    "abstracts_pure = kw.thesaurus_matching(abstracts_pure)\n",
    "keywords = kw.thesaurus_matching(keywords)\n",
    "keywords_index = kw.thesaurus_matching(keywords_index)\n",
    "\n",
    "# =============================================================================\n",
    "# Term to string corpus for co-word analysis\n",
    "# =============================================================================\n",
    "print(\"\\nTerm to string corpus for co-word analysis\")\n",
    "corpus_abstract = []\n",
    "for words in tqdm(abstracts, total=len(abstracts)):\n",
    "    corpus_abstract.append(' '.join(words))\n",
    "\n",
    "corpus_abstract_pure = []\n",
    "for words in tqdm(abstracts_pure, total=len(abstracts_pure)):\n",
    "    corpus_abstract_pure.append(' '.join(words))\n",
    "\n",
    "corpus_keywords = []\n",
    "for words in tqdm(keywords, total=len(keywords)):\n",
    "    corpus_keywords.append(';'.join(words))\n",
    "    \n",
    "corpus_keywords_index = []\n",
    "for words in tqdm(keywords_index, total=len(keywords_index)):\n",
    "    corpus_keywords_index.append(';'.join(words))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Remove substrings : \n",
    "#   be careful with this one! It might remove parts of a string or half of a word\n",
    "# =============================================================================\n",
    "thesaurus = pd.read_csv('data/thesaurus/to_remove.csv')\n",
    "thesaurus['alt'] = ''\n",
    "thesaurus = thesaurus.values.tolist()\n",
    "print(\"\\nRemoving substrings\")\n",
    "\n",
    "corpus_abstract_tr = []\n",
    "for paragraph in tqdm(corpus_abstract, total=len(corpus_abstract)):\n",
    "    paragraph = kw.filter_string(paragraph,thesaurus)\n",
    "    corpus_abstract_tr.append(paragraph)\n",
    "\n",
    "corpus_abstract_pure_tr = []\n",
    "for paragraph in tqdm(corpus_abstract_pure, total=len(corpus_abstract_pure)):\n",
    "    paragraph = kw.filter_string(paragraph,thesaurus)\n",
    "    corpus_abstract_pure_tr.append(paragraph)\n",
    "\n",
    "corpus_keywords_tr = []\n",
    "for paragraph in tqdm(corpus_keywords, total=len(corpus_keywords)):\n",
    "    paragraph = kw.filter_string(paragraph,thesaurus)\n",
    "    corpus_keywords_tr.append(paragraph)\n",
    "    \n",
    "corpus_keywords_index_tr = []\n",
    "for paragraph in tqdm(corpus_keywords_index, total=len(corpus_keywords_index)):\n",
    "    paragraph = kw.filter_string(paragraph,thesaurus)\n",
    "    corpus_keywords_index_tr.append(paragraph)\n",
    "    \n",
    "# =============================================================================\n",
    "# Final clean-up (double space and leading space)\n",
    "# =============================================================================\n",
    "tmp_data = []\n",
    "for paragraph in tqdm(corpus_abstract, total=len(corpus_abstract)):\n",
    "    paragraph = ' '.join(paragraph.split())\n",
    "    tmp_data.append(paragraph)\n",
    "corpus_abstract = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for paragraph in tqdm(corpus_abstract_tr, total=len(corpus_abstract_tr)):\n",
    "    paragraph = ' '.join(paragraph.split())\n",
    "    tmp_data.append(paragraph)\n",
    "corpus_abstract_tr = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for paragraph in tqdm(corpus_abstract_pure, total=len(corpus_abstract_pure)):\n",
    "    paragraph = ' '.join(paragraph.split())\n",
    "    tmp_data.append(paragraph)\n",
    "corpus_abstract_pure = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for paragraph in tqdm(corpus_abstract_pure_tr, total=len(corpus_abstract_pure_tr)):\n",
    "    paragraph = ' '.join(paragraph.split())\n",
    "    tmp_data.append(paragraph)\n",
    "corpus_abstract_pure_tr = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for paragraph in tqdm(corpus_keywords, total=len(corpus_keywords)):\n",
    "    paragraph = ' '.join(paragraph.split(' '))\n",
    "    paragraph = ';'.join(paragraph.split(';'))\n",
    "    tmp_data.append(paragraph)\n",
    "corpus_keywords = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for paragraph in tqdm(corpus_keywords_tr, total=len(corpus_keywords_tr)):\n",
    "    paragraph = ' '.join(paragraph.split(' '))\n",
    "    paragraph = ';'.join(paragraph.split(';'))\n",
    "    tmp_data.append(paragraph)\n",
    "corpus_keywords_tr = tmp_data.copy()\n",
    "del tmp_data\n",
    "tmp_data = []\n",
    "for paragraph in tqdm(corpus_keywords_index, total=len(corpus_keywords_index)):\n",
    "    paragraph = ' '.join(paragraph.split(' '))\n",
    "    paragraph = ';'.join(paragraph.split(';'))\n",
    "    tmp_data.append(paragraph)\n",
    "corpus_keywords_index = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for paragraph in tqdm(corpus_keywords_index_tr, total=len(corpus_keywords_index_tr)):\n",
    "    paragraph = ' '.join(paragraph.split(' '))\n",
    "    paragraph = ';'.join(paragraph.split(';'))\n",
    "    tmp_data.append(paragraph)\n",
    "corpus_keywords_index_tr = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "# =============================================================================\n",
    "# Write to disk\n",
    "# =============================================================================\n",
    "corpus_abstract = pd.DataFrame(corpus_abstract,columns=['words'])\n",
    "corpus_abstract_tr = pd.DataFrame(corpus_abstract_tr,columns=['words'])\n",
    "corpus_abstract_pure = pd.DataFrame(corpus_abstract_pure,columns=['words'])\n",
    "corpus_abstract_pure_tr = pd.DataFrame(corpus_abstract_pure_tr,columns=['words'])\n",
    "corpus_keywords = pd.DataFrame(corpus_keywords,columns=['words'])\n",
    "corpus_keywords_tr = pd.DataFrame(corpus_keywords_tr,columns=['words'])\n",
    "corpus_keywords_index = pd.DataFrame(corpus_keywords_index,columns=['words'])\n",
    "corpus_keywords_index_tr = pd.DataFrame(corpus_keywords_index_tr,columns=['words'])\n",
    "\n",
    "corpus_abstract.to_csv(root_dir+subdir+''+str(year_from)+'-'+str(year_to-1)+' abstract_title_keys',index=False,header=False)\n",
    "corpus_abstract_tr.to_csv(root_dir+subdir+''+str(year_from)+'-'+str(year_to-1)+' abstract_title_keys-terms_removed' ,index=False,header=False)\n",
    "corpus_abstract_pure.to_csv(root_dir+subdir+''+str(year_from)+'-'+str(year_to-1)+' abstract_title',index=False,header=False)\n",
    "corpus_abstract_pure_tr.to_csv(root_dir+subdir+''+str(year_from)+'-'+str(year_to-1)+' abstract_title-terms_removed',index=False,header=False)\n",
    "corpus_keywords.to_csv(root_dir+subdir+''+str(year_from)+'-'+str(year_to-1)+' keywords',index=False,header=False)\n",
    "corpus_keywords_tr.to_csv(root_dir+subdir+''+str(year_from)+'-'+str(year_to-1)+' keywords-terms_removed',index=False,header=False)\n",
    "corpus_keywords_index.to_csv(root_dir+subdir+''+str(year_from)+'-'+str(year_to-1)+' keywords_index',index=False,header=False)\n",
    "corpus_keywords_index_tr.to_csv(root_dir+subdir+''+str(year_from)+'-'+str(year_to-1)+' keywords_index-terms_removed',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
