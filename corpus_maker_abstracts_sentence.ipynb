{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sciosci.assets import keyword_assets as kw\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sahand/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_from = 1900\n",
    "year_to = 2020\n",
    "\n",
    "MAKE_SENTENCE_CORPUS = False\n",
    "MAKE_SENTENCE_CORPUS_ADVANCED = True\n",
    "MAKE_REGULAR_CORPUS = False\n",
    "GET_WORD_FREQ_IN_SENTENCE = True\n",
    "\n",
    "\n",
    "stops = ['a','an','we','result','however','yet','since','previously','although','propose','proposed','this']\n",
    "nltk.download('stopwords')\n",
    "stop_words = list(set(stopwords.words(\"english\")))+stops\n",
    "\n",
    "\n",
    "#data_path_rel = '/mnt/6016589416586D52/Users/z5204044/Documents/Dataset/WoS/Relevant Results _ DOI duplication - scopus keywords - document types - 31 july.csv'\n",
    "data_path_rel = '/home/sahand/Data/AI ALL 1900-2019 - reformat'\n",
    "data_full_relevant = pd.read_csv(data_path_rel)\n",
    "\n",
    "root_dir = '/home/sahand/Data/Corpus/'\n",
    "subdir = 'AL ALL lemmatized_stopword_removed_thesaurus_sep/' # no_lemmatization_no_stopwords\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Pre-Processing :\n",
    "Following tags requires WoS format. Change them otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2118312/2118312 [01:07<00:00, 31401.76it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "100%|██████████| 2118312/2118312 [02:51<00:00, 12325.42it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered = data_full_relevant.copy()\n",
    "data_filtered = data_filtered[pd.notnull(data_filtered['PY'])]\n",
    "\n",
    "data_filtered = data_filtered[data_filtered['PY'].astype('int')>year_from-1]\n",
    "data_filtered = data_filtered[data_filtered['PY'].astype('int')<year_to]\n",
    "\n",
    "# Remove columns without keywords/abstract list \n",
    "data_with_keywords = data_filtered[pd.notnull(data_filtered['DE'])]\n",
    "data_with_abstract = data_filtered[pd.notnull(data_filtered['AB'])]\n",
    "\n",
    "# Remove numbers from abstracts to eliminate decimal points and other unnecessary data\n",
    "data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_c(x) if pd.notnull(x) else np.nan).str.lower()\n",
    "# gc.collect()\n",
    "abstracts = []\n",
    "for abstract in tqdm(data_with_abstract['AB'].values.tolist()):\n",
    "    numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", abstract)\n",
    "    for number in numbers:\n",
    "        abstract = kw.find_and_remove_term(abstract,number)\n",
    "    abstracts.append(abstract)\n",
    "data_with_abstract['AB'] = abstracts.copy()\n",
    "del  abstracts\n",
    "\n",
    "year_list = pd.DataFrame(data_with_abstract['PY'].values.tolist(),columns=['year'])\n",
    "year_list.to_csv(root_dir+subdir+str(year_from)+'-'+str(year_to-1)+' corpus years',index=False) # Save year indices to disk for further use\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if MAKE_SENTENCE_CORPUS is True:\n",
    "    thesaurus = pd.read_csv('data/thesaurus/thesaurus_for_ai_keyword_with_().csv')\n",
    "    thesaurus = thesaurus.fillna('')\n",
    "    print(\"\\nSentence maker and thesaurus matching. \\nThis will take some time...\")\n",
    "    \n",
    "    data_with_abstract['AB_no_c'] = data_with_abstract['AB'].apply(lambda x: kw.find_and_remove_c(x) if pd.notnull(x) else np.nan)\n",
    "    sentence_corpus = []\n",
    "    \n",
    "    for index,row in tqdm(data_with_abstract.iterrows(),total=data_with_abstract.shape[0]):\n",
    "        words = re.split('( |\\\\n|\\.|\\?|!|:|;|,|_|\\[|\\])',row['AB_no_c'].lower())\n",
    "        new_words = []\n",
    "        year = row['PY']\n",
    "        flag_word_removed = False\n",
    "        for w_idx,word in enumerate(words):\n",
    "            if flag_word_removed is True:\n",
    "                if word==' ':\n",
    "                    flag_word_removed = False\n",
    "                    continue\n",
    "            if word in thesaurus['alt'].values.tolist():\n",
    "                word_old = word\n",
    "                buffer_word = word\n",
    "                word = thesaurus[thesaurus['alt']==word]['original'].values.tolist()[0]\n",
    "#                print(\"changed '\",word_old,\"' to '\",word,\"'.\")\n",
    "                \n",
    "            new_words.append(word)\n",
    "            \n",
    "        row = ''.join(new_words)\n",
    "        \n",
    "        sentences = re.split('(\\. |\\? |\\\\n)',row)\n",
    "        sentences = [i+j for i,j in zip(sentences[0::2], sentences[1::2])]\n",
    "        \n",
    "        for sentence_n in sentences:\n",
    "            sentence_corpus.append([index,sentence_n,year])\n",
    "    \n",
    "    sentence_corpus = pd.DataFrame(sentence_corpus,columns=['article_index','sentence','year'])\n",
    "    \n",
    "    sentence_corpus.to_csv(root_dir+subdir+str(year_from)+'-'+str(year_to-1)+' corpus sentences abstract-title',index=False,header=True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2118312/2118312 [00:07<00:00, 270454.44it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "100%|██████████| 2118312/2118312 [00:06<00:00, 317268.99it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "100%|██████████| 2118312/2118312 [00:07<00:00, 296542.02it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "100%|██████████| 2118312/2118312 [00:06<00:00, 339469.11it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "100%|██████████| 2118312/2118312 [00:07<00:00, 298901.29it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "100%|██████████| 2118312/2118312 [00:06<00:00, 338120.58it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "  0%|          | 0/2118312 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2118312/2118312 [04:20<00:00, 8137.48it/s]\n"
     ]
    }
   ],
   "source": [
    "if MAKE_SENTENCE_CORPUS_ADVANCED is True:\n",
    "    data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'et al.') if pd.notnull(x) else np.nan)\n",
    "    data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'eg.') if pd.notnull(x) else np.nan)\n",
    "    data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'ie.') if pd.notnull(x) else np.nan)\n",
    "    data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'vs.') if pd.notnull(x) else np.nan)\n",
    "    data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'ieee') if pd.notnull(x) else np.nan)\n",
    "    data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'fig.','figure') if pd.notnull(x) else np.nan)\n",
    "    data_with_abstract['TI_AB'] = data_with_abstract.TI.map(str) + \". \" + data_with_abstract.AB\n",
    "    data_fresh = data_with_abstract[['TI_AB','PY']].copy()\n",
    "    del data_with_abstract\n",
    "    gc.collect()\n",
    "    \n",
    "    data_tmp = data_fresh[1:10]\n",
    "    data_fresh[-2:-1]\n",
    "\n",
    "    print(\"\\nSentence extraction\")\n",
    "    sentences = []\n",
    "    years = []\n",
    "    indices = []\n",
    "    for index,row in tqdm(data_fresh.iterrows(),total=data_fresh.shape[0]):\n",
    "        abstract_str = row['TI_AB']\n",
    "        year = row['PY']\n",
    "        abstract_sentences = re.split('\\. |\\? |\\\\n',abstract_str)\n",
    "        length = len(abstract_sentences)\n",
    "        \n",
    "        sentences.extend(abstract_sentences)\n",
    "        years.extend([year for x in range(length)])\n",
    "        indices.extend([index for x in range(length)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 62/17531338 [00:00<7:51:33, 619.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 5379568/17531338 [15:31<34:15, 5911.79it/s]  "
     ]
    }
   ],
   "source": [
    "    print(\"\\nTokenizing\")\n",
    "    tmp = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        tmp.append(word_tokenize(sentence))\n",
    "    sentences = tmp.copy()\n",
    "    del tmp\n",
    "\n",
    "    print(\"\\nString pre processing for abstracts: lower and strip\")\n",
    "    sentences = [list(map(str.lower, x)) for x in sentences]\n",
    "    sentences = [list(map(str.strip, x)) for x in sentences]\n",
    "    \n",
    "    tmp = []\n",
    "    print(\"\\nString pre processing for abstracts: lemmatize and stop word removal\")\n",
    "    for string_list in tqdm(sentences, total=len(sentences)):\n",
    "        tmp_list = [kw.string_pre_processing(x,stemming_method='None',lemmatization=False,stop_word_removal=False,stop_words_extra=stops,verbose=False,download_nltk=False) for x in string_list]\n",
    "        tmp.append(tmp_list)\n",
    "    sentences = tmp.copy()\n",
    "    del tmp\n",
    "    \n",
    "    tmp = []\n",
    "    print(\"\\nString pre processing for abstracts: null word removal\")\n",
    "    for string_list in tqdm(sentences, total=len(sentences)):\n",
    "        tmp.append([x for x in string_list if x!=''])\n",
    "    sentences = tmp.copy()\n",
    "    del tmp\n",
    "    \n",
    "    print(\"\\nThesaurus matching\")\n",
    "    sentences = kw.thesaurus_matching(sentences)\n",
    "    \n",
    "    print(\"\\nStitiching words\")\n",
    "    tmp = []\n",
    "    for words in tqdm(sentences, total=len(sentences)):\n",
    "        tmp.append(' '.join(words))\n",
    "    sentences = tmp.copy()\n",
    "    del tmp\n",
    "    \n",
    "    sentence_df = pd.DataFrame(indices,columns=['article_index'])\n",
    "    sentence_df['sentence'] = sentences\n",
    "    sentence_df['year'] = years\n",
    "    sentence_df.to_csv(root_dir+subdir+str(year_from)+'-'+str(year_to-1)+' corpus sentences abstract-title',index=False,header=True)\n",
    "    \n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Corpus Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
