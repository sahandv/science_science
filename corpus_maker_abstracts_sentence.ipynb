{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sciosci.assets import keyword_assets as kw\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sahand/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_from = 1900\n",
    "year_to = 2020\n",
    "\n",
    "MAKE_SENTENCE_CORPUS = False\n",
    "MAKE_SENTENCE_CORPUS_ADVANCED = True\n",
    "MAKE_REGULAR_CORPUS = False\n",
    "GET_WORD_FREQ_IN_SENTENCE = True\n",
    "\n",
    "\n",
    "stops = ['a','an','we','result','however','yet','since','previously','although','propose','proposed','this']\n",
    "nltk.download('stopwords')\n",
    "stop_words = list(set(stopwords.words(\"english\")))+stops\n",
    "\n",
    "\n",
    "#data_path_rel = '/mnt/6016589416586D52/Users/z5204044/Documents/Dataset/WoS/Relevant Results _ DOI duplication - scopus keywords - document types - 31 july.csv'\n",
    "data_path_rel = '/home/sahand/Data/AI ALL 1900-2019 - reformat'\n",
    "data_full_relevant = pd.read_csv(data_path_rel)\n",
    "\n",
    "root_dir = '/home/sahand/Data/Corpus/'\n",
    "subdir = 'AL ALL lemmatized_stopword_removed_thesaurus_sep/' # no_lemmatization_no_stopwords\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Pre-Processing :\n",
    "Following tags requires WoS format. Change them otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2118312/2118312 [01:07<00:00, 31401.76it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "100%|██████████| 2118312/2118312 [02:51<00:00, 12325.42it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered = data_full_relevant.copy()\n",
    "data_filtered = data_filtered[pd.notnull(data_filtered['PY'])]\n",
    "\n",
    "data_filtered = data_filtered[data_filtered['PY'].astype('int')>year_from-1]\n",
    "data_filtered = data_filtered[data_filtered['PY'].astype('int')<year_to]\n",
    "\n",
    "# Remove columns without keywords/abstract list \n",
    "data_with_keywords = data_filtered[pd.notnull(data_filtered['DE'])]\n",
    "data_with_abstract = data_filtered[pd.notnull(data_filtered['AB'])]\n",
    "\n",
    "# Remove numbers from abstracts to eliminate decimal points and other unnecessary data\n",
    "data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_c(x) if pd.notnull(x) else np.nan).str.lower()\n",
    "# gc.collect()\n",
    "abstracts = []\n",
    "for abstract in tqdm(data_with_abstract['AB'].values.tolist()):\n",
    "    numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", abstract)\n",
    "    for number in numbers:\n",
    "        abstract = kw.find_and_remove_term(abstract,number)\n",
    "    abstracts.append(abstract)\n",
    "data_with_abstract['AB'] = abstracts.copy()\n",
    "del  abstracts\n",
    "\n",
    "year_list = pd.DataFrame(data_with_abstract['PY'].values.tolist(),columns=['year'])\n",
    "year_list.to_csv(root_dir+subdir+str(year_from)+'-'+str(year_to-1)+' corpus years',index=False) # Save year indices to disk for further use\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if MAKE_SENTENCE_CORPUS is True:\n",
    "    thesaurus = pd.read_csv('data/thesaurus/thesaurus_for_ai_keyword_with_().csv')\n",
    "    thesaurus = thesaurus.fillna('')\n",
    "    print(\"\\nSentence maker and thesaurus matching. \\nThis will take some time...\")\n",
    "    \n",
    "    data_with_abstract['AB_no_c'] = data_with_abstract['AB'].apply(lambda x: kw.find_and_remove_c(x) if pd.notnull(x) else np.nan)\n",
    "    sentence_corpus = []\n",
    "    \n",
    "    for index,row in tqdm(data_with_abstract.iterrows(),total=data_with_abstract.shape[0]):\n",
    "        words = re.split('( |\\\\n|\\.|\\?|!|:|;|,|_|\\[|\\])',row['AB_no_c'].lower())\n",
    "        new_words = []\n",
    "        year = row['PY']\n",
    "        flag_word_removed = False\n",
    "        for w_idx,word in enumerate(words):\n",
    "            if flag_word_removed is True:\n",
    "                if word==' ':\n",
    "                    flag_word_removed = False\n",
    "                    continue\n",
    "            if word in thesaurus['alt'].values.tolist():\n",
    "                word_old = word\n",
    "                buffer_word = word\n",
    "                word = thesaurus[thesaurus['alt']==word]['original'].values.tolist()[0]\n",
    "#                print(\"changed '\",word_old,\"' to '\",word,\"'.\")\n",
    "                \n",
    "            new_words.append(word)\n",
    "            \n",
    "        row = ''.join(new_words)\n",
    "        \n",
    "        sentences = re.split('(\\. |\\? |\\\\n)',row)\n",
    "        sentences = [i+j for i,j in zip(sentences[0::2], sentences[1::2])]\n",
    "        \n",
    "        for sentence_n in sentences:\n",
    "            sentence_corpus.append([index,sentence_n,year])\n",
    "    \n",
    "    sentence_corpus = pd.DataFrame(sentence_corpus,columns=['article_index','sentence','year'])\n",
    "    \n",
    "    sentence_corpus.to_csv(root_dir+subdir+str(year_from)+'-'+str(year_to-1)+' corpus sentences abstract-title',index=False,header=True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2118312/2118312 [00:07<00:00, 270454.44it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "100%|██████████| 2118312/2118312 [00:06<00:00, 317268.99it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "100%|██████████| 2118312/2118312 [00:07<00:00, 296542.02it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "100%|██████████| 2118312/2118312 [00:06<00:00, 339469.11it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "100%|██████████| 2118312/2118312 [00:07<00:00, 298901.29it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "100%|██████████| 2118312/2118312 [00:06<00:00, 338120.58it/s]\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/home/sahand/anaconda3/envs/sciosci/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "  0%|          | 0/2118312 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2118312/2118312 [04:20<00:00, 8137.48it/s]\n"
     ]
    }
   ],
   "source": [
    "if MAKE_SENTENCE_CORPUS_ADVANCED is True:\n",
    "    data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'et al.') if pd.notnull(x) else np.nan)\n",
    "    data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'eg.') if pd.notnull(x) else np.nan)\n",
    "    data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'ie.') if pd.notnull(x) else np.nan)\n",
    "    data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'vs.') if pd.notnull(x) else np.nan)\n",
    "    data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'ieee') if pd.notnull(x) else np.nan)\n",
    "    data_with_abstract['AB'] = data_with_abstract['AB'].progress_apply(lambda x: kw.find_and_remove_term(x,'fig.','figure') if pd.notnull(x) else np.nan)\n",
    "    data_with_abstract['TI_AB'] = data_with_abstract.TI.map(str) + \". \" + data_with_abstract.AB\n",
    "    data_fresh = data_with_abstract[['TI_AB','PY']].copy()\n",
    "    del data_with_abstract\n",
    "    gc.collect()\n",
    "    \n",
    "    data_tmp = data_fresh[1:10]\n",
    "    data_fresh[-2:-1]\n",
    "\n",
    "    print(\"\\nSentence extraction\")\n",
    "    sentences = []\n",
    "    years = []\n",
    "    indices = []\n",
    "    for index,row in tqdm(data_fresh.iterrows(),total=data_fresh.shape[0]):\n",
    "        abstract_str = row['TI_AB']\n",
    "        year = row['PY']\n",
    "        abstract_sentences = re.split('\\. |\\? |\\\\n',abstract_str)\n",
    "        length = len(abstract_sentences)\n",
    "        \n",
    "        sentences.extend(abstract_sentences)\n",
    "        years.extend([year for x in range(length)])\n",
    "        indices.extend([index for x in range(length)])\n",
    "        \n",
    "    print(\"\\nTokenizing\")\n",
    "    tmp = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        tmp.append(word_tokenize(sentence))\n",
    "    sentences = tmp.copy()\n",
    "    del tmp\n",
    "\n",
    "    print(\"\\nString pre processing for abstracts: lower and strip\")\n",
    "    sentences = [list(map(str.lower, x)) for x in sentences]\n",
    "    sentences = [list(map(str.strip, x)) for x in sentences]\n",
    "    \n",
    "    tmp = []\n",
    "    print(\"\\nString pre processing for abstracts: lemmatize and stop word removal\")\n",
    "    for string_list in tqdm(sentences, total=len(sentences)):\n",
    "        tmp_list = [kw.string_pre_processing(x,stemming_method='None',lemmatization=False,stop_word_removal=False,stop_words_extra=stops,verbose=False,download_nltk=False) for x in string_list]\n",
    "        tmp.append(tmp_list)\n",
    "    sentences = tmp.copy()\n",
    "    del tmp\n",
    "    \n",
    "    tmp = []\n",
    "    print(\"\\nString pre processing for abstracts: null word removal\")\n",
    "    for string_list in tqdm(sentences, total=len(sentences)):\n",
    "        tmp.append([x for x in string_list if x!=''])\n",
    "    sentences = tmp.copy()\n",
    "    del tmp\n",
    "    \n",
    "    print(\"\\nThesaurus matching\")\n",
    "    sentences = kw.thesaurus_matching(sentences)\n",
    "    \n",
    "    print(\"\\nStitiching words\")\n",
    "    tmp = []\n",
    "    for words in tqdm(sentences, total=len(sentences)):\n",
    "        tmp.append(' '.join(words))\n",
    "    sentences = tmp.copy()\n",
    "    del tmp\n",
    "    \n",
    "    sentence_df = pd.DataFrame(indices,columns=['article_index'])\n",
    "    sentence_df['sentence'] = sentences\n",
    "    sentence_df['year'] = years\n",
    "    sentence_df.to_csv(root_dir+subdir+str(year_from)+'-'+str(year_to-1)+' corpus sentences abstract-title',index=False,header=True)\n",
    "    \n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 62/17531338 [00:00<7:51:33, 619.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17531338/17531338 [52:06<00:00, 5607.28it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String pre processing for abstracts: lower and strip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 32/17531338 [00:00<15:21:17, 317.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String pre processing for abstracts: lemmatize and stop word removal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17531338/17531338 [8:01:48<00:00, 606.45it/s]   \n",
      "  0%|          | 0/17531338 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String pre processing for abstracts: null word removal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17531338/17531338 [01:57<00:00, 149619.63it/s]\n",
      "  0%|          | 0/17531338 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thesaurus matching\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17531338/17531338 [35:15<00:00, 8288.73it/s]  \n",
      "  0%|          | 74345/17531338 [00:00<00:23, 743448.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stitiching words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17531338/17531338 [00:15<00:00, 1113684.79it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Corpus Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_REGULAR_CORPUS is False:\n",
    "    sys.exit('Did not continue to create normal corpus. If you want a corpus, set it to True at init section.')\n",
    "# =============================================================================\n",
    "#   Get word frequency in sentence corpus -- OPTIONAL\n",
    "# =============================================================================\n",
    "if GET_WORD_FREQ_IN_SENTENCE is True:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    file = root_dir+subdir+str(year_from)+'-'+str(year_to-1)+' corpus sentences abstract-title'#'/home/sahand/GoogleDrive/Data/corpus/AI ALL/1900-2019 corpus sentences abstract-title'\n",
    "    file = pd.read_csv(file)\n",
    "    size = 500000\n",
    "    unique = []\n",
    "    for data_start_point in tqdm(np.arange(0,file.shape[0],size)):\n",
    "        if data_start_point+size<file.shape[0]:\n",
    "            end_point = data_start_point+size\n",
    "        else:\n",
    "            end_point = file.shape[0]-1\n",
    "    #    print(data_start_point,end_point)\n",
    "        str_split = list(file.sentence[data_start_point:end_point].str.split())\n",
    "        str_flat = pd.DataFrame([item for sublist in str_split for item in sublist])\n",
    "        str_flat.columns = ['words']\n",
    "        str_flat.head()\n",
    "    \n",
    "        unique = unique+list(str_flat.words.unique())\n",
    "    \n",
    "    unique = pd.DataFrame(unique)\n",
    "    unique.columns = ['words']\n",
    "    unique = list(unique.words.unique())\n",
    "    len(unique)\n",
    "# =============================================================================\n",
    "# Tokenize (Author Keywords and Abstracts+Titles)\n",
    "# =============================================================================\n",
    "abstracts = []\n",
    "keywords = []\n",
    "keywords_index = []\n",
    "abstracts_pure = []\n",
    "for index,paper in tqdm(data_with_abstract.iterrows(),total=data_with_abstract.shape[0]):\n",
    "    keywords_str = paper['DE']\n",
    "    keywords_index_str = paper['ID']\n",
    "    abstract_str = paper['AB']\n",
    "    title_str = paper['TI']\n",
    "    abstract_dic = word_tokenize(title_str+' '+abstract_str)\n",
    "    abstract_dic_pure = abstract_dic.copy()\n",
    "    if pd.notnull(paper['DE']):\n",
    "        keywords_dic = word_tokenize(keywords_str)\n",
    "        keywords.append(keywords_str.split(';'))\n",
    "        abstract_dic.extend(keywords_dic)\n",
    "    else:\n",
    "        keywords.append([])\n",
    "    if pd.notnull(paper['ID']):\n",
    "        keywords_index.append(keywords_index_str.split(';'))\n",
    "    else:\n",
    "        keywords_index.append([])\n",
    "    abstracts.append(abstract_dic)\n",
    "    abstracts_pure.append(abstract_dic_pure)\n",
    "\n",
    "# Add to main df. Not necessary\n",
    "data_with_abstract['AB_split'] = abstracts_pure \n",
    "data_with_abstract['AB_KW_split'] = abstracts\n",
    "\n",
    "# =============================================================================\n",
    "# Strip and lowe case \n",
    "# =============================================================================\n",
    "abstracts_pure = [list(map(str.strip, x)) for x in abstracts_pure]\n",
    "abstracts_pure = [list(map(str.lower, x)) for x in abstracts_pure]\n",
    "\n",
    "abstracts = [list(map(str.strip, x)) for x in abstracts]\n",
    "abstracts = [list(map(str.lower, x)) for x in abstracts]\n",
    "\n",
    "keywords = [list(map(str.strip, x)) for x in keywords]\n",
    "keywords = [list(map(str.lower, x)) for x in keywords]\n",
    "\n",
    "keywords_index = [list(map(str.strip, x)) for x in keywords_index]\n",
    "keywords_index = [list(map(str.lower, x)) for x in keywords_index]\n",
    "# =============================================================================\n",
    "# Pre Process \n",
    "# =============================================================================\n",
    "tmp_data = []\n",
    "print(\"\\nString pre processing for abstracts\")\n",
    "for string_list in tqdm(abstracts, total=len(abstracts)):\n",
    "    tmp_list = [kw.string_pre_processing(x,stemming_method='None',lemmatization=True,stop_word_removal=True,stop_words_extra=stops,verbose=False,download_nltk=False) for x in string_list]\n",
    "    tmp_data.append(tmp_list)\n",
    "abstracts = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for string_list in tqdm(abstracts_pure, total=len(abstracts_pure)):\n",
    "    tmp_list = [kw.string_pre_processing(x,stemming_method='None',lemmatization=True,stop_word_removal=True,stop_words_extra=stops,verbose=False,download_nltk=False) for x in string_list]\n",
    "    tmp_data.append(tmp_list)\n",
    "abstracts_pure = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "print(\"\\nString pre processing for keywords\")\n",
    "tmp_data = []\n",
    "for string_list in tqdm(keywords, total=len(keywords)):\n",
    "    tmp_list = []\n",
    "    for string in string_list:\n",
    "        tmp_sub_list = string.split()\n",
    "        tmp_list.append(' '.join([kw.string_pre_processing(x,stemming_method='None',lemmatization=True,stop_word_removal=True,stop_words_extra=stops,verbose=False,download_nltk=False) for x in tmp_sub_list]))\n",
    "    tmp_data.append(tmp_list)\n",
    "keywords = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for string_list in tqdm(keywords_index, total=len(keywords_index)):\n",
    "    tmp_list = []\n",
    "    for string in string_list:\n",
    "        tmp_sub_list = string.split()\n",
    "        tmp_list.append(' '.join([kw.string_pre_processing(x,stemming_method='None',lemmatization=True,stop_word_removal=True,stop_words_extra=stops,verbose=False,download_nltk=False) for x in tmp_sub_list]))\n",
    "    tmp_data.append(tmp_list)\n",
    "keywords_index = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "#tmp_data = []\n",
    "#for string_list in tqdm(keywords, total=len(keywords)):\n",
    "#    tmp_list = []\n",
    "#    for sub_string_list in string_list:\n",
    "#        tmp_list.append(' '.join(sub_string_list))\n",
    "#    tmp_data.append(tmp_list)\n",
    "#keywords = tmp_data.copy()\n",
    "#del tmp_data\n",
    "\n",
    "# =============================================================================\n",
    "# Clean-up dead words\n",
    "# =============================================================================\n",
    "tmp_data = []\n",
    "for string_list in tqdm(abstracts, total=len(abstracts)):\n",
    "    tmp_data.append([x for x in string_list if x!=''])\n",
    "abstracts = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for string_list in tqdm(abstracts_pure, total=len(abstracts_pure)):\n",
    "    tmp_data.append([x for x in string_list if x!=''])\n",
    "abstracts_pure = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for string_list in tqdm(keywords, total=len(keywords)):\n",
    "    tmp_data.append([x for x in string_list if x!=''])\n",
    "keywords = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for string_list in tqdm(keywords_index, total=len(keywords_index)):\n",
    "    tmp_data.append([x for x in string_list if x!=''])\n",
    "keywords_index = tmp_data.copy()\n",
    "del tmp_data\n",
    "# =============================================================================\n",
    "# Break-down abstracts again\n",
    "# =============================================================================\n",
    "tmp_data = []\n",
    "for abstract in tqdm(abstracts):\n",
    "    words = []\n",
    "    for word in abstract:\n",
    "        words = words+word.split()\n",
    "    tmp_data.append(words)\n",
    "abstracts = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for abstract in tqdm(abstracts_pure):\n",
    "    words = []\n",
    "    for word in abstract:\n",
    "        words = words+word.split()\n",
    "    tmp_data.append(words)\n",
    "abstracts_pure = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "# =============================================================================\n",
    "# Thesaurus matching\n",
    "# =============================================================================\n",
    "print(\"\\nThesaurus matching\")\n",
    "\n",
    "abstracts_backup = abstracts.copy()\n",
    "abstracts_pure_backup = abstracts_pure.copy()\n",
    "keywords_backup = keywords.copy()\n",
    "keywords_index_backup = keywords_index.copy()\n",
    "\n",
    "abstracts = abstracts_backup.copy()\n",
    "abstracts_pure = abstracts_pure_backup.copy()\n",
    "keywords = keywords_backup.copy()\n",
    "keywords_index = keywords_index_backup.copy()\n",
    "\n",
    "abstracts = kw.thesaurus_matching(abstracts)\n",
    "abstracts_pure = kw.thesaurus_matching(abstracts_pure)\n",
    "keywords = kw.thesaurus_matching(keywords)\n",
    "keywords_index = kw.thesaurus_matching(keywords_index)\n",
    "\n",
    "# =============================================================================\n",
    "# Term to string corpus for co-word analysis\n",
    "# =============================================================================\n",
    "print(\"\\nTerm to string corpus for co-word analysis\")\n",
    "corpus_abstract = []\n",
    "for words in tqdm(abstracts, total=len(abstracts)):\n",
    "    corpus_abstract.append(' '.join(words))\n",
    "\n",
    "corpus_abstract_pure = []\n",
    "for words in tqdm(abstracts_pure, total=len(abstracts_pure)):\n",
    "    corpus_abstract_pure.append(' '.join(words))\n",
    "\n",
    "corpus_keywords = []\n",
    "for words in tqdm(keywords, total=len(keywords)):\n",
    "    corpus_keywords.append(';'.join(words))\n",
    "    \n",
    "corpus_keywords_index = []\n",
    "for words in tqdm(keywords_index, total=len(keywords_index)):\n",
    "    corpus_keywords_index.append(';'.join(words))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Remove substrings : \n",
    "#   be careful with this one! It might remove parts of a string or half of a word\n",
    "# =============================================================================\n",
    "thesaurus = pd.read_csv('data/thesaurus/to_remove.csv')\n",
    "thesaurus['alt'] = ''\n",
    "thesaurus = thesaurus.values.tolist()\n",
    "print(\"\\nRemoving substrings\")\n",
    "\n",
    "corpus_abstract_tr = []\n",
    "for paragraph in tqdm(corpus_abstract, total=len(corpus_abstract)):\n",
    "    paragraph = kw.filter_string(paragraph,thesaurus)\n",
    "    corpus_abstract_tr.append(paragraph)\n",
    "\n",
    "corpus_abstract_pure_tr = []\n",
    "for paragraph in tqdm(corpus_abstract_pure, total=len(corpus_abstract_pure)):\n",
    "    paragraph = kw.filter_string(paragraph,thesaurus)\n",
    "    corpus_abstract_pure_tr.append(paragraph)\n",
    "\n",
    "corpus_keywords_tr = []\n",
    "for paragraph in tqdm(corpus_keywords, total=len(corpus_keywords)):\n",
    "    paragraph = kw.filter_string(paragraph,thesaurus)\n",
    "    corpus_keywords_tr.append(paragraph)\n",
    "    \n",
    "corpus_keywords_index_tr = []\n",
    "for paragraph in tqdm(corpus_keywords_index, total=len(corpus_keywords_index)):\n",
    "    paragraph = kw.filter_string(paragraph,thesaurus)\n",
    "    corpus_keywords_index_tr.append(paragraph)\n",
    "    \n",
    "# =============================================================================\n",
    "# Final clean-up (double space and leading space)\n",
    "# =============================================================================\n",
    "tmp_data = []\n",
    "for paragraph in tqdm(corpus_abstract, total=len(corpus_abstract)):\n",
    "    paragraph = ' '.join(paragraph.split())\n",
    "    tmp_data.append(paragraph)\n",
    "corpus_abstract = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for paragraph in tqdm(corpus_abstract_tr, total=len(corpus_abstract_tr)):\n",
    "    paragraph = ' '.join(paragraph.split())\n",
    "    tmp_data.append(paragraph)\n",
    "corpus_abstract_tr = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for paragraph in tqdm(corpus_abstract_pure, total=len(corpus_abstract_pure)):\n",
    "    paragraph = ' '.join(paragraph.split())\n",
    "    tmp_data.append(paragraph)\n",
    "corpus_abstract_pure = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for paragraph in tqdm(corpus_abstract_pure_tr, total=len(corpus_abstract_pure_tr)):\n",
    "    paragraph = ' '.join(paragraph.split())\n",
    "    tmp_data.append(paragraph)\n",
    "corpus_abstract_pure_tr = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for paragraph in tqdm(corpus_keywords, total=len(corpus_keywords)):\n",
    "    paragraph = ' '.join(paragraph.split(' '))\n",
    "    paragraph = ';'.join(paragraph.split(';'))\n",
    "    tmp_data.append(paragraph)\n",
    "corpus_keywords = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for paragraph in tqdm(corpus_keywords_tr, total=len(corpus_keywords_tr)):\n",
    "    paragraph = ' '.join(paragraph.split(' '))\n",
    "    paragraph = ';'.join(paragraph.split(';'))\n",
    "    tmp_data.append(paragraph)\n",
    "corpus_keywords_tr = tmp_data.copy()\n",
    "del tmp_data\n",
    "tmp_data = []\n",
    "for paragraph in tqdm(corpus_keywords_index, total=len(corpus_keywords_index)):\n",
    "    paragraph = ' '.join(paragraph.split(' '))\n",
    "    paragraph = ';'.join(paragraph.split(';'))\n",
    "    tmp_data.append(paragraph)\n",
    "corpus_keywords_index = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "tmp_data = []\n",
    "for paragraph in tqdm(corpus_keywords_index_tr, total=len(corpus_keywords_index_tr)):\n",
    "    paragraph = ' '.join(paragraph.split(' '))\n",
    "    paragraph = ';'.join(paragraph.split(';'))\n",
    "    tmp_data.append(paragraph)\n",
    "corpus_keywords_index_tr = tmp_data.copy()\n",
    "del tmp_data\n",
    "\n",
    "# =============================================================================\n",
    "# Write to disk\n",
    "# =============================================================================\n",
    "corpus_abstract = pd.DataFrame(corpus_abstract,columns=['words'])\n",
    "corpus_abstract_tr = pd.DataFrame(corpus_abstract_tr,columns=['words'])\n",
    "corpus_abstract_pure = pd.DataFrame(corpus_abstract_pure,columns=['words'])\n",
    "corpus_abstract_pure_tr = pd.DataFrame(corpus_abstract_pure_tr,columns=['words'])\n",
    "corpus_keywords = pd.DataFrame(corpus_keywords,columns=['words'])\n",
    "corpus_keywords_tr = pd.DataFrame(corpus_keywords_tr,columns=['words'])\n",
    "corpus_keywords_index = pd.DataFrame(corpus_keywords_index,columns=['words'])\n",
    "corpus_keywords_index_tr = pd.DataFrame(corpus_keywords_index_tr,columns=['words'])\n",
    "\n",
    "corpus_abstract.to_csv(root_dir+subdir+''+str(year_from)+'-'+str(year_to-1)+' abstract_title_keys',index=False,header=False)\n",
    "corpus_abstract_tr.to_csv(root_dir+subdir+''+str(year_from)+'-'+str(year_to-1)+' abstract_title_keys-terms_removed' ,index=False,header=False)\n",
    "corpus_abstract_pure.to_csv(root_dir+subdir+''+str(year_from)+'-'+str(year_to-1)+' abstract_title',index=False,header=False)\n",
    "corpus_abstract_pure_tr.to_csv(root_dir+subdir+''+str(year_from)+'-'+str(year_to-1)+' abstract_title-terms_removed',index=False,header=False)\n",
    "corpus_keywords.to_csv(root_dir+subdir+''+str(year_from)+'-'+str(year_to-1)+' keywords',index=False,header=False)\n",
    "corpus_keywords_tr.to_csv(root_dir+subdir+''+str(year_from)+'-'+str(year_to-1)+' keywords-terms_removed',index=False,header=False)\n",
    "corpus_keywords_index.to_csv(root_dir+subdir+''+str(year_from)+'-'+str(year_to-1)+' keywords_index',index=False,header=False)\n",
    "corpus_keywords_index_tr.to_csv(root_dir+subdir+''+str(year_from)+'-'+str(year_to-1)+' keywords_index-terms_removed',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
