{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sahandv/scioscipred/blob/master/BERT_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijoObp_DTMSD"
   },
   "source": [
    "# BERT WORD EMBEDDING\n",
    "\n",
    "Credits to https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sahand/Projects/science_science\n",
      "env: /home/sahand/anaconda3/envs/tf-1/bin/python\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "import sys\n",
    "import os\n",
    "print('env:',sys.executable)\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JyUry-7GTUFy"
   },
   "source": [
    "### Mount Google Drive\n",
    "If on colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l3c1LxI8TQuy",
    "outputId": "8f63d47f-db5f-45bb-e99a-0c3e619ab005"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LvD9I1PhTbDS",
    "outputId": "20f779cb-0faa-4c31-d81b-626bdc4c46ed"
   },
   "outputs": [],
   "source": [
    "!ls 'drive/My Drive/Data/1990-2018 corpus sentences abstract-title'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWIJmpvETXoC"
   },
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "hFu5wVYbUDuC",
    "outputId": "69adfaa9-b89d-49f6-89c9-5572bca74fe6"
   },
   "outputs": [],
   "source": [
    "!rm -rf bert\n",
    "!git clone https://github.com/google-research/bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MUFmzDnwTjmC",
    "outputId": "166a4dff-245a-44dc-efc4-8ba8480a8405"
   },
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-LgLOc5jTn5R"
   },
   "source": [
    "### Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t40Yz-6STr1a"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import codecs\n",
    "import collections\n",
    "import json\n",
    "import re\n",
    "\n",
    "import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from BERT import modeling\n",
    "from BERT import tokenization\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If on colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "sys.path.append('bert/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-N4KEslNTw75"
   },
   "source": [
    "### Make sure colab TPU is being used\n",
    "If on colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "89-CDu2KTxQx",
    "outputId": "3f8bdf4c-f041-42d5-ddf6-49c34ef38e17"
   },
   "outputs": [],
   "source": [
    "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
    "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "print('TPU address is', TPU_ADDRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "GKPbwFaOU7SB",
    "outputId": "593057a5-56e4-448d-9f27-8b7cdf85de09"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "with tf.Session(TPU_ADDRESS) as session:\n",
    "  print('TPU devices:')\n",
    "  pprint.pprint(session.list_devices())\n",
    "\n",
    "  # Upload credentials to TPU.\n",
    "  with open('/content/adc.json', 'r') as f:\n",
    "    auth_info = json.load(f)\n",
    "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
    "  # Now credentials are set for all future sessions on this TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9o5R71SVHdY"
   },
   "source": [
    "### Select which BERT model we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "v2gvHlriVIrY",
    "outputId": "5f0bc4c6-8e26-4071-df48-2d110cf7c867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** BERT pretrained directory: /home/sahand/Projects/science_science/BERT_weights/scibert_scivocab_uncased *****\n",
      "/bin/bash: gsutil: command not found\r\n"
     ]
    }
   ],
   "source": [
    "# Available pretrained model checkpoints:\n",
    "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
    "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
    "#   cased_L-12_H-768_A-12: cased BERT large model\n",
    "BERT_MODEL = 'scibert_scivocab_uncased'\n",
    "BERT_PRETRAINED_DIR = '/home/sahand/Projects/science_science/BERT_weights/' + BERT_MODEL\n",
    "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
    "!gsutil ls $BERT_PRETRAINED_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHt6zH_JX8d7"
   },
   "source": [
    "### Define some global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjqpTzIPX8ur"
   },
   "outputs": [],
   "source": [
    "LAYERS = [-1,-2,-3,-4]\n",
    "NUM_TPU_CORES = 272\n",
    "MAX_SEQ_LENGTH = 500\n",
    "BERT_CONFIG = BERT_PRETRAINED_DIR + '/bert_config.json'\n",
    "CHKPT_DIR = BERT_PRETRAINED_DIR + '/bert_model.ckpt'\n",
    "VOCAB_FILE = BERT_PRETRAINED_DIR + '/vocab.txt'\n",
    "INIT_CHECKPOINT = BERT_PRETRAINED_DIR + '/bert_model.ckpt'\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Astne8rYYChc"
   },
   "source": [
    "### Define wrapper classes for the input before processing and after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UUEDmql4YCtb"
   },
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "\n",
    "  def __init__(self, unique_id, text_a, text_b=None):\n",
    "    self.unique_id = unique_id\n",
    "    self.text_a = text_a\n",
    "    self.text_b = text_b\n",
    "    \n",
    "class InputFeatures(object):\n",
    "  \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "  def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
    "    self.unique_id = unique_id\n",
    "    self.tokens = tokens\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.input_type_ids = input_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j7wN0LtWYRkb"
   },
   "source": [
    "### Set up our Estimator\n",
    "\n",
    "The estimator is a wrapper around the BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQtArQ6TYRpz"
   },
   "outputs": [],
   "source": [
    "def input_fn_builder(features, seq_length):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  all_unique_ids = []\n",
    "  all_input_ids = []\n",
    "  all_input_mask = []\n",
    "  all_input_type_ids = []\n",
    "\n",
    "  for feature in features:\n",
    "    all_unique_ids.append(feature.unique_id)\n",
    "    all_input_ids.append(feature.input_ids)\n",
    "    all_input_mask.append(feature.input_mask)\n",
    "    all_input_type_ids.append(feature.input_type_ids)\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    batch_size = params[\"batch_size\"]\n",
    "\n",
    "    num_examples = len(features)\n",
    "\n",
    "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
    "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
    "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
    "    d = tf.data.Dataset.from_tensor_slices({\n",
    "        \"unique_ids\":\n",
    "            tf.constant(all_unique_ids, shape=[num_examples], dtype=tf.int32),\n",
    "        \"input_ids\":\n",
    "            tf.constant(\n",
    "                all_input_ids, shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"input_mask\":\n",
    "            tf.constant(\n",
    "                all_input_mask,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"input_type_ids\":\n",
    "            tf.constant(\n",
    "                all_input_type_ids,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "    })\n",
    "\n",
    "    d = d.batch(batch_size=batch_size, drop_remainder=False)\n",
    "    return d\n",
    "\n",
    "  return input_fn\n",
    "  \n",
    "def model_fn_builder(bert_config, init_checkpoint, layer_indexes, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    unique_ids = features[\"unique_ids\"]\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    input_type_ids = features[\"input_type_ids\"]\n",
    "\n",
    "    # The model is defined here\n",
    "    model = modeling.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=False,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        token_type_ids=input_type_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "      raise ValueError(\"Only PREDICT modes are supported: %s\" % (mode))\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    scaffold_fn = None\n",
    "    (assignment_map,\n",
    "     initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(\n",
    "         tvars, init_checkpoint)\n",
    "    if use_tpu:\n",
    "\n",
    "      def tpu_scaffold():\n",
    "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "        return tf.train.Scaffold()\n",
    "\n",
    "      scaffold_fn = tpu_scaffold\n",
    "    else:\n",
    "      tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "    tf.logging.info(\"**** Trainable Variables ****\")\n",
    "    for var in tvars:\n",
    "      init_string = \"\"\n",
    "      if var.name in initialized_variable_names:\n",
    "        init_string = \", *INIT_FROM_CKPT*\"\n",
    "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "                      init_string)\n",
    "\n",
    "    all_layers = model.get_all_encoder_layers()\n",
    "\n",
    "    predictions = {\n",
    "        \"unique_id\": unique_ids,\n",
    "    }\n",
    "\n",
    "    # Tredictions from the top 4 layers are extracted here\n",
    "    for (i, layer_index) in enumerate(layer_indexes):\n",
    "      predictions[\"layer_output_%d\" % i] = all_layers[layer_index]\n",
    "\n",
    "    output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "        mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n",
    "    return output_spec\n",
    "\n",
    "  return model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wNuP-aQdY53S"
   },
   "source": [
    "### Transform InputExamples to InputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dK58xgpLY6Ez"
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, seq_length, tokenizer):\n",
    "  \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "  features = []\n",
    "  for (ex_index, example) in enumerate(examples):\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "      tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "    if tokens_b:\n",
    "      # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "      # length is less than the specified length.\n",
    "      # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "      _truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)\n",
    "    else:\n",
    "      # Account for [CLS] and [SEP] with \"- 2\"\n",
    "      if len(tokens_a) > seq_length - 2:\n",
    "        tokens_a = tokens_a[0:(seq_length - 2)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0     0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = []\n",
    "    input_type_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    input_type_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "      tokens.append(token)\n",
    "      input_type_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "    if tokens_b:\n",
    "      for token in tokens_b:\n",
    "        tokens.append(token)\n",
    "        input_type_ids.append(1)\n",
    "      tokens.append(\"[SEP]\")\n",
    "      input_type_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < seq_length:\n",
    "      input_ids.append(0)\n",
    "      input_mask.append(0)\n",
    "      input_type_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == seq_length\n",
    "    assert len(input_mask) == seq_length\n",
    "    assert len(input_type_ids) == seq_length\n",
    "\n",
    "    if ex_index < 5:\n",
    "      tf.logging.info(\"*** Example ***\")\n",
    "      tf.logging.info(\"unique_id: %s\" % (example.unique_id))\n",
    "      tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "          [tokenization.printable_text(x) for x in tokens]))\n",
    "      tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "      tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "      tf.logging.info(\n",
    "          \"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
    "\n",
    "    features.append(\n",
    "        InputFeatures(\n",
    "            unique_id=example.unique_id,\n",
    "            tokens=tokens,\n",
    "            input_ids=input_ids,\n",
    "            input_mask=input_mask,\n",
    "            input_type_ids=input_type_ids))\n",
    "  return features\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "  # This is a simple heuristic which will always truncate the longer sequence\n",
    "  # one token at a time. This makes more sense than truncating an equal percent\n",
    "  # of tokens from each, since if one sequence is very short then each token\n",
    "  # that's truncated likely contains more information than a longer sequence.\n",
    "  while True:\n",
    "    total_length = len(tokens_a) + len(tokens_b)\n",
    "    if total_length <= max_length:\n",
    "      break\n",
    "    if len(tokens_a) > len(tokens_b):\n",
    "      tokens_a.pop()\n",
    "    else:\n",
    "      tokens_b.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QbVmmo1iZycC"
   },
   "source": [
    "### Converting a normal string sequence to InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PBmwXYBZyn4"
   },
   "outputs": [],
   "source": [
    "def read_sequence(input_sentences):\n",
    "  examples = []\n",
    "  unique_id = 0\n",
    "  for sentence in input_sentences:\n",
    "    line = tokenization.convert_to_unicode(sentence)\n",
    "    examples.append(InputExample(unique_id=unique_id, text_a=line))\n",
    "    unique_id += 1\n",
    "  return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eeG-WZ9aZyzY"
   },
   "source": [
    "### Processing functions for inputs\n",
    "\n",
    "A function which accepts *an array of strings* as a parameter and the desired *dimension (max 768) of the embedding output* and returns a dictionary with the token as key and the embedding vector as value.\n",
    "\n",
    "The function builds the estimator and invokes a prediction based on the given inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(input_text, dim=768,output_labels=None,output_file='embeddings.json',\n",
    "             start_from_idx=0,use_TPU=False,doc_embedding=True):\n",
    "    \n",
    "    layer_indexes = LAYERS\n",
    "    bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\n",
    "    tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=VOCAB_FILE, do_lower_case=True)\n",
    "\n",
    "    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
    "    run_config = tf.contrib.tpu.RunConfig(\n",
    "      cluster=tpu_cluster_resolver,\n",
    "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "          num_shards=NUM_TPU_CORES,\n",
    "          per_host_input_for_training=is_per_host))\n",
    "\n",
    "    examples = read_sequence(input_text)\n",
    "    print(\"example len is \",len(examples))\n",
    "\n",
    "    features = convert_examples_to_features(\n",
    "      examples=examples, seq_length=MAX_SEQ_LENGTH, tokenizer=tokenizer)\n",
    "    print('\\nsample features are:',features[1].tokens)\n",
    "\n",
    "    \n",
    "    unique_id_to_feature = {}\n",
    "    for feature in features:\n",
    "        unique_id_to_feature[feature.unique_id] = feature\n",
    "\n",
    "    print(\"features len is \",len(features))\n",
    "    username = input(\"Press return to continue...\")\n",
    "    \n",
    "    model_fn = model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      init_checkpoint=INIT_CHECKPOINT,\n",
    "      layer_indexes=layer_indexes,\n",
    "      use_tpu=use_TPU,\n",
    "      use_one_hot_embeddings=True)\n",
    "\n",
    "    # If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "    # or GPU.\n",
    "    estimator = tf.contrib.tpu.TPUEstimator(\n",
    "      use_tpu=use_TPU,\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      predict_batch_size=BATCH_SIZE,\n",
    "      train_batch_size=BATCH_SIZE)\n",
    "\n",
    "    input_fn = input_fn_builder(\n",
    "      features=features, seq_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "    # Get features\n",
    "    output_dict = {}\n",
    "    all_embeddings = []\n",
    "    counter = 0\n",
    "    for result in estimator.predict(input_fn, yield_single_examples=True):\n",
    "        unique_id = int(result[\"unique_id\"])\n",
    "        feature = unique_id_to_feature[unique_id]\n",
    "        output = collections.OrderedDict()\n",
    "        sentence_dict = {}\n",
    "        for (i, token) in enumerate(feature.tokens):\n",
    "            layers = []\n",
    "            for (j, layer_index) in enumerate(layer_indexes):\n",
    "                layer_output = result[\"layer_output_%d\" % j]\n",
    "                layer_output_flat = np.array([x for x in layer_output[i:(i + 1)].flat])\n",
    "                layers.append(layer_output_flat)\n",
    "                \n",
    "            if doc_embedding==True:\n",
    "                if token == '[CLS]':\n",
    "                    output[token] = list(sum(layers)[:dim]) # You can also concatenate or average layers here\n",
    "                    sentence_dict.update({token:str(output[token])})\n",
    "            else:\n",
    "                output[token] = list(sum(layers)[:dim]) # You can also concatenate or average layers here\n",
    "                sentence_dict.update({token:str(output[token])})\n",
    "                \n",
    "        output_dict[str(counter+start_from_idx)] = {'article':str(output_labels[counter]),'vectors':sentence_dict}\n",
    "        counter += 1\n",
    "\n",
    "    print('estimator is finalized and is at sentence', counter)\n",
    "    print('witing to disk...')\n",
    "\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(output_dict, json_file)\n",
    "\n",
    "    print('Finished writing embeddings with len',len(output_dict))\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x54s2Th-Zy6Y"
   },
   "outputs": [],
   "source": [
    "def get_features(input_text, dim=768,output_labels=None,output_file='embeddings.json',\n",
    "                 start_from_idx=0,use_TPU=False):\n",
    "  layer_indexes = LAYERS\n",
    "\n",
    "  bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\n",
    "\n",
    "  tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=VOCAB_FILE, do_lower_case=True)\n",
    "\n",
    "  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
    "  run_config = tf.contrib.tpu.RunConfig(\n",
    "      cluster=tpu_cluster_resolver,\n",
    "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "          num_shards=NUM_TPU_CORES,\n",
    "          per_host_input_for_training=is_per_host))\n",
    "\n",
    "  examples = read_sequence(input_text)\n",
    "  print(\"example len is \",len(examples))\n",
    "\n",
    "  features = convert_examples_to_features(\n",
    "      examples=examples, seq_length=MAX_SEQ_LENGTH, tokenizer=tokenizer)\n",
    "\n",
    "  unique_id_to_feature = {}\n",
    "  for feature in features:\n",
    "    unique_id_to_feature[feature.unique_id] = feature\n",
    "\n",
    "  print(\"features len is \",len(features))\n",
    "\n",
    "  model_fn = model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      init_checkpoint=INIT_CHECKPOINT,\n",
    "      layer_indexes=layer_indexes,\n",
    "      use_tpu=use_TPU,\n",
    "      use_one_hot_embeddings=True)\n",
    "\n",
    "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "  # or GPU.\n",
    "  estimator = tf.contrib.tpu.TPUEstimator(\n",
    "      use_tpu=use_TPU,\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      predict_batch_size=BATCH_SIZE,\n",
    "      train_batch_size=BATCH_SIZE)\n",
    "\n",
    "  input_fn = input_fn_builder(\n",
    "      features=features, seq_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "  # Get features\n",
    "  output_dict = {}\n",
    "  all_embeddings = []\n",
    "  counter = 0\n",
    "  for result in estimator.predict(input_fn, yield_single_examples=True):\n",
    "    unique_id = int(result[\"unique_id\"])\n",
    "    feature = unique_id_to_feature[unique_id]\n",
    "    output = collections.OrderedDict()\n",
    "    sentence_dict = {}\n",
    "    for (i, token) in enumerate(feature.tokens):\n",
    "      layers = []\n",
    "      for (j, layer_index) in enumerate(layer_indexes):\n",
    "        layer_output = result[\"layer_output_%d\" % j]\n",
    "        layer_output_flat = np.array([x for x in layer_output[i:(i + 1)].flat])\n",
    "        layers.append(layer_output_flat)\n",
    "      output[token] = list(sum(layers)[:dim]) # You can also concatenate or average layers here \n",
    "      sentence_dict.update({token:str(output[token])})\n",
    "\n",
    "    output_dict[str(counter+start_from_idx)] = {'article':str(output_labels[counter]),'vectors':sentence_dict}\n",
    "    counter += 1\n",
    "    \n",
    "  print('estimator is finalized and is at sentence', counter)\n",
    "  print('witing to disk...')\n",
    "\n",
    "  with open(output_file, 'w') as json_file:\n",
    "    json.dump(output_dict, json_file)\n",
    "\n",
    "  print('Finished writing embeddings with len',len(output_dict))\n",
    "\n",
    "  return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_DSh2GLarwP"
   },
   "source": [
    "### Run the model and get the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DHM-FKqK9S_e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lighting system disclosed comprising plurality...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>photographic device eliminates photometric err...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>print demand camera system camera unit incorpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>simplified camera mechanism method allowing op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beam splitter digital camera split light passe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19705</th>\n",
       "      <td>display engine video graphic system includes p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19706</th>\n",
       "      <td>facilitating interaction may enabled communica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19707</th>\n",
       "      <td>present invention generally directed system me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19708</th>\n",
       "      <td>disclosed system producing image including app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19709</th>\n",
       "      <td>fully programmable graphic microprocessor embo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19710 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence\n",
       "0      lighting system disclosed comprising plurality...\n",
       "1      photographic device eliminates photometric err...\n",
       "2      print demand camera system camera unit incorpo...\n",
       "3      simplified camera mechanism method allowing op...\n",
       "4      beam splitter digital camera split light passe...\n",
       "...                                                  ...\n",
       "19705  display engine video graphic system includes p...\n",
       "19706  facilitating interaction may enabled communica...\n",
       "19707  present invention generally directed system me...\n",
       "19708  disclosed system producing image including app...\n",
       "19709  fully programmable graphic microprocessor embo...\n",
       "\n",
       "[19710 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/mnt/16A4A9BCA4A99EAD/GoogleDrive/Data/Corpus/KPRIS/clean/abstract_title deflemm',names=['sentence'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8AVBeafE_R7S",
    "outputId": "6c449112-3b1d-4907-b5fe-1276372b9b4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sahand/Projects/science_science/BERT/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example len is  19710\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 0\n",
      "INFO:tensorflow:tokens: [CLS] lighting system disclosed comprising plural ##ity image projection lighting device including first image projection lighting device central controller communication system permit central controller communicate plural ##ity image projection lighting device first image projection lighting device comprised camera light valve wherein light valve used project first projected image onto projection surface camera first image projection light device capture least portion first projected image pr [SEP]\n",
      "INFO:tensorflow:input_ids: 102 16703 429 22118 12201 22533 208 1572 6815 16703 3264 1471 705 1572 6815 16703 3264 2435 4919 2498 429 9362 2435 4919 11160 22533 208 1572 6815 16703 3264 705 1572 6815 16703 3264 10508 5859 2011 9042 19749 2011 9042 501 2575 705 11643 1572 5095 6815 1437 5859 705 1572 6815 2011 3264 4083 1823 6492 705 11643 1572 492 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 1\n",
      "INFO:tensorflow:tokens: [CLS] photograph ##ic device eliminates photometric error occur photo ##metry obtains accurate exposure shorten ##s time required photo ##metry device includes orientation position detector photograph ##ic mode sett ##er output photograph ##ic condition based set photograph ##ic mode photo ##meter divide subject field multiple region analyzes light exposure calc ##ulator calculates exposure value based output photo ##meter memory store weighing coefficient corresponding multiple light brightness pattern tilt switch posi [SEP]\n",
      "INFO:tensorflow:input_ids: 102 24039 141 3264 19644 25701 1913 1259 8830 29868 15419 4119 2718 11964 30113 532 1761 8830 29868 3264 3753 5135 1607 6591 24039 141 2955 15979 114 1892 24039 141 803 791 610 24039 141 2955 8830 27304 12380 2950 1281 1624 1025 21127 2011 2718 992 5985 18572 2718 973 791 1892 8830 27304 2411 7522 21334 3698 1737 1624 2011 16039 1377 18212 6216 9660 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 2\n",
      "INFO:tensorflow:tokens: [CLS] print demand camera system camera unit incorporating lens image sensor image processing circuitry connected det ##ach ##able printer unit way pcm ##cia interface printer unit may incorporate business card sized paper cart ##ridge overall dimension pcm ##cia type iii card [SEP]\n",
      "INFO:tensorflow:input_ids: 102 12153 3880 5859 429 5859 2515 11628 7473 1572 3121 1572 2307 20882 3777 466 339 318 28744 2515 1804 23851 19639 3396 28744 2515 552 10586 3948 1787 18487 1203 10068 28611 2103 2334 23851 19639 1211 2786 1787 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 3\n",
      "INFO:tensorflow:tokens: [CLS] simplified camera mechanism method allowing operator select distinct one several aperture actuation finger engage ##able aperture select ##or well insure return select ##or initial mode position shut ##ter actuator assembly return original una ##ct ##uated position following release thereof operator [SEP]\n",
      "INFO:tensorflow:input_ids: 102 9475 5859 2264 551 5644 4196 5470 3646 482 1323 13797 26535 7405 11569 318 13797 5470 122 804 22393 3988 5470 122 1700 2955 1607 15926 192 15930 6530 3988 2592 10313 1669 8397 1607 982 3408 27013 4196 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 4\n",
      "INFO:tensorflow:tokens: [CLS] beam split ##ter digital camera split light passed photograph ##ing lens beam directed ccd find ##er optical system beam split ##ter includes least optical element provided beam splitting surface inclined respect optical axis photograph ##ing lens multi layer film including plural ##ity layer dielectric material formed beam splitting surface multi layer film configured beam splitting surface exhibit substantially constant reflectivity least visible light incident [SEP]\n",
      "INFO:tensorflow:input_ids: 102 4555 5218 192 3787 5859 5218 2011 9930 24039 140 7473 4555 6742 15676 1040 114 3451 429 4555 5218 192 3753 1823 3451 2690 1966 4555 11050 1437 25309 2646 3451 4636 24039 140 7473 869 2474 5796 1471 22533 208 2474 10082 1440 4127 4555 11050 1437 869 2474 5796 19779 4555 11050 1437 5537 7790 2080 26583 1823 6310 2011 7956 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "\n",
      "sample features are: ['[CLS]', 'photograph', '##ic', 'device', 'eliminates', 'photometric', 'error', 'occur', 'photo', '##metry', 'obtains', 'accurate', 'exposure', 'shorten', '##s', 'time', 'required', 'photo', '##metry', 'device', 'includes', 'orientation', 'position', 'detector', 'photograph', '##ic', 'mode', 'sett', '##er', 'output', 'photograph', '##ic', 'condition', 'based', 'set', 'photograph', '##ic', 'mode', 'photo', '##meter', 'divide', 'subject', 'field', 'multiple', 'region', 'analyzes', 'light', 'exposure', 'calc', '##ulator', 'calculates', 'exposure', 'value', 'based', 'output', 'photo', '##meter', 'memory', 'store', 'weighing', 'coefficient', 'corresponding', 'multiple', 'light', 'brightness', 'pattern', 'tilt', 'switch', 'posi', '[SEP]']\n",
      "features len is  19710\n",
      "Press return to continue...\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f95c416d950>) includes params argument, but params are not passed to Estimator.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpa6m5g3nn\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpa6m5g3nn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "isolate_session_state: true\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': None, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=2, num_shards=272, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f95c3ccf250>}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
      "INFO:tensorflow:Could not find trained model in model_dir: /tmp/tmpa6m5g3nn, running initialization to predict.\n",
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running infer on CPU\n",
      "WARNING:tensorflow:From /home/sahand/Projects/science_science/BERT/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sahand/Projects/science_science/BERT/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sahand/Projects/science_science/BERT/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sahand/Projects/science_science/BERT/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (31090, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "estimator is finalized and is at sentence 19710\n",
      "witing to disk...\n",
      "Finished writing embeddings with len 19710\n",
      "\n",
      "Time of process for all sentences: 1130.9177770000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "tic = time.clock()\n",
    "\n",
    "\"\"\"\n",
    "period              start_from_idx\n",
    "1990-2004           0\n",
    "2005-2007           43482\n",
    "2008-2010           17873+43482=61355\n",
    "2011-2013           17873+43482+24613=85968\n",
    "2014-2016           17873+43482+24613+26560=112528\n",
    "2017-2018           17873+43482+24613+26560+40386=152914\n",
    "\"\"\"\n",
    "TPU_ADDRESS = ''\n",
    "get_features(data['sentence'][:].values.tolist(),\n",
    "  output_file = '/mnt/16A4A9BCA4A99EAD/tmp bert results/[CLS] results.json',\n",
    "  output_labels = list(data.index),\n",
    "  start_from_idx=0)\n",
    "\n",
    "# get_features(data['sentence'][70000:140000].values.tolist(), dim=300,\n",
    "#   output_file = 'drive/My Drive/Data/1990-2018 corpus sentences abstract-title - vectors 300 - 70-140k.json',\n",
    "#   output_labels = data['article_index'][70000:140000].values.tolist(),start_from_idx=70000)\n",
    "\n",
    "# get_features(data['sentence'][140000:].values.tolist(), dim=300,\n",
    "#   output_file = 'drive/My Drive/Data/1990-2018 corpus sentences abstract-title - vectors 300 - 140-205k.json',\n",
    "#   output_labels = data['article_index'][140000:].values.tolist(),start_from_idx=140000)\n",
    "\n",
    "toc = time.clock()\n",
    "print('\\nTime of process for all sentences:',toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "0GXKQ759Vjad",
    "outputId": "015cd434-3567-4b44-dd7f-a7366d0dcb0f"
   },
   "outputs": [],
   "source": [
    "files.download('drive/My Drive/Data/1990-2018 corpus sentences abstract-title - vectors - all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6CqzNdWFV7FT"
   },
   "outputs": [],
   "source": [
    "print(data.head(5))\n",
    "print(data[-10:].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSJEt45fVlOr"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart kernel here to free up space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "output_file = '/mnt/16A4A9BCA4A99EAD/tmp bert results/[CLS] results.json'\n",
    "with open(output_file) as f:\n",
    "    results = json.loads(f.read())\n",
    "results_clean = [results[x]['vectors']['[CLS]'][1:-1].split(', ') for x in results]\n",
    "print(len(results_clean))\n",
    "print(len(results_clean[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_clean = pd.DataFrame(results_clean)\n",
    "results_clean.to_csv('/mnt/16A4A9BCA4A99EAD/tmp bert results/[CLS] results.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fj_bqaMtZ79b"
   },
   "source": [
    "Read the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results = pd.read_csv('/mnt/16A4A9BCA4A99EAD/tmp bert results/[CLS] results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.093991</td>\n",
       "      <td>0.481889</td>\n",
       "      <td>0.936157</td>\n",
       "      <td>0.362795</td>\n",
       "      <td>-1.231043</td>\n",
       "      <td>-0.625693</td>\n",
       "      <td>-3.721168</td>\n",
       "      <td>-6.064877</td>\n",
       "      <td>0.714241</td>\n",
       "      <td>-0.606902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.900749</td>\n",
       "      <td>-0.727135</td>\n",
       "      <td>-4.898857</td>\n",
       "      <td>-1.018077</td>\n",
       "      <td>0.680102</td>\n",
       "      <td>1.727248</td>\n",
       "      <td>-0.107641</td>\n",
       "      <td>1.598622</td>\n",
       "      <td>1.403962</td>\n",
       "      <td>-0.623033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.463815</td>\n",
       "      <td>-0.394525</td>\n",
       "      <td>0.873828</td>\n",
       "      <td>-1.372293</td>\n",
       "      <td>-0.601543</td>\n",
       "      <td>1.371147</td>\n",
       "      <td>-1.653654</td>\n",
       "      <td>-3.728278</td>\n",
       "      <td>1.308728</td>\n",
       "      <td>-1.877245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.523725</td>\n",
       "      <td>-0.375501</td>\n",
       "      <td>-4.733455</td>\n",
       "      <td>-1.548404</td>\n",
       "      <td>0.737615</td>\n",
       "      <td>0.732650</td>\n",
       "      <td>-1.483686</td>\n",
       "      <td>1.427930</td>\n",
       "      <td>-0.722826</td>\n",
       "      <td>0.316824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.444905</td>\n",
       "      <td>-0.496883</td>\n",
       "      <td>0.143387</td>\n",
       "      <td>-0.016598</td>\n",
       "      <td>-0.548146</td>\n",
       "      <td>0.161970</td>\n",
       "      <td>-3.092156</td>\n",
       "      <td>-3.658966</td>\n",
       "      <td>1.391729</td>\n",
       "      <td>-0.732584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114351</td>\n",
       "      <td>-1.909591</td>\n",
       "      <td>-4.020740</td>\n",
       "      <td>-3.141164</td>\n",
       "      <td>-0.884360</td>\n",
       "      <td>2.126058</td>\n",
       "      <td>-2.103223</td>\n",
       "      <td>1.011418</td>\n",
       "      <td>1.328651</td>\n",
       "      <td>0.043055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129338</td>\n",
       "      <td>1.244845</td>\n",
       "      <td>-0.099512</td>\n",
       "      <td>-0.016150</td>\n",
       "      <td>-2.191903</td>\n",
       "      <td>0.672146</td>\n",
       "      <td>-3.541779</td>\n",
       "      <td>-2.563775</td>\n",
       "      <td>0.481966</td>\n",
       "      <td>-1.218525</td>\n",
       "      <td>...</td>\n",
       "      <td>1.030245</td>\n",
       "      <td>-2.337068</td>\n",
       "      <td>-5.826959</td>\n",
       "      <td>-1.795447</td>\n",
       "      <td>0.804469</td>\n",
       "      <td>2.098383</td>\n",
       "      <td>-1.806895</td>\n",
       "      <td>-1.718628</td>\n",
       "      <td>0.792790</td>\n",
       "      <td>-1.045472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.203263</td>\n",
       "      <td>1.313853</td>\n",
       "      <td>2.369816</td>\n",
       "      <td>0.086279</td>\n",
       "      <td>-0.161791</td>\n",
       "      <td>0.479322</td>\n",
       "      <td>-3.706724</td>\n",
       "      <td>-3.451765</td>\n",
       "      <td>1.136608</td>\n",
       "      <td>0.958250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.516233</td>\n",
       "      <td>-0.151334</td>\n",
       "      <td>-3.595501</td>\n",
       "      <td>-0.376670</td>\n",
       "      <td>-0.894569</td>\n",
       "      <td>1.587983</td>\n",
       "      <td>0.075020</td>\n",
       "      <td>0.405111</td>\n",
       "      <td>-0.419986</td>\n",
       "      <td>-2.057433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19705</th>\n",
       "      <td>-0.016106</td>\n",
       "      <td>-0.246208</td>\n",
       "      <td>-0.047454</td>\n",
       "      <td>-2.847529</td>\n",
       "      <td>0.080989</td>\n",
       "      <td>0.421679</td>\n",
       "      <td>-0.905605</td>\n",
       "      <td>-5.266561</td>\n",
       "      <td>2.162794</td>\n",
       "      <td>0.462337</td>\n",
       "      <td>...</td>\n",
       "      <td>1.059816</td>\n",
       "      <td>1.197620</td>\n",
       "      <td>-4.490421</td>\n",
       "      <td>1.088950</td>\n",
       "      <td>-0.083379</td>\n",
       "      <td>0.621057</td>\n",
       "      <td>-2.521836</td>\n",
       "      <td>1.762247</td>\n",
       "      <td>-0.572201</td>\n",
       "      <td>0.205469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19706</th>\n",
       "      <td>3.810331</td>\n",
       "      <td>2.579611</td>\n",
       "      <td>1.169846</td>\n",
       "      <td>-1.085377</td>\n",
       "      <td>-1.651331</td>\n",
       "      <td>-0.098971</td>\n",
       "      <td>-2.075445</td>\n",
       "      <td>-4.282493</td>\n",
       "      <td>3.299706</td>\n",
       "      <td>-3.023206</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.179699</td>\n",
       "      <td>-3.143414</td>\n",
       "      <td>-3.506091</td>\n",
       "      <td>0.881107</td>\n",
       "      <td>-0.778475</td>\n",
       "      <td>1.073418</td>\n",
       "      <td>-0.001388</td>\n",
       "      <td>1.839229</td>\n",
       "      <td>-0.939617</td>\n",
       "      <td>-1.775609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19707</th>\n",
       "      <td>1.894238</td>\n",
       "      <td>1.973035</td>\n",
       "      <td>1.086454</td>\n",
       "      <td>-1.864109</td>\n",
       "      <td>-0.780507</td>\n",
       "      <td>-1.720195</td>\n",
       "      <td>-5.610694</td>\n",
       "      <td>-6.171289</td>\n",
       "      <td>0.486777</td>\n",
       "      <td>0.371356</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071672</td>\n",
       "      <td>-0.702177</td>\n",
       "      <td>-4.376399</td>\n",
       "      <td>0.603304</td>\n",
       "      <td>-1.550666</td>\n",
       "      <td>3.675741</td>\n",
       "      <td>-1.259607</td>\n",
       "      <td>0.434585</td>\n",
       "      <td>-1.581164</td>\n",
       "      <td>0.000677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19708</th>\n",
       "      <td>-0.894446</td>\n",
       "      <td>-1.722048</td>\n",
       "      <td>1.292111</td>\n",
       "      <td>-1.467540</td>\n",
       "      <td>1.039447</td>\n",
       "      <td>0.882132</td>\n",
       "      <td>-1.665698</td>\n",
       "      <td>-3.301647</td>\n",
       "      <td>1.404132</td>\n",
       "      <td>-0.586678</td>\n",
       "      <td>...</td>\n",
       "      <td>2.221418</td>\n",
       "      <td>1.048154</td>\n",
       "      <td>-5.086555</td>\n",
       "      <td>-2.768722</td>\n",
       "      <td>-0.346986</td>\n",
       "      <td>0.141288</td>\n",
       "      <td>-4.304899</td>\n",
       "      <td>0.077840</td>\n",
       "      <td>1.744139</td>\n",
       "      <td>-1.221411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19709</th>\n",
       "      <td>0.723120</td>\n",
       "      <td>-0.633688</td>\n",
       "      <td>0.662232</td>\n",
       "      <td>-1.175202</td>\n",
       "      <td>-1.114556</td>\n",
       "      <td>1.019399</td>\n",
       "      <td>-0.939215</td>\n",
       "      <td>-3.455509</td>\n",
       "      <td>2.360814</td>\n",
       "      <td>-1.305676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865384</td>\n",
       "      <td>-2.249327</td>\n",
       "      <td>-4.825214</td>\n",
       "      <td>-2.788386</td>\n",
       "      <td>2.666067</td>\n",
       "      <td>-0.307681</td>\n",
       "      <td>-0.605039</td>\n",
       "      <td>1.012239</td>\n",
       "      <td>-1.696799</td>\n",
       "      <td>-0.246692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19710 rows Ã— 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      2.093991  0.481889  0.936157  0.362795 -1.231043 -0.625693 -3.721168   \n",
       "1      0.463815 -0.394525  0.873828 -1.372293 -0.601543  1.371147 -1.653654   \n",
       "2     -1.444905 -0.496883  0.143387 -0.016598 -0.548146  0.161970 -3.092156   \n",
       "3      0.129338  1.244845 -0.099512 -0.016150 -2.191903  0.672146 -3.541779   \n",
       "4     -0.203263  1.313853  2.369816  0.086279 -0.161791  0.479322 -3.706724   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19705 -0.016106 -0.246208 -0.047454 -2.847529  0.080989  0.421679 -0.905605   \n",
       "19706  3.810331  2.579611  1.169846 -1.085377 -1.651331 -0.098971 -2.075445   \n",
       "19707  1.894238  1.973035  1.086454 -1.864109 -0.780507 -1.720195 -5.610694   \n",
       "19708 -0.894446 -1.722048  1.292111 -1.467540  1.039447  0.882132 -1.665698   \n",
       "19709  0.723120 -0.633688  0.662232 -1.175202 -1.114556  1.019399 -0.939215   \n",
       "\n",
       "              7         8         9  ...       758       759       760  \\\n",
       "0     -6.064877  0.714241 -0.606902  ...  0.900749 -0.727135 -4.898857   \n",
       "1     -3.728278  1.308728 -1.877245  ...  0.523725 -0.375501 -4.733455   \n",
       "2     -3.658966  1.391729 -0.732584  ...  0.114351 -1.909591 -4.020740   \n",
       "3     -2.563775  0.481966 -1.218525  ...  1.030245 -2.337068 -5.826959   \n",
       "4     -3.451765  1.136608  0.958250  ...  1.516233 -0.151334 -3.595501   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "19705 -5.266561  2.162794  0.462337  ...  1.059816  1.197620 -4.490421   \n",
       "19706 -4.282493  3.299706 -3.023206  ... -1.179699 -3.143414 -3.506091   \n",
       "19707 -6.171289  0.486777  0.371356  ... -0.071672 -0.702177 -4.376399   \n",
       "19708 -3.301647  1.404132 -0.586678  ...  2.221418  1.048154 -5.086555   \n",
       "19709 -3.455509  2.360814 -1.305676  ...  0.865384 -2.249327 -4.825214   \n",
       "\n",
       "            761       762       763       764       765       766       767  \n",
       "0     -1.018077  0.680102  1.727248 -0.107641  1.598622  1.403962 -0.623033  \n",
       "1     -1.548404  0.737615  0.732650 -1.483686  1.427930 -0.722826  0.316824  \n",
       "2     -3.141164 -0.884360  2.126058 -2.103223  1.011418  1.328651  0.043055  \n",
       "3     -1.795447  0.804469  2.098383 -1.806895 -1.718628  0.792790 -1.045472  \n",
       "4     -0.376670 -0.894569  1.587983  0.075020  0.405111 -0.419986 -2.057433  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "19705  1.088950 -0.083379  0.621057 -2.521836  1.762247 -0.572201  0.205469  \n",
       "19706  0.881107 -0.778475  1.073418 -0.001388  1.839229 -0.939617 -1.775609  \n",
       "19707  0.603304 -1.550666  3.675741 -1.259607  0.434585 -1.581164  0.000677  \n",
       "19708 -2.768722 -0.346986  0.141288 -4.304899  0.077840  1.744139 -1.221411  \n",
       "19709 -2.788386  2.666067 -0.307681 -0.605039  1.012239 -1.696799 -0.246692  \n",
       "\n",
       "[19710 rows x 768 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "BERT embedding sample",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
