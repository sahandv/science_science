{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sahandv/scioscipred/blob/master/BERT_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijoObp_DTMSD"
   },
   "source": [
    "# BERT WORD EMBEDDING\n",
    "\n",
    "Credits to https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sahand/Projects/science_science\n",
      "env: /home/sahand/anaconda3/envs/tf-1/bin/python\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "import sys\n",
    "import os\n",
    "print('env:',sys.executable)\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JyUry-7GTUFy"
   },
   "source": [
    "### Mount Google Drive\n",
    "If on colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l3c1LxI8TQuy",
    "outputId": "8f63d47f-db5f-45bb-e99a-0c3e619ab005"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LvD9I1PhTbDS",
    "outputId": "20f779cb-0faa-4c31-d81b-626bdc4c46ed"
   },
   "outputs": [],
   "source": [
    "!ls 'drive/My Drive/Data/1990-2018 corpus sentences abstract-title'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWIJmpvETXoC"
   },
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "hFu5wVYbUDuC",
    "outputId": "69adfaa9-b89d-49f6-89c9-5572bca74fe6"
   },
   "outputs": [],
   "source": [
    "!rm -rf bert\n",
    "!git clone https://github.com/google-research/bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MUFmzDnwTjmC",
    "outputId": "166a4dff-245a-44dc-efc4-8ba8480a8405"
   },
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-LgLOc5jTn5R"
   },
   "source": [
    "### Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t40Yz-6STr1a"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import codecs\n",
    "import collections\n",
    "import json\n",
    "import re\n",
    "\n",
    "import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from BERT import modeling\n",
    "from BERT import tokenization\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If on colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "sys.path.append('bert/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-N4KEslNTw75"
   },
   "source": [
    "### Make sure colab TPU is being used\n",
    "If on colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "89-CDu2KTxQx",
    "outputId": "3f8bdf4c-f041-42d5-ddf6-49c34ef38e17"
   },
   "outputs": [],
   "source": [
    "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
    "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "print('TPU address is', TPU_ADDRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "GKPbwFaOU7SB",
    "outputId": "593057a5-56e4-448d-9f27-8b7cdf85de09"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "with tf.Session(TPU_ADDRESS) as session:\n",
    "  print('TPU devices:')\n",
    "  pprint.pprint(session.list_devices())\n",
    "\n",
    "  # Upload credentials to TPU.\n",
    "  with open('/content/adc.json', 'r') as f:\n",
    "    auth_info = json.load(f)\n",
    "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
    "  # Now credentials are set for all future sessions on this TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9o5R71SVHdY"
   },
   "source": [
    "### Select which BERT model we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "v2gvHlriVIrY",
    "outputId": "5f0bc4c6-8e26-4071-df48-2d110cf7c867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** BERT pretrained directory: /home/sahand/Projects/science_science/BERT_weights/uncased_L-12_H-768_A-12 *****\n",
      "/bin/bash: gsutil: command not found\r\n"
     ]
    }
   ],
   "source": [
    "# Available pretrained model checkpoints:\n",
    "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
    "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
    "#   cased_L-12_H-768_A-12: cased BERT large model\n",
    "BERT_MODEL = 'uncased_L-12_H-768_A-12'\n",
    "BERT_PRETRAINED_DIR = '/home/sahand/Projects/science_science/BERT_weights/' + BERT_MODEL\n",
    "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
    "!gsutil ls $BERT_PRETRAINED_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHt6zH_JX8d7"
   },
   "source": [
    "### Define some global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjqpTzIPX8ur"
   },
   "outputs": [],
   "source": [
    "LAYERS = [-1,-2,-3,-4]\n",
    "NUM_TPU_CORES = 272\n",
    "MAX_SEQ_LENGTH = 500\n",
    "BERT_CONFIG = BERT_PRETRAINED_DIR + '/bert_config.json'\n",
    "CHKPT_DIR = BERT_PRETRAINED_DIR + '/bert_model.ckpt'\n",
    "VOCAB_FILE = BERT_PRETRAINED_DIR + '/vocab.txt'\n",
    "INIT_CHECKPOINT = BERT_PRETRAINED_DIR + '/bert_model.ckpt'\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Astne8rYYChc"
   },
   "source": [
    "### Define wrapper classes for the input before processing and after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UUEDmql4YCtb"
   },
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "\n",
    "  def __init__(self, unique_id, text_a, text_b=None):\n",
    "    self.unique_id = unique_id\n",
    "    self.text_a = text_a\n",
    "    self.text_b = text_b\n",
    "    \n",
    "class InputFeatures(object):\n",
    "  \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "  def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
    "    self.unique_id = unique_id\n",
    "    self.tokens = tokens\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.input_type_ids = input_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j7wN0LtWYRkb"
   },
   "source": [
    "### Set up our Estimator\n",
    "\n",
    "The estimator is a wrapper around the BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQtArQ6TYRpz"
   },
   "outputs": [],
   "source": [
    "def input_fn_builder(features, seq_length):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  all_unique_ids = []\n",
    "  all_input_ids = []\n",
    "  all_input_mask = []\n",
    "  all_input_type_ids = []\n",
    "\n",
    "  for feature in features:\n",
    "    all_unique_ids.append(feature.unique_id)\n",
    "    all_input_ids.append(feature.input_ids)\n",
    "    all_input_mask.append(feature.input_mask)\n",
    "    all_input_type_ids.append(feature.input_type_ids)\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    batch_size = params[\"batch_size\"]\n",
    "\n",
    "    num_examples = len(features)\n",
    "\n",
    "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
    "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
    "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
    "    d = tf.data.Dataset.from_tensor_slices({\n",
    "        \"unique_ids\":\n",
    "            tf.constant(all_unique_ids, shape=[num_examples], dtype=tf.int32),\n",
    "        \"input_ids\":\n",
    "            tf.constant(\n",
    "                all_input_ids, shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"input_mask\":\n",
    "            tf.constant(\n",
    "                all_input_mask,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"input_type_ids\":\n",
    "            tf.constant(\n",
    "                all_input_type_ids,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "    })\n",
    "\n",
    "    d = d.batch(batch_size=batch_size, drop_remainder=False)\n",
    "    return d\n",
    "\n",
    "  return input_fn\n",
    "  \n",
    "def model_fn_builder(bert_config, init_checkpoint, layer_indexes, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    unique_ids = features[\"unique_ids\"]\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    input_type_ids = features[\"input_type_ids\"]\n",
    "\n",
    "    # The model is defined here\n",
    "    model = modeling.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=False,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        token_type_ids=input_type_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "      raise ValueError(\"Only PREDICT modes are supported: %s\" % (mode))\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    scaffold_fn = None\n",
    "    (assignment_map,\n",
    "     initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(\n",
    "         tvars, init_checkpoint)\n",
    "    if use_tpu:\n",
    "\n",
    "      def tpu_scaffold():\n",
    "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "        return tf.train.Scaffold()\n",
    "\n",
    "      scaffold_fn = tpu_scaffold\n",
    "    else:\n",
    "      tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "    tf.logging.info(\"**** Trainable Variables ****\")\n",
    "    for var in tvars:\n",
    "      init_string = \"\"\n",
    "      if var.name in initialized_variable_names:\n",
    "        init_string = \", *INIT_FROM_CKPT*\"\n",
    "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "                      init_string)\n",
    "\n",
    "    all_layers = model.get_all_encoder_layers()\n",
    "\n",
    "    predictions = {\n",
    "        \"unique_id\": unique_ids,\n",
    "    }\n",
    "\n",
    "    # Tredictions from the top 4 layers are extracted here\n",
    "    for (i, layer_index) in enumerate(layer_indexes):\n",
    "      predictions[\"layer_output_%d\" % i] = all_layers[layer_index]\n",
    "\n",
    "    output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "        mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n",
    "    return output_spec\n",
    "\n",
    "  return model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wNuP-aQdY53S"
   },
   "source": [
    "### Transform InputExamples to InputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dK58xgpLY6Ez"
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, seq_length, tokenizer):\n",
    "  \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "  features = []\n",
    "  for (ex_index, example) in enumerate(examples):\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "      tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "    if tokens_b:\n",
    "      # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "      # length is less than the specified length.\n",
    "      # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "      _truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)\n",
    "    else:\n",
    "      # Account for [CLS] and [SEP] with \"- 2\"\n",
    "      if len(tokens_a) > seq_length - 2:\n",
    "        tokens_a = tokens_a[0:(seq_length - 2)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0     0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = []\n",
    "    input_type_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    input_type_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "      tokens.append(token)\n",
    "      input_type_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "    if tokens_b:\n",
    "      for token in tokens_b:\n",
    "        tokens.append(token)\n",
    "        input_type_ids.append(1)\n",
    "      tokens.append(\"[SEP]\")\n",
    "      input_type_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < seq_length:\n",
    "      input_ids.append(0)\n",
    "      input_mask.append(0)\n",
    "      input_type_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == seq_length\n",
    "    assert len(input_mask) == seq_length\n",
    "    assert len(input_type_ids) == seq_length\n",
    "\n",
    "    if ex_index < 5:\n",
    "      tf.logging.info(\"*** Example ***\")\n",
    "      tf.logging.info(\"unique_id: %s\" % (example.unique_id))\n",
    "      tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "          [tokenization.printable_text(x) for x in tokens]))\n",
    "      tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "      tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "      tf.logging.info(\n",
    "          \"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
    "\n",
    "    features.append(\n",
    "        InputFeatures(\n",
    "            unique_id=example.unique_id,\n",
    "            tokens=tokens,\n",
    "            input_ids=input_ids,\n",
    "            input_mask=input_mask,\n",
    "            input_type_ids=input_type_ids))\n",
    "  return features\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "  # This is a simple heuristic which will always truncate the longer sequence\n",
    "  # one token at a time. This makes more sense than truncating an equal percent\n",
    "  # of tokens from each, since if one sequence is very short then each token\n",
    "  # that's truncated likely contains more information than a longer sequence.\n",
    "  while True:\n",
    "    total_length = len(tokens_a) + len(tokens_b)\n",
    "    if total_length <= max_length:\n",
    "      break\n",
    "    if len(tokens_a) > len(tokens_b):\n",
    "      tokens_a.pop()\n",
    "    else:\n",
    "      tokens_b.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QbVmmo1iZycC"
   },
   "source": [
    "### Converting a normal string sequence to InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PBmwXYBZyn4"
   },
   "outputs": [],
   "source": [
    "def read_sequence(input_sentences):\n",
    "  examples = []\n",
    "  unique_id = 0\n",
    "  for sentence in input_sentences:\n",
    "    line = tokenization.convert_to_unicode(sentence)\n",
    "    examples.append(InputExample(unique_id=unique_id, text_a=line))\n",
    "    unique_id += 1\n",
    "  return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eeG-WZ9aZyzY"
   },
   "source": [
    "### Processing functions for inputs\n",
    "\n",
    "A function which accepts *an array of strings* as a parameter and the desired *dimension (max 768) of the embedding output* and returns a dictionary with the token as key and the embedding vector as value.\n",
    "\n",
    "The function builds the estimator and invokes a prediction based on the given inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(input_text, dim=768,output_labels=None,output_file='embeddings.json',\n",
    "             start_from_idx=0,use_TPU=False,doc_embedding=True):\n",
    "    \n",
    "    layer_indexes = LAYERS\n",
    "    bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\n",
    "    tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=VOCAB_FILE, do_lower_case=True)\n",
    "\n",
    "    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
    "    run_config = tf.contrib.tpu.RunConfig(\n",
    "      cluster=tpu_cluster_resolver,\n",
    "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "          num_shards=NUM_TPU_CORES,\n",
    "          per_host_input_for_training=is_per_host))\n",
    "\n",
    "    examples = read_sequence(input_text)\n",
    "    print(\"example len is \",len(examples))\n",
    "\n",
    "    features = convert_examples_to_features(\n",
    "      examples=examples, seq_length=MAX_SEQ_LENGTH, tokenizer=tokenizer)\n",
    "    print('\\nsample features are:',features[1].tokens)\n",
    "\n",
    "    \n",
    "    unique_id_to_feature = {}\n",
    "    for feature in features:\n",
    "        unique_id_to_feature[feature.unique_id] = feature\n",
    "\n",
    "    print(\"features len is \",len(features))\n",
    "    username = input(\"Press return to continue...\")\n",
    "    \n",
    "    model_fn = model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      init_checkpoint=INIT_CHECKPOINT,\n",
    "      layer_indexes=layer_indexes,\n",
    "      use_tpu=use_TPU,\n",
    "      use_one_hot_embeddings=True)\n",
    "\n",
    "    # If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "    # or GPU.\n",
    "    estimator = tf.contrib.tpu.TPUEstimator(\n",
    "      use_tpu=use_TPU,\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      predict_batch_size=BATCH_SIZE,\n",
    "      train_batch_size=BATCH_SIZE)\n",
    "\n",
    "    input_fn = input_fn_builder(\n",
    "      features=features, seq_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "    # Get features\n",
    "    output_dict = {}\n",
    "    all_embeddings = []\n",
    "    counter = 0\n",
    "    for result in estimator.predict(input_fn, yield_single_examples=True):\n",
    "        unique_id = int(result[\"unique_id\"])\n",
    "        feature = unique_id_to_feature[unique_id]\n",
    "        output = collections.OrderedDict()\n",
    "        sentence_dict = {}\n",
    "        for (i, token) in enumerate(feature.tokens):\n",
    "            layers = []\n",
    "            for (j, layer_index) in enumerate(layer_indexes):\n",
    "                layer_output = result[\"layer_output_%d\" % j]\n",
    "                layer_output_flat = np.array([x for x in layer_output[i:(i + 1)].flat])\n",
    "                layers.append(layer_output_flat)\n",
    "                \n",
    "            if doc_embedding==True:\n",
    "                if token == '[CLS]':\n",
    "                    output[token] = list(sum(layers)[:dim]) # You can also concatenate or average layers here\n",
    "                    sentence_dict.update({token:str(output[token])})\n",
    "            else:\n",
    "                output[token] = list(sum(layers)[:dim]) # You can also concatenate or average layers here\n",
    "                sentence_dict.update({token:str(output[token])})\n",
    "                \n",
    "        output_dict[str(counter+start_from_idx)] = {'article':str(output_labels[counter]),'vectors':sentence_dict}\n",
    "        counter += 1\n",
    "\n",
    "    print('estimator is finalized and is at sentence', counter)\n",
    "    print('witing to disk...')\n",
    "\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(output_dict, json_file)\n",
    "\n",
    "    print('Finished writing embeddings with len',len(output_dict))\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x54s2Th-Zy6Y"
   },
   "outputs": [],
   "source": [
    "def get_features(input_text, dim=768,output_labels=None,output_file='embeddings.json',\n",
    "                 start_from_idx=0,use_TPU=False):\n",
    "  layer_indexes = LAYERS\n",
    "\n",
    "  bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\n",
    "\n",
    "  tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=VOCAB_FILE, do_lower_case=True)\n",
    "\n",
    "  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
    "  run_config = tf.contrib.tpu.RunConfig(\n",
    "      cluster=tpu_cluster_resolver,\n",
    "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "          num_shards=NUM_TPU_CORES,\n",
    "          per_host_input_for_training=is_per_host))\n",
    "\n",
    "  examples = read_sequence(input_text)\n",
    "  print(\"example len is \",len(examples))\n",
    "\n",
    "  features = convert_examples_to_features(\n",
    "      examples=examples, seq_length=MAX_SEQ_LENGTH, tokenizer=tokenizer)\n",
    "\n",
    "  unique_id_to_feature = {}\n",
    "  for feature in features:\n",
    "    unique_id_to_feature[feature.unique_id] = feature\n",
    "\n",
    "  print(\"features len is \",len(features))\n",
    "\n",
    "  model_fn = model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      init_checkpoint=INIT_CHECKPOINT,\n",
    "      layer_indexes=layer_indexes,\n",
    "      use_tpu=use_TPU,\n",
    "      use_one_hot_embeddings=True)\n",
    "\n",
    "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "  # or GPU.\n",
    "  estimator = tf.contrib.tpu.TPUEstimator(\n",
    "      use_tpu=use_TPU,\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      predict_batch_size=BATCH_SIZE,\n",
    "      train_batch_size=BATCH_SIZE)\n",
    "\n",
    "  input_fn = input_fn_builder(\n",
    "      features=features, seq_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "  # Get features\n",
    "  output_dict = {}\n",
    "  all_embeddings = []\n",
    "  counter = 0\n",
    "  for result in estimator.predict(input_fn, yield_single_examples=True):\n",
    "    unique_id = int(result[\"unique_id\"])\n",
    "    feature = unique_id_to_feature[unique_id]\n",
    "    output = collections.OrderedDict()\n",
    "    sentence_dict = {}\n",
    "    for (i, token) in enumerate(feature.tokens):\n",
    "      layers = []\n",
    "      for (j, layer_index) in enumerate(layer_indexes):\n",
    "        layer_output = result[\"layer_output_%d\" % j]\n",
    "        layer_output_flat = np.array([x for x in layer_output[i:(i + 1)].flat])\n",
    "        layers.append(layer_output_flat)\n",
    "      output[token] = list(sum(layers)[:dim]) # You can also concatenate or average layers here \n",
    "      sentence_dict.update({token:str(output[token])})\n",
    "\n",
    "    output_dict[str(counter+start_from_idx)] = {'article':str(output_labels[counter]),'vectors':sentence_dict}\n",
    "    counter += 1\n",
    "    \n",
    "  print('estimator is finalized and is at sentence', counter)\n",
    "  print('witing to disk...')\n",
    "\n",
    "  with open(output_file, 'w') as json_file:\n",
    "    json.dump(output_dict, json_file)\n",
    "\n",
    "  print('Finished writing embeddings with len',len(output_dict))\n",
    "\n",
    "  return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_DSh2GLarwP"
   },
   "source": [
    "### Run the model and get the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DHM-FKqK9S_e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lighting system disclosed comprising plurality...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>photographic device eliminates photometric err...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>print demand camera system camera unit incorpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>simplified camera mechanism method allowing op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beam splitter digital camera split light passe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19705</th>\n",
       "      <td>display engine video graphic system includes p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19706</th>\n",
       "      <td>facilitating interaction may enabled communica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19707</th>\n",
       "      <td>present invention generally directed system me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19708</th>\n",
       "      <td>disclosed system producing image including app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19709</th>\n",
       "      <td>fully programmable graphic microprocessor embo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19710 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence\n",
       "0      lighting system disclosed comprising plurality...\n",
       "1      photographic device eliminates photometric err...\n",
       "2      print demand camera system camera unit incorpo...\n",
       "3      simplified camera mechanism method allowing op...\n",
       "4      beam splitter digital camera split light passe...\n",
       "...                                                  ...\n",
       "19705  display engine video graphic system includes p...\n",
       "19706  facilitating interaction may enabled communica...\n",
       "19707  present invention generally directed system me...\n",
       "19708  disclosed system producing image including app...\n",
       "19709  fully programmable graphic microprocessor embo...\n",
       "\n",
       "[19710 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/mnt/16A4A9BCA4A99EAD/GoogleDrive/Data/Corpus/KPRIS/clean/abstract_title deflemm',names=['sentence'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8AVBeafE_R7S",
    "outputId": "6c449112-3b1d-4907-b5fe-1276372b9b4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sahand/Projects/science_science/BERT/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example len is  19710\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 0\n",
      "INFO:tensorflow:tokens: [CLS] lighting system disclosed comprising plurality image projection lighting device including first image projection lighting device central controller communication system permit central controller communicate plurality image projection lighting device first image projection lighting device comprised camera light valve wherein light valve used project first projected image onto projection surface camera first image projection light device capture least portion first projected image pr [SEP]\n",
      "INFO:tensorflow:input_ids: 101 7497 2291 21362 9605 29018 3746 13996 7497 5080 2164 2034 3746 13996 7497 5080 2430 11486 4807 2291 9146 2430 11486 10639 29018 3746 13996 7497 5080 2034 3746 13996 7497 5080 11539 4950 2422 10764 16726 2422 10764 2109 2622 2034 11310 3746 3031 13996 3302 4950 2034 3746 13996 2422 5080 5425 2560 4664 2034 11310 3746 10975 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 1\n",
      "INFO:tensorflow:tokens: [CLS] photographic device eliminate ##s photo ##metric error occur photo ##metry obtain ##s accurate exposure short ##ens time required photo ##metry device includes orientation position detector photographic mode set ##ter output photographic condition based set photographic mode photo ##meter divide subject field multiple region analyze ##s light exposure cal ##cula ##tor calculate ##s exposure value based output photo ##meter memory store weighing coefficient corresponding multiple light brightness pattern tilt switch po ##si [SEP]\n",
      "INFO:tensorflow:input_ids: 101 12416 5080 11027 2015 6302 12589 7561 5258 6302 24327 6855 2015 8321 7524 2460 6132 2051 3223 6302 24327 5080 2950 10296 2597 19034 12416 5549 2275 3334 6434 12416 4650 2241 2275 12416 5549 6302 22828 11443 3395 2492 3674 2555 17908 2015 2422 7524 10250 19879 4263 18422 2015 7524 3643 2241 6434 6302 22828 3638 3573 15243 19064 7978 3674 2422 18295 5418 17010 6942 13433 5332 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 2\n",
      "INFO:tensorflow:tokens: [CLS] print demand camera system camera unit incorporating lens image sensor image processing circuit ##ry connected det ##ach ##able printer unit way pc ##mc ##ia interface printer unit may incorporate business card sized paper cartridge overall dimension pc ##mc ##ia type iii card [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 6140 5157 4950 2291 4950 3131 13543 10014 3746 13617 3746 6364 4984 2854 4198 20010 6776 3085 15041 3131 2126 7473 12458 2401 8278 15041 3131 2089 13265 2449 4003 7451 3259 15110 3452 9812 7473 12458 2401 2828 3523 4003 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 3\n",
      "INFO:tensorflow:tokens: [CLS] simplified camera mechanism method allowing operator select distinct one several aperture act ##uation finger engage ##able aperture selector well ins ##ure return selector initial mode position shutter act ##ua ##tor assembly return original una ##ctuated position following release thereof operator [SEP]\n",
      "INFO:tensorflow:input_ids: 101 11038 4950 7337 4118 4352 6872 7276 5664 2028 2195 18892 2552 14505 4344 8526 3085 18892 27000 2092 16021 5397 2709 27000 3988 5549 2597 28180 2552 6692 4263 3320 2709 2434 14477 25638 2597 2206 2713 21739 6872 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 4\n",
      "INFO:tensorflow:tokens: [CLS] beam split ##ter digital camera split light passed photograph ##ing lens beam directed cc ##d find ##er optical system beam split ##ter includes least optical element provided beam splitting surface inclined respect optical axis photograph ##ing lens multi layer film including plurality layer die ##le ##ctric material formed beam splitting surface multi layer film configured beam splitting surface exhibit substantially constant reflect ##ivity least visible light incident [SEP]\n",
      "INFO:tensorflow:input_ids: 101 7504 3975 3334 3617 4950 3975 2422 2979 9982 2075 10014 7504 2856 10507 2094 2424 2121 9380 2291 7504 3975 3334 2950 2560 9380 5783 3024 7504 14541 3302 13050 4847 9380 8123 9982 2075 10014 4800 6741 2143 2164 29018 6741 3280 2571 22601 3430 2719 7504 14541 3302 4800 6741 2143 26928 7504 14541 3302 8327 12381 5377 8339 7730 2560 5710 2422 5043 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "\n",
      "sample features are: ['[CLS]', 'photographic', 'device', 'eliminate', '##s', 'photo', '##metric', 'error', 'occur', 'photo', '##metry', 'obtain', '##s', 'accurate', 'exposure', 'short', '##ens', 'time', 'required', 'photo', '##metry', 'device', 'includes', 'orientation', 'position', 'detector', 'photographic', 'mode', 'set', '##ter', 'output', 'photographic', 'condition', 'based', 'set', 'photographic', 'mode', 'photo', '##meter', 'divide', 'subject', 'field', 'multiple', 'region', 'analyze', '##s', 'light', 'exposure', 'cal', '##cula', '##tor', 'calculate', '##s', 'exposure', 'value', 'based', 'output', 'photo', '##meter', 'memory', 'store', 'weighing', 'coefficient', 'corresponding', 'multiple', 'light', 'brightness', 'pattern', 'tilt', 'switch', 'po', '##si', '[SEP]']\n",
      "features len is  19710\n",
      "Press return to continue...\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f5960aecb00>) includes params argument, but params are not passed to Estimator.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpfoyrqdg1\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpfoyrqdg1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "isolate_session_state: true\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': None, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=2, num_shards=272, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f5965fb78d0>}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
      "INFO:tensorflow:Could not find trained model in model_dir: /tmp/tmpfoyrqdg1, running initialization to predict.\n",
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running infer on CPU\n",
      "WARNING:tensorflow:From /home/sahand/Projects/science_science/BERT/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sahand/Projects/science_science/BERT/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sahand/Projects/science_science/BERT/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sahand/Projects/science_science/BERT/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "estimator is finalized and is at sentence 19710\n",
      "witing to disk...\n",
      "Finished writing embeddings with len 19710\n",
      "\n",
      "Time of process for all sentences: 1175.902498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "tic = time.clock()\n",
    "\n",
    "\"\"\"\n",
    "period              start_from_idx\n",
    "1990-2004           0\n",
    "2005-2007           43482\n",
    "2008-2010           17873+43482=61355\n",
    "2011-2013           17873+43482+24613=85968\n",
    "2014-2016           17873+43482+24613+26560=112528\n",
    "2017-2018           17873+43482+24613+26560+40386=152914\n",
    "\"\"\"\n",
    "TPU_ADDRESS = ''\n",
    "get_features(data['sentence'][:].values.tolist(),\n",
    "  output_file = '/mnt/16A4A9BCA4A99EAD/tmp bert results/uncased a12 results.json',\n",
    "  output_labels = list(data.index),\n",
    "  start_from_idx=0)\n",
    "\n",
    "# get_features(data['sentence'][70000:140000].values.tolist(), dim=300,\n",
    "#   output_file = 'drive/My Drive/Data/1990-2018 corpus sentences abstract-title - vectors 300 - 70-140k.json',\n",
    "#   output_labels = data['article_index'][70000:140000].values.tolist(),start_from_idx=70000)\n",
    "\n",
    "# get_features(data['sentence'][140000:].values.tolist(), dim=300,\n",
    "#   output_file = 'drive/My Drive/Data/1990-2018 corpus sentences abstract-title - vectors 300 - 140-205k.json',\n",
    "#   output_labels = data['article_index'][140000:].values.tolist(),start_from_idx=140000)\n",
    "\n",
    "toc = time.clock()\n",
    "print('\\nTime of process for all sentences:',toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "0GXKQ759Vjad",
    "outputId": "015cd434-3567-4b44-dd7f-a7366d0dcb0f"
   },
   "outputs": [],
   "source": [
    "files.download('drive/My Drive/Data/1990-2018 corpus sentences abstract-title - vectors - all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6CqzNdWFV7FT"
   },
   "outputs": [],
   "source": [
    "print(data.head(5))\n",
    "print(data[-10:].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSJEt45fVlOr"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart kernel here to free up space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19710\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "output_file = '/mnt/16A4A9BCA4A99EAD/tmp bert results/uncased a12 results.json'\n",
    "with open(output_file) as f:\n",
    "    results = json.loads(f.read())\n",
    "results_clean = [results[x]['vectors']['[CLS]'][1:-1].split(', ') for x in results]\n",
    "print(len(results_clean))\n",
    "print(len(results_clean[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_clean = pd.DataFrame(results_clean)\n",
    "results_clean.to_csv('/mnt/16A4A9BCA4A99EAD/tmp bert results/uncased a12 results.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fj_bqaMtZ79b"
   },
   "source": [
    "Read the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results = pd.read_csv('/mnt/16A4A9BCA4A99EAD/tmp bert results/uncased a12 results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.026509</td>\n",
       "      <td>2.504253</td>\n",
       "      <td>0.503871</td>\n",
       "      <td>3.906834</td>\n",
       "      <td>-1.673985</td>\n",
       "      <td>2.106960</td>\n",
       "      <td>2.552151</td>\n",
       "      <td>1.821394</td>\n",
       "      <td>0.516371</td>\n",
       "      <td>-2.088084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.413400</td>\n",
       "      <td>2.032628</td>\n",
       "      <td>-0.006751</td>\n",
       "      <td>1.525444</td>\n",
       "      <td>-0.754306</td>\n",
       "      <td>-2.074172</td>\n",
       "      <td>-3.057952</td>\n",
       "      <td>1.717090</td>\n",
       "      <td>-0.970531</td>\n",
       "      <td>-0.126739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.911358</td>\n",
       "      <td>0.409554</td>\n",
       "      <td>1.801092</td>\n",
       "      <td>2.508711</td>\n",
       "      <td>0.555396</td>\n",
       "      <td>1.461679</td>\n",
       "      <td>1.617921</td>\n",
       "      <td>1.156700</td>\n",
       "      <td>0.860279</td>\n",
       "      <td>-1.815634</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.458633</td>\n",
       "      <td>0.550587</td>\n",
       "      <td>1.575503</td>\n",
       "      <td>2.240396</td>\n",
       "      <td>-0.132238</td>\n",
       "      <td>-1.989019</td>\n",
       "      <td>-1.310395</td>\n",
       "      <td>2.471140</td>\n",
       "      <td>-1.049675</td>\n",
       "      <td>0.338686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.915477</td>\n",
       "      <td>1.071991</td>\n",
       "      <td>1.447053</td>\n",
       "      <td>3.872259</td>\n",
       "      <td>1.034379</td>\n",
       "      <td>2.023182</td>\n",
       "      <td>1.432221</td>\n",
       "      <td>0.486669</td>\n",
       "      <td>-0.324363</td>\n",
       "      <td>-0.316492</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.827034</td>\n",
       "      <td>1.895121</td>\n",
       "      <td>-0.388996</td>\n",
       "      <td>1.900664</td>\n",
       "      <td>0.065443</td>\n",
       "      <td>-1.141070</td>\n",
       "      <td>-1.927792</td>\n",
       "      <td>1.595579</td>\n",
       "      <td>-0.142450</td>\n",
       "      <td>-0.374619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.431598</td>\n",
       "      <td>1.751323</td>\n",
       "      <td>1.036286</td>\n",
       "      <td>4.552248</td>\n",
       "      <td>-0.327865</td>\n",
       "      <td>-0.004574</td>\n",
       "      <td>1.287302</td>\n",
       "      <td>-0.500691</td>\n",
       "      <td>-1.073438</td>\n",
       "      <td>-1.240185</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.950429</td>\n",
       "      <td>2.444352</td>\n",
       "      <td>1.486849</td>\n",
       "      <td>1.694134</td>\n",
       "      <td>-1.108196</td>\n",
       "      <td>-0.602699</td>\n",
       "      <td>-1.547576</td>\n",
       "      <td>3.101694</td>\n",
       "      <td>0.400036</td>\n",
       "      <td>-0.000953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.752660</td>\n",
       "      <td>1.733307</td>\n",
       "      <td>1.712471</td>\n",
       "      <td>1.633501</td>\n",
       "      <td>-0.840243</td>\n",
       "      <td>0.003783</td>\n",
       "      <td>2.205566</td>\n",
       "      <td>2.245139</td>\n",
       "      <td>0.219095</td>\n",
       "      <td>-1.854049</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.821422</td>\n",
       "      <td>1.508105</td>\n",
       "      <td>0.029575</td>\n",
       "      <td>3.466185</td>\n",
       "      <td>-0.427217</td>\n",
       "      <td>-2.435753</td>\n",
       "      <td>-1.119000</td>\n",
       "      <td>0.712866</td>\n",
       "      <td>-1.061971</td>\n",
       "      <td>0.263015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19705</th>\n",
       "      <td>-2.902460</td>\n",
       "      <td>1.740900</td>\n",
       "      <td>2.163507</td>\n",
       "      <td>3.490622</td>\n",
       "      <td>0.939976</td>\n",
       "      <td>1.804377</td>\n",
       "      <td>2.560888</td>\n",
       "      <td>1.353295</td>\n",
       "      <td>-0.499162</td>\n",
       "      <td>-1.898771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.303553</td>\n",
       "      <td>1.572323</td>\n",
       "      <td>-0.434638</td>\n",
       "      <td>2.738880</td>\n",
       "      <td>-1.137716</td>\n",
       "      <td>-2.152367</td>\n",
       "      <td>-2.406308</td>\n",
       "      <td>1.453490</td>\n",
       "      <td>-0.187996</td>\n",
       "      <td>1.601338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19706</th>\n",
       "      <td>-2.356663</td>\n",
       "      <td>0.942000</td>\n",
       "      <td>2.887390</td>\n",
       "      <td>2.679152</td>\n",
       "      <td>0.134619</td>\n",
       "      <td>1.345732</td>\n",
       "      <td>0.769376</td>\n",
       "      <td>0.559078</td>\n",
       "      <td>0.101078</td>\n",
       "      <td>-1.326284</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.175369</td>\n",
       "      <td>1.377228</td>\n",
       "      <td>0.937739</td>\n",
       "      <td>3.535929</td>\n",
       "      <td>-0.800291</td>\n",
       "      <td>-2.653674</td>\n",
       "      <td>-2.857182</td>\n",
       "      <td>2.988899</td>\n",
       "      <td>-0.237624</td>\n",
       "      <td>0.793541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19707</th>\n",
       "      <td>-2.637773</td>\n",
       "      <td>0.749630</td>\n",
       "      <td>2.165326</td>\n",
       "      <td>3.366657</td>\n",
       "      <td>-1.072513</td>\n",
       "      <td>1.090916</td>\n",
       "      <td>3.868383</td>\n",
       "      <td>0.457475</td>\n",
       "      <td>-0.581894</td>\n",
       "      <td>-2.917851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215782</td>\n",
       "      <td>2.137603</td>\n",
       "      <td>0.429043</td>\n",
       "      <td>-0.074911</td>\n",
       "      <td>-0.784254</td>\n",
       "      <td>-2.052609</td>\n",
       "      <td>-1.813932</td>\n",
       "      <td>1.436829</td>\n",
       "      <td>0.376520</td>\n",
       "      <td>-0.170649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19708</th>\n",
       "      <td>-1.897598</td>\n",
       "      <td>0.158943</td>\n",
       "      <td>3.194553</td>\n",
       "      <td>3.641513</td>\n",
       "      <td>-0.887907</td>\n",
       "      <td>1.153879</td>\n",
       "      <td>2.913718</td>\n",
       "      <td>0.782875</td>\n",
       "      <td>1.262358</td>\n",
       "      <td>0.169749</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.035436</td>\n",
       "      <td>2.242933</td>\n",
       "      <td>0.402399</td>\n",
       "      <td>2.037804</td>\n",
       "      <td>-0.124317</td>\n",
       "      <td>-2.031441</td>\n",
       "      <td>-3.417457</td>\n",
       "      <td>2.160957</td>\n",
       "      <td>0.867672</td>\n",
       "      <td>1.033926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19709</th>\n",
       "      <td>-1.755909</td>\n",
       "      <td>0.521917</td>\n",
       "      <td>2.195981</td>\n",
       "      <td>1.703635</td>\n",
       "      <td>1.012074</td>\n",
       "      <td>1.326659</td>\n",
       "      <td>1.811861</td>\n",
       "      <td>-0.202989</td>\n",
       "      <td>0.180799</td>\n",
       "      <td>0.198041</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.016682</td>\n",
       "      <td>2.128467</td>\n",
       "      <td>0.849900</td>\n",
       "      <td>3.001059</td>\n",
       "      <td>-0.504313</td>\n",
       "      <td>-1.926787</td>\n",
       "      <td>-2.650674</td>\n",
       "      <td>0.910218</td>\n",
       "      <td>0.200215</td>\n",
       "      <td>0.995429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19710 rows Ã— 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -1.026509  2.504253  0.503871  3.906834 -1.673985  2.106960  2.552151   \n",
       "1     -1.911358  0.409554  1.801092  2.508711  0.555396  1.461679  1.617921   \n",
       "2     -1.915477  1.071991  1.447053  3.872259  1.034379  2.023182  1.432221   \n",
       "3     -1.431598  1.751323  1.036286  4.552248 -0.327865 -0.004574  1.287302   \n",
       "4     -0.752660  1.733307  1.712471  1.633501 -0.840243  0.003783  2.205566   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19705 -2.902460  1.740900  2.163507  3.490622  0.939976  1.804377  2.560888   \n",
       "19706 -2.356663  0.942000  2.887390  2.679152  0.134619  1.345732  0.769376   \n",
       "19707 -2.637773  0.749630  2.165326  3.366657 -1.072513  1.090916  3.868383   \n",
       "19708 -1.897598  0.158943  3.194553  3.641513 -0.887907  1.153879  2.913718   \n",
       "19709 -1.755909  0.521917  2.195981  1.703635  1.012074  1.326659  1.811861   \n",
       "\n",
       "              7         8         9  ...       758       759       760  \\\n",
       "0      1.821394  0.516371 -2.088084  ... -0.413400  2.032628 -0.006751   \n",
       "1      1.156700  0.860279 -1.815634  ... -1.458633  0.550587  1.575503   \n",
       "2      0.486669 -0.324363 -0.316492  ... -0.827034  1.895121 -0.388996   \n",
       "3     -0.500691 -1.073438 -1.240185  ... -2.950429  2.444352  1.486849   \n",
       "4      2.245139  0.219095 -1.854049  ... -0.821422  1.508105  0.029575   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "19705  1.353295 -0.499162 -1.898771  ... -0.303553  1.572323 -0.434638   \n",
       "19706  0.559078  0.101078 -1.326284  ... -1.175369  1.377228  0.937739   \n",
       "19707  0.457475 -0.581894 -2.917851  ... -0.215782  2.137603  0.429043   \n",
       "19708  0.782875  1.262358  0.169749  ... -1.035436  2.242933  0.402399   \n",
       "19709 -0.202989  0.180799  0.198041  ... -2.016682  2.128467  0.849900   \n",
       "\n",
       "            761       762       763       764       765       766       767  \n",
       "0      1.525444 -0.754306 -2.074172 -3.057952  1.717090 -0.970531 -0.126739  \n",
       "1      2.240396 -0.132238 -1.989019 -1.310395  2.471140 -1.049675  0.338686  \n",
       "2      1.900664  0.065443 -1.141070 -1.927792  1.595579 -0.142450 -0.374619  \n",
       "3      1.694134 -1.108196 -0.602699 -1.547576  3.101694  0.400036 -0.000953  \n",
       "4      3.466185 -0.427217 -2.435753 -1.119000  0.712866 -1.061971  0.263015  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "19705  2.738880 -1.137716 -2.152367 -2.406308  1.453490 -0.187996  1.601338  \n",
       "19706  3.535929 -0.800291 -2.653674 -2.857182  2.988899 -0.237624  0.793541  \n",
       "19707 -0.074911 -0.784254 -2.052609 -1.813932  1.436829  0.376520 -0.170649  \n",
       "19708  2.037804 -0.124317 -2.031441 -3.417457  2.160957  0.867672  1.033926  \n",
       "19709  3.001059 -0.504313 -1.926787 -2.650674  0.910218  0.200215  0.995429  \n",
       "\n",
       "[19710 rows x 768 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "BERT embedding sample",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
