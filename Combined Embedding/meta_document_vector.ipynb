{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "time_series_analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahandv/science_science/blob/master/Combined%20Embedding/meta_document_vector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEtsmUJkczAC"
      },
      "source": [
        "# Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkerTjGRc48S"
      },
      "source": [
        "# datapath = '/home/sahand/GoogleDrive/Data/' # Local\n",
        "# datapath = '/mnt/16A4A9BCA4A99EAD/GoogleDrive/Data/' # Local\n",
        "datapath = 'drive/My Drive/Data/' # Remote"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mqrqlHBc8lm",
        "outputId": "23d3c2bd-6867-4559-ec54-31386e27fb43"
      },
      "source": [
        "!rm -rf 'science_science'\n",
        "!rm -rf 'science_science'\n",
        "Username = \"sahandv\"#@param {type:\"string\"}\n",
        "Token = \"\"#@param {type:\"string\"}\n",
        "\n",
        "!git clone https://$Username:$Token@github.com/sahandv/science_science.git\n",
        "!ls"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'science_science'...\n",
            "remote: Enumerating objects: 1983, done.\u001b[K\n",
            "remote: Counting objects: 100% (647/647), done.\u001b[K\n",
            "remote: Compressing objects: 100% (569/569), done.\u001b[K\n",
            "remote: Total 1983 (delta 406), reused 310 (delta 71), pack-reused 1336\u001b[K\n",
            "Receiving objects: 100% (1983/1983), 168.87 MiB | 28.07 MiB/s, done.\n",
            "Resolving deltas: 100% (1205/1205), done.\n",
            "Checking out files: 100% (217/217), done.\n",
            "drive  sample_data  science_science\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG1NeDL6c8Rb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "880d5a79-3d0a-44df-d0d5-5b84b37718ee"
      },
      "source": [
        "# NOT REQUIRED. Unless want to save model to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q91Y6F9ydBmc",
        "outputId": "a6098dec-f89d-4aba-be20-4ae979f47a00"
      },
      "source": [
        "!pip install -r 'science_science/requirements (colab).txt'"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 1)) (4.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 3)) (1.1.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 4)) (3.2.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 5)) (2.2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 6)) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 8)) (0.11.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 9)) (1.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 10)) (3.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 12)) (0.8.9)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 13)) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 14)) (2.5.1)\n",
            "Requirement already satisfied: netgraph in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 15)) (4.0.4)\n",
            "Requirement already satisfied: yellowbrick in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 16)) (0.9.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 17)) (4.4.1)\n",
            "Requirement already satisfied: stellargraph in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 18)) (1.2.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 19)) (0.10.3)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 20)) (1.3.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements (colab).txt (line 21)) (0.10.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (0.3.4)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (5.1.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (3.12.4)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (21.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r science_science/requirements (colab).txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r science_science/requirements (colab).txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements (colab).txt (line 5)) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements (colab).txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements (colab).txt (line 5)) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements (colab).txt (line 5)) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements (colab).txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements (colab).txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements (colab).txt (line 5)) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements (colab).txt (line 5)) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements (colab).txt (line 5)) (57.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements (colab).txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r science_science/requirements (colab).txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r science_science/requirements (colab).txt (line 10)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r science_science/requirements (colab).txt (line 10)) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r science_science/requirements (colab).txt (line 10)) (2.4.7)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->-r science_science/requirements (colab).txt (line 13)) (5.1.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->-r science_science/requirements (colab).txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: grandalf in /usr/local/lib/python3.7/dist-packages (from netgraph->-r science_science/requirements (colab).txt (line 15)) (0.7)\n",
            "Requirement already satisfied: rectangle-packer in /usr/local/lib/python3.7/dist-packages (from netgraph->-r science_science/requirements (colab).txt (line 15)) (2.0.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->-r science_science/requirements (colab).txt (line 17)) (1.3.3)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from stellargraph->-r science_science/requirements (colab).txt (line 18)) (2.5.0)\n",
            "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (1.53.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->-r science_science/requirements (colab).txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->-r science_science/requirements (colab).txt (line 5)) (4.5.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (0.36.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (2.5.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (3.3.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (0.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (2.5.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (0.2.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (1.34.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (3.7.4.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (1.12)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (1.12.1)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (1.1.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (1.31.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (0.6.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (1.5.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.1.0->stellargraph->-r science_science/requirements (colab).txt (line 18)) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQj_YbU5c6uC"
      },
      "source": [
        "## Import\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5o7GgNXIZD0"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon May  3 15:46:31 2021\n",
        "\n",
        "@author: github.com/sahandv\n",
        "\"\"\"\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer,WordpieceTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tokenizers import Tokenizer,BertWordPieceTokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from science_science.sciosci.assets import text_assets as ta\n",
        "from science_science.sciosci.assets import ann_assets as anna\n",
        "from sklearn.model_selection import train_test_split\n",
        "# !wget 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt'\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "tokenizer = 'word'\n",
        "embedding_dim = 128\n",
        "num_epochs = 500\n",
        "vocab_limit = 50000\n",
        "min_paragraph_len = 35  #percentage of each paragraph\n",
        "n_inputs = 3 #network type selection\n",
        "batch_size = 512\n",
        "\n",
        "# =============================================================================\n",
        "# Prepare GPU\n",
        "# =============================================================================\n",
        "from numba import cuda \n",
        "device = cuda.get_current_device()\n",
        "device.reset()\n",
        "\n",
        "\n",
        "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
        "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "# =============================================================================\n",
        "## Next Word Prediction task\n",
        "# https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/B80b0/notebook-for-lesson-1\n",
        "# \n",
        "# For character prediction see\n",
        "# https://www.tensorflow.org/tutorials/text/text_generation\n",
        "# =============================================================================\n",
        "# dir_root = '/mnt/6016589416586D52/Users/z5204044/GoogleDrive/GoogleDrive/Data/Corpus/cora-classify/cora/' # C1314\n",
        "# dir_root = '/mnt/16A4A9BCA4A99EAD/GoogleDrive/Data/Corpus/cora-classify/cora/' # Ryzen\n",
        "dir_root = 'drive/MyDrive/Data/Corpus/cora-classify/cora/' # Colab\n",
        "\n",
        "    # =============================================================================\n",
        "    # Load and prepare features\n",
        "    # =============================================================================\n",
        "corpus_idx = pd.read_csv(dir_root+'clean/single_component_small_18k/corpus_idx_original')['id'].values.tolist()\n",
        "net_vecs = pd.read_csv(dir_root+'embeddings/single_component_small_18k/n2v 300-70-20 p1q05').drop('Unnamed: 0',axis=1)\n",
        "net_vecs.columns = ['net_cid_'+str(x) for x in range(len(net_vecs.columns))]\n",
        "\n",
        "data_path_rel = dir_root+'extractions_with_unique_id_labeled_single_component.csv'\n",
        "data = pd.read_csv(data_path_rel)\n",
        "data = data[data['id'].isin(corpus_idx)]\n",
        "    # =============================================================================\n",
        "    # Load and tokenize text\n",
        "    # =============================================================================\n",
        "corpus = pd.read_csv(dir_root+'clean/single_component_small_18k/abstract_title super duper pure',names=['abstract'])\n",
        "corpus['abstract'] = \"[documentembeddingtoken] \"+corpus['abstract'] \n",
        "# corpus.to_csv(dir_root+'clean/single_component_small_18k/abstract_title super duper pure with [DOC]',header=False,index=False)\n",
        "corpus = corpus['abstract'].values.tolist()\n",
        "\n",
        "text_lens = np.array([len(p.split()) for p in corpus])\n",
        "max_paragraph_len = int(np.percentile(text_lens, 95)) # take Nth percentile as the sentence length threshold\n",
        "\n",
        "# corpus = [\n",
        "#     '1 red brown fox',\n",
        "#     'red brown cat!',\n",
        "#     'red cat loves chicken wings.',\n",
        "#     'black cat loves chicken wings as well',\n",
        "#     'brown fox hates cats'\n",
        "#     ]\n",
        "\n",
        "n_docs = len(corpus_idx)+1\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n9W-HDZIh2D",
        "outputId": "1145fc12-e32d-4f36-b442-548dd06bc944",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if tokenizer=='word':\n",
        "##################\n",
        "# TF word tokenize\n",
        "##################\n",
        "    # oov is out of vocabulary token replacement. you can num_words=int\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='[OOV_TKN]',num_words=vocab_limit)#(oov_token='<oov_tkn>') \n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    total_words = len(tokenizer.word_index)+1\n",
        "    word_index = tokenizer.word_index\n",
        "    print(total_words)\n",
        "    # print(tokenizer.word_index)\n",
        "    print(tokenizer.word_index['documentembeddingtoken'])\n",
        "    \n",
        "    \n",
        "    ##################\n",
        "    # unlimited length\n",
        "    ##################\n",
        "    # extract n-gram sequences from n=2 to n=number_of_grams_in_sentences \n",
        "    input_sequences = []\n",
        "    input_sequences_doc_id = []\n",
        "    for cid,sent in tqdm(enumerate(corpus),total=len(corpus)):\n",
        "        token_list = tokenizer.texts_to_sequences([sent])[0]\n",
        "        for i in range(int(len(token_list)*min_paragraph_len/100)-1,len(token_list)):\n",
        "            n_gram_sentence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sentence)   \n",
        "            input_sequences_doc_id.append(cid)\n",
        "    ##################\n",
        "    # limited length\n",
        "    ##################\n",
        "    # OR extract n-gram sequences from n=2 to n=min(number_of_grams_in_sentences,max_paragraph_len )\n",
        "    input_sequences = []\n",
        "    input_sequences_doc_id = []\n",
        "    for cid,sent in tqdm(enumerate(corpus),total=len(corpus)):\n",
        "        token_list = tokenizer.texts_to_sequences([sent])[0]\n",
        "        for i in range(int(len(token_list)*min_paragraph_len/100)-1,min(len(token_list),max_paragraph_len)):\n",
        "            n_gram_sentence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sentence)\n",
        "            input_sequences_doc_id.append(cid)\n",
        "\n",
        "n_classes = vocab_limit#total_words\n",
        "\n",
        "if tokenizer=='bpe':\n",
        "##################\n",
        "# BPE tokenize\n",
        "##################\n",
        "    tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
        "    tokenizer.enable_padding(pad_id=3, pad_token=\"[PAD]\")\n",
        "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
        "    tokenizer.decoder = decoders.ByteLevel()\n",
        "    tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
        "    trainer = trainers.BpeTrainer(vocab_size=20000, min_frequency=2,special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"[documentembeddingtoken]\"])\n",
        "    tokenizer.train([\n",
        "    # \t\"./path/to/dataset/1.txt\",\n",
        "    # \t\"./path/to/dataset/2.txt\",\n",
        "    \tdir_root+'clean/single_component_small_18k/abstract_title super duper pure'\n",
        "    ], trainer=trainer)\n",
        "    tokenizer.save(dir_root+\"clean/single_component_small_18k/byte-level-bpe.tokenizer.json\", pretty=True)\n",
        "    \n",
        "    tokenizer = Tokenizer.from_file(dir_root+\"clean/single_component_small_18k/byte-level-bpe.tokenizer.json\")\n",
        "    output = tokenizer.encode_batch([\"I can feel the magic, can you?\"])\n",
        "    print(output[0].tokens)\n",
        "    print(output[0].ids)\n",
        "\n",
        "if tokenizer=='bert':\n",
        "##################\n",
        "# BERT WP tokenize\n",
        "##################\n",
        "    tokenizer = BertWordPieceTokenizer(\"data/vocabs/bert-base-uncased-vocab.txt\", lowercase=True)\n",
        "\n",
        "\n",
        "\n",
        "    # =============================================================================\n",
        "    # Prepare sequences\n",
        "    # =============================================================================\n",
        "print(\"\\nPreparing sequences\")\n",
        "revers_word_index = ta.reverse_word_index(word_index)\n",
        "\n",
        "# you can add maxlen=int. you can add padding='post', default is 'pre'. you can truncate='post' to remove from the end, default is 'pre' again\n",
        "max_seq_len = max([len(x) for x in input_sequences])\n",
        "# max_seq_len = int(max_paragraph_len)\n",
        "input_sequences = pad_sequences(input_sequences,maxlen=max_seq_len,padding='pre')\n",
        "input_sequences = np.array(input_sequences)\n",
        "input_sequences_tmp = input_sequences[:3000,:]\n",
        "\n",
        "    # =============================================================================\n",
        "    # Prepare model inputs and outputs\n",
        "    # =============================================================================\n",
        "# create X and Y\n",
        "X,labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "input_df = pd.DataFrame(X)\n",
        "input_df['Y'] = labels\n",
        "input_df['corpus_index'] = input_sequences_doc_id\n",
        "input_df_sample = input_df.sample(10)\n",
        "x2 = net_vecs.values\n",
        "\n",
        "# shuffle dataset\n",
        "input_df = input_df.sample(frac=1)\n",
        "\n",
        "# split train test\n",
        "msk = np.random.rand(len(input_df)) < 0.8\n",
        "train = input_df[msk]\n",
        "test = input_df[~msk]\n",
        "\n",
        "\n",
        "train_corpus_idx = train['corpus_index'].values\n",
        "train_y = train['Y'].values\n",
        "# train_y_cat = tf.keras.utils.to_categorical(train_y, num_classes=n_classes)\n",
        "train_x1 = train[list(range(max_seq_len-1))].values\n",
        "train_x2 = pd.DataFrame(train['corpus_index']).reset_index()\n",
        "train_x2.columns = ['train_index','index']\n",
        "train_x2 = train_x2.merge(net_vecs.reset_index(),on='index',how='left').drop('index',axis=1)\n",
        "train_x2 = train_x2.drop('train_index',axis=1).values\n",
        "\n",
        "test_corpus_idx = test['corpus_index'].values\n",
        "test_y = test['Y'].values\n",
        "# test_y_cat = tf.keras.utils.to_categorical(test_y, num_classes=n_classes)\n",
        "test_x1 = test[list(range(max_seq_len-1))].values\n",
        "# test_x2 = pd.DataFrame(test['corpus_index']).reset_index().merge(net_vecs.reset_index(),on='index',how='left').drop('index',axis=1).drop('corpus_index',axis=1).values\n",
        "test_x2 = pd.DataFrame(test['corpus_index']).reset_index()\n",
        "test_x2.columns = ['test_index','index']\n",
        "test_x2 = test_x2.merge(net_vecs.reset_index(),on='index',how='left').drop('index',axis=1)\n",
        "test_x2 = test_x2.drop('test_index',axis=1).values\n",
        "\n",
        "# corpus_idx = train_corpus_idx\n",
        "# y = train_y\n",
        "\n",
        "# Xtrain, Xtest, Labeltrain, Labeltest = train_test_split(X, labels, test_size=0.2, random_state=100,shuffle=True)\n",
        "# ys = tf.keras.utils.to_categorical(labels,num_classes=total_words) # Not suitable for large data\n",
        "\n",
        "# del train, test, msk, input_df, X, x2\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##################\n",
        "# Generator object - updated - reference https://medium.com/analytics-vidhya/write-your-own-custom-data-generator-for-tensorflow-keras-1252b64e41c3\n",
        "##################\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    \"\"\"\n",
        "    Generates data for Keras\n",
        "    Sequence based data generator.\n",
        "    \"\"\"\n",
        "    def __init__(self,x1,x2,y=None,corpus_idx=None,n_classes:int=None,n_docs:int=None,batch_size=5,to_fit=True,n_inputs=3):\n",
        "        self.x1 = x1\n",
        "        self.x2 = x2\n",
        "        self.y = y\n",
        "        self.n_classes = n_classes\n",
        "        self.n_docs = n_docs\n",
        "        self.corpus_idx = corpus_idx\n",
        "        self.batch_size = batch_size\n",
        "        self.to_fit = to_fit\n",
        "        self.ids = list(range(len(x1)))\n",
        "        self.n_inputs = n_inputs\n",
        "        self.IDmemory = None\n",
        "        self.Imemory = None\n",
        "        \n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.x1.shape[0] / self.batch_size))\n",
        "    \n",
        "    # def __getitem__(self, index):\n",
        "    #     \"\"\"\n",
        "    #     Generate one batch of data\n",
        "        \n",
        "    #     Parameters\n",
        "    #     -------\n",
        "    #     index: index of the batch\n",
        "        \n",
        "    #     Returns\n",
        "    #     -------\n",
        "    #     X and y when fitting. X only when predicting\n",
        "    #     W\n",
        "    #     Exptected network inputs:\n",
        "    #        [ inputs_seq , inputs_doc , inputs_netvec ]\n",
        "        \n",
        "    #     \"\"\"\n",
        "    #     ID_list = self.ids[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "    #     self.Imemory = index\n",
        "        \n",
        "    #     input_1 = self._generate_input_1(ID_list)\n",
        "    #     input_2 = self._generate_input_2(ID_list)\n",
        "    #     input_3 = self._generate_input_3(ID_list)\n",
        "        \n",
        "    #     if self.to_fit:\n",
        "    #         y = self._generate_y(ID_list)\n",
        "    #         return [input_1,input_2,input_3], y\n",
        "    #     else:\n",
        "    #         return [input_1,input_2,input_3]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Generate one batch of data\n",
        "        \n",
        "        Parameters\n",
        "        -------\n",
        "        index: index of the batch\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        X and y when fitting. X only when predicting\n",
        "        W\n",
        "        Exptected network inputs:\n",
        "           [ inputs_seq , inputs_doc , inputs_netvec ]\n",
        "        \n",
        "        \"\"\"\n",
        "        ID_list = self.ids[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        self.Imemory = index\n",
        "        \n",
        "        inputs_1,inputs_2,inputs_3 = self._generate_input_123(ID_list)\n",
        "\n",
        "        y = self._generate_y(ID_list)\n",
        "        if self.n_inputs==1:\n",
        "            return [inputs_2,inputs_3], y\n",
        "        if self.n_inputs==2:\n",
        "            return [inputs_1,inputs_3], y\n",
        "        if self.n_inputs==3:\n",
        "            return [inputs_1,inputs_2,inputs_3], y\n",
        "\n",
        "    \n",
        "    def _generate_y(self, ID_list):\n",
        "        y_batch = np.empty((self.batch_size), dtype=int)\n",
        "        for n,i in enumerate(ID_list):\n",
        "            y_batch[n] = self.y[i]\n",
        "        return tf.keras.utils.to_categorical(y_batch, num_classes=self.n_classes) \n",
        "        \n",
        "    def _generate_input_1(self, ID_list): \n",
        "        #inputs_seq\n",
        "        # self.IDmemory = ID_list\n",
        "        \n",
        "        x_batch = np.empty((self.batch_size,self.x1.shape[1]), dtype=int)\n",
        "        for n,i in enumerate(ID_list):\n",
        "            x_batch[n,] = self.x1[i]\n",
        "        return(x_batch) \n",
        "       \n",
        "    def _generate_input_2(self, ID_list): \n",
        "        #inputs_doc\n",
        "        x_batch = np.empty((self.batch_size,1), dtype=int)\n",
        "        for n,i in enumerate(ID_list):\n",
        "            x_batch[n,] = [self.corpus_idx[i]]\n",
        "        # return(tf.keras.utils.to_categorical(x_batch, num_classes=self.n_docs) )\n",
        "        return(x_batch)\n",
        "    \n",
        "    def _generate_input_3(self, ID_list): \n",
        "        #inputs_netvec\n",
        "        x_batch = np.empty((self.batch_size,self.x2.shape[1]), dtype=int)\n",
        "        for n,i in enumerate(ID_list):\n",
        "            x_batch[n,] = self.x2[i]\n",
        "        return(x_batch) \n",
        "\n",
        "    def _generate_input_123(self, ID_list):\n",
        "        x1_batch = np.empty((self.batch_size,self.x1.shape[1]), dtype=int)\n",
        "        x2_batch = np.empty((self.batch_size,1), dtype=int)\n",
        "        x3_batch = np.empty((self.batch_size,self.x2.shape[1]), dtype=int)\n",
        "        for n,i in enumerate(ID_list):\n",
        "            x1_batch[n,] = self.x1[i]\n",
        "            x2_batch[n,] = [self.corpus_idx[i]]\n",
        "            x3_batch[n,] = self.x2[i]\n",
        "        # x2_batch_f = tf.keras.utils.to_categorical(x2_batch, num_classes=self.n_docs) \n",
        "        return x1_batch,x2_batch,x3_batch\n",
        "        \n",
        "            \n",
        "        \n",
        "    # def _generate_input_3(self, ID_list): \n",
        "    #     #inputs_netvec\n",
        "    #     x_batch = np.empty((self.batch_size,self.x2.shape[1]), dtype=int)\n",
        "    #     for i in ID_list:\n",
        "    #         x_batch[i,] = self.x2[self.corpus_idx[i]]\n",
        "    #     return(x_batch)\n",
        "\n",
        "train_dataset = DataGenerator(x1=train_x1, x2=train_x2,y=train_y,n_inputs=n_inputs,\n",
        "                              n_classes=n_classes,n_docs=n_docs,corpus_idx=train_corpus_idx,batch_size=batch_size)\n",
        "valid_dataset = DataGenerator(x1=test_x1, x2=train_x2,y=test_y,n_inputs=n_inputs,\n",
        "                              n_classes=n_classes,n_docs=n_docs,corpus_idx=test_corpus_idx,batch_size=batch_size)\n",
        "\n",
        "# for item in train_dataset:\n",
        "#     print(item)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 306/18838 [00:00<00:06, 3055.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "45044\n",
            "15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 18838/18838 [00:13<00:00, 1446.65it/s]\n",
            "100%|██████████| 18838/18838 [00:11<00:00, 1679.86it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Preparing sequences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EczJLQk9KN5u"
      },
      "source": [
        "\n",
        "    # =============================================================================\n",
        "    # Network 1 inputs (netvec & docvec)\n",
        "    # =============================================================================\n",
        "if n_inputs==1:\n",
        "    inputs_doc = tf.keras.Input(shape=(1,), name='input_2')\n",
        "    x_12 = tf.keras.layers.Embedding(n_docs,embedding_dim,input_length=1,name='doc_embedding')(inputs_doc)\n",
        "    x_12 = tf.keras.layers.Flatten(name='doc_flatten')(x_12)\n",
        "    x_12 = tf.keras.layers.Dense(15,activation='relu')(x_12)\n",
        "    \n",
        "    # x_12 = tf.keras.layers.Reshape((100,))(x_12)\n",
        "    # # x_12 = tf.keras.layers.Dense(15,activation='relu')(x_12)\n",
        "    \n",
        "    # x_12 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(10))(x_12)\n",
        "    x_12 = tf.keras.Model(inputs=inputs_doc, outputs=x_12)\n",
        "    \n",
        "    \n",
        "    inputs_netvec = tf.keras.Input(shape=(300,), name='input_3')\n",
        "    x_2 = tf.keras.layers.Dense(128,activation='relu',name='net_vec_dense_1')(inputs_netvec)\n",
        "    x_2 = tf.keras.Model(inputs=inputs_netvec, outputs=x_2)\n",
        "    \n",
        "    x = tf.keras.layers.concatenate([x_12.output, x_2.output], name='concatenate')\n",
        "    x = tf.keras.layers.Dense(128,activation='relu',name='main_dense_1')(x)\n",
        "    # x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
        "    outputs = tf.keras.layers.Dense(n_classes, activation=\"softmax\",name='final_dense')(x)\n",
        "    \n",
        "    model = keras.Model(inputs=[x_12.input, x_2.input], outputs=outputs)\n",
        "    \n",
        "    adam = tf.keras.optimizers.Adam(lr=0.01)\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    tf.keras.utils.plot_model(model, to_file='combined_embedding.png', show_shapes=True, show_layer_names=True)\n",
        "    \n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy',patience=35)\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint('./models/pretrain_next_word_pred.h5', monitor='accuracy', mode='min', save_best_only=True)\n",
        "    tensorboard = tf.keras.callbacks.TensorBoard(\n",
        "                              log_dir='.\\logs',\n",
        "                              histogram_freq=1,\n",
        "                              write_images=True\n",
        "                            )\n",
        "\n",
        "    # =============================================================================\n",
        "    # Network 2 inputs (netvec & wordvec)\n",
        "    # =============================================================================\n",
        "if n_inputs==2:\n",
        "    inputs_seq = tf.keras.Input(shape=(max_seq_len-1,), name='input_1')\n",
        "    x_11 = tf.keras.layers.Embedding(n_classes,embedding_dim,input_length=max_seq_len-1,name='token_embedding')(inputs_seq)\n",
        "    x_11 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100,return_sequences=False,name='token_LSTM'),name='token_bidirectional')(x_11)\n",
        "    # x_11 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50))(x_11)\n",
        "    x_11 = tf.keras.layers.Dense(100,activation='relu',name='token_dense')(x_11)\n",
        "    x_11 = tf.keras.Model(inputs=inputs_seq, outputs=x_11)\n",
        "    \n",
        "    inputs_netvec = tf.keras.Input(shape=(300,), name='input_3')\n",
        "    x_2 = tf.keras.layers.Dense(128,activation='relu',name='net_vec_dense_1')(inputs_netvec)\n",
        "    x_2 = tf.keras.Model(inputs=inputs_netvec, outputs=x_2)\n",
        "    \n",
        "    x = tf.keras.layers.concatenate([x_11.output, x_2.output], name='concatenate')\n",
        "    x = tf.keras.layers.Dense(128,activation='relu',name='main_dense_1')(x)\n",
        "    # x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
        "    outputs = tf.keras.layers.Dense(n_classes, activation=\"softmax\",name='final_dense')(x)\n",
        "    \n",
        "    model = keras.Model(inputs=[x_11.input, x_2.input], outputs=outputs)\n",
        "    \n",
        "    adam = tf.keras.optimizers.Adam(lr=0.01)\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    tf.keras.utils.plot_model(model, to_file='combined_embedding.png', show_shapes=True, show_layer_names=True)\n",
        "    \n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy',patience=35)\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint('./models/pretrain_next_word_pred.h5', monitor='accuracy', mode='min', save_best_only=True)\n",
        "    tensorboard = tf.keras.callbacks.TensorBoard(\n",
        "                              log_dir='.\\logs',\n",
        "                              histogram_freq=1,\n",
        "                              write_images=True\n",
        "                            )\n",
        "\n",
        "    # =============================================================================\n",
        "    # Network 3 inputs (all)\n",
        "    # =============================================================================\n",
        "if n_inputs==3:\n",
        "    inputs_seq = tf.keras.Input(shape=(max_seq_len-1,), name='input_1')\n",
        "    x_11 = tf.keras.layers.Embedding(n_classes,embedding_dim,input_length=max_seq_len-1,name='token_embedding')(inputs_seq)\n",
        "    x_11 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200,return_sequences=True,name='token_LSTM_1'),name='token_bidirectional_1')(x_11)\n",
        "    x_11 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200,name='token_LSTM_2'),name='token_bidirectional_2')(x_11)\n",
        "    x_11 = tf.keras.layers.Dense(200,activation='relu',name='token_dense')(x_11)\n",
        "    x_11 = tf.keras.Model(inputs=inputs_seq, outputs=x_11)\n",
        "    \n",
        "    inputs_doc = tf.keras.Input(shape=(1,), name='input_2')\n",
        "    x_12 = tf.keras.layers.Embedding(n_docs,embedding_dim,input_length=1,name='doc_embedding')(inputs_doc)\n",
        "    x_12 = tf.keras.layers.Flatten(name='doc_flatten')(x_12)\n",
        "    x_12 = tf.keras.layers.Dense(50,activation='relu')(x_12)\n",
        "    # x_12 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(10))(x_12)\n",
        "    x_12 = tf.keras.Model(inputs=inputs_doc, outputs=x_12)\n",
        "    \n",
        "    inputs_netvec = tf.keras.Input(shape=(300,), name='input_3')\n",
        "    x_2 = tf.keras.layers.Dense(200,activation='relu',name='net_vec_dense_1')(inputs_netvec)\n",
        "    x_2 = tf.keras.layers.Dense(100,activation='relu',name='net_vec_dense_2')(x_2)\n",
        "    x_2 = tf.keras.Model(inputs=inputs_netvec, outputs=x_2)\n",
        "    \n",
        "    x = tf.keras.layers.concatenate([x_11.output, x_12.output, x_2.output], name='concatenate')\n",
        "    x = tf.keras.layers.Dense(300,activation='relu',name='main_dense_1')(x)\n",
        "    x = tf.keras.layers.Dense(150,activation='relu',name='main_dense_2')(x)\n",
        "    outputs = tf.keras.layers.Dense(n_classes, activation=\"softmax\",name='final_dense')(x)\n",
        "    \n",
        "    model = keras.Model(inputs=[x_11.input,x_12.input, x_2.input], outputs=outputs)\n",
        "    adam = tf.keras.optimizers.Adam(lr=0.01)\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    tf.keras.utils.plot_model(model, to_file='combined_embedding.png', show_shapes=True, show_layer_names=True)\n",
        "    \n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy',patience=25)\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint('./models/pretrain_next_word_pred.h5', monitor='accuracy', mode='min', save_best_only=True)\n",
        "    tensorboard = tf.keras.callbacks.TensorBoard(\n",
        "                              log_dir='.\\logs',\n",
        "                              histogram_freq=1,\n",
        "                              write_images=True\n",
        "                            )\n",
        " \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf1WQFFJIj_x"
      },
      "source": [
        "    # =============================================================================\n",
        "    # Train\n",
        "    # =============================================================================\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=num_epochs, \n",
        "                    validation_data=valid_dataset,\n",
        "                    verbose=1,\n",
        "                    callbacks=[callback, checkpoint,tensorboard])\n",
        "\n",
        "anna.plot_graphs(history,'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}