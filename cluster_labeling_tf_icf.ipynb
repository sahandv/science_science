{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cluster labeling tf-icf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO94hCJrSO0HPweE4dSyuBY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahandv/science_science/blob/master/cluster_labeling_tf_icf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Utd6fj1FOXv"
      },
      "source": [
        "# datapath = '/mnt/6016589416586D52/Users/z5204044/GoogleDrive/GoogleDrive/Data/' # Local\n",
        "datapath = 'drive/My Drive/Data/' # Remote"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9LRiG2ZFVhF"
      },
      "source": [
        "!rm -rf 'science_science'\n",
        "username = \"sahandv\"#@param {type:\"string\"}\n",
        "# password = \"\"#@param {type:\"string\"} \n",
        "\n",
        "!git clone https://github.com/$username/science_science.git\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3Q4RK3cFXgz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl1cQp7LFZ0k"
      },
      "source": [
        "!pip install -r 'science_science/requirements.txt'\n",
        "! pip install gensim==3.8.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74rz_bvDFdZm"
      },
      "source": [
        "import sys\n",
        "import time\n",
        "import gc\n",
        "import collections\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import pprint\n",
        "from random import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans, SpectralClustering, AffinityPropagation\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.manifold import TSNE\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from scipy import spatial,sparse,sign\n",
        "from sklearn.feature_extraction.text import TfidfTransformer , TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import preprocessing\n",
        "from bokeh.io import push_notebook, show, output_notebook, output_file\n",
        "from bokeh.plotting import figure\n",
        "from bokeh.models import ColumnDataSource, LabelSet\n",
        "from gensim.models import FastText as fasttext_gensim\n",
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "tqdm.pandas()\n",
        "\n",
        "from science_science.sciosci.assets import keyword_assets as kw\n",
        "from science_science.sciosci.assets import generic_assets as sci\n",
        "from science_science.sciosci.assets import advanced_assets as aa\n",
        "\n",
        "# from sciosci.assets import text_assets as kw\n",
        "# from sciosci.assets import generic_assets as sci\n",
        "# from sciosci.assets import advanced_assets as aa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-x957G2FfYz"
      },
      "source": [
        "#@markdown Don't forget to set the \"year_to\" to a year higher than intended!\n",
        "year_from = 1990#@param {type:\"number\"}\n",
        "year_to = 2020#@param {type:\"number\"}\n",
        "period = str(year_from)+'-'+str(year_to-1)\n",
        "#@markdown File address for main WoS file:\n",
        "# file_address = datapath+'Corpus/AI 4k/copyr_deflem_stopword_removed_thesaurus May 28/by period/n-gram by 6 repetition keywords/'+period+' abstract_title'\n",
        "file_address = datapath+'Corpus/AI 4k/copyr_deflem_stopword_removed_thesaurus May 28/1990-2019/1990-2019 n-gram by 6 repetition keywords'\n",
        "abstracts = pd.read_csv(file_address,names=['abstract'])\n",
        "print('period:',period,'\\n',abstracts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1Kq_CWhFhM1"
      },
      "source": [
        "def get_abstract_keywords(corpus,keywords_wanted,max_df=0.9,max_features=None):\n",
        "    cv=CountVectorizer(max_df=max_df,stop_words=stop_words, max_features=max_features, ngram_range=(1,1))\n",
        "    X=cv.fit_transform(corpus)\n",
        "    # get feature names\n",
        "    feature_names=cv.get_feature_names()\n",
        "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "    tfidf_transformer.fit(X)\n",
        "    keywords_tfidf = []\n",
        "    keywords_sorted = []\n",
        "    for doc in tqdm(corpus,total=len(corpus)):\n",
        "        tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
        "        sorted_items=kw.sort_coo(tf_idf_vector.tocoo())\n",
        "        keywords_sorted.append(sorted_items)\n",
        "        keywords_tfidf.append(kw.extract_topn_from_vector(feature_names,sorted_items,keywords_wanted))\n",
        "    return keywords_tfidf\n",
        "\n",
        "def get_corpus_top_keywords(abstract_keywords_dict=None):\n",
        "    if abstract_keywords_dict == None:\n",
        "        print(\"keywords should be provided\")\n",
        "        return False\n",
        "    terms = []\n",
        "    values = []\n",
        "    for doc in abstract_keywords_dict:\n",
        "        if doc != None:\n",
        "            terms = terms+list(doc.keys())\n",
        "            values = values+list(doc.values())\n",
        "    terms_df = pd.DataFrame({'terms':terms,'value':values}).groupby('terms').sum().sort_values('value',ascending=False)\n",
        "    return terms_df\n",
        "\n",
        "def find_max_item_value_in_all_cluster(haystack,needle,cluster_exception=None):\n",
        "    max_val = 0\n",
        "    max_index = None\n",
        "    counter = 0\n",
        "    for item in haystack:\n",
        "        try:\n",
        "            if item[needle]>max_val:\n",
        "                if cluster_exception==None:\n",
        "                    max_val = item[needle]\n",
        "                    max_index = counter\n",
        "                else:\n",
        "                    if cluster_exception != counter:\n",
        "                        max_val = item[needle] \n",
        "                        max_index = counter\n",
        "        except:\n",
        "            pass\n",
        "        counter+=1\n",
        "\n",
        "        if max_index!=None:\n",
        "            row_max = haystack[max_index][list(haystack[max_index].keys())[0]] # Will give the maximum value (first item) of the row with max value of the needle. This gives us a perspective to see how this score compares to the max in the same row.\n",
        "        else:\n",
        "            row_max = 0\n",
        "    # except:\n",
        "        # row_max = None\n",
        "    \n",
        "\n",
        "    return max_val,row_max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTw31pZ4FlP0"
      },
      "source": [
        "cluster_path = datapath+'Corpus/AI 4k/embeddings/clustering/k10/DEC/200,500,20/Doc2Vec patent_wos_ai corpus DEC 200,500,20 k10 labels'\n",
        "clusters_df = pd.read_csv(cluster_path)\n",
        "clusters_df.columns = ['clusters']\n",
        "clusters_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH_WWsYzFl3K"
      },
      "source": [
        "clusters = clusters_df.groupby('clusters').groups\n",
        "clusters.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNnhy0feFn_q"
      },
      "source": [
        "# TF-IDF (CTF-ICF)\n",
        "cluster_as_string = []\n",
        "year_abstracts = pd.read_csv(file_address,names=['abstract'])['abstract']\n",
        "clusters = clusters_df.groupby('clusters').groups\n",
        "for key in clusters.keys():\n",
        "    cluster_as_string.append(' '.join(year_abstracts[list(clusters[key])]))\n",
        "cluster_keywords_tfidf = get_abstract_keywords(cluster_as_string,1000,max_df=0.8)\n",
        "\n",
        "cluster_keywords = []\n",
        "cluster_index = 0\n",
        "for items in cluster_keywords_tfidf:\n",
        "    items_tmp = []\n",
        "    for item in items:\n",
        "        max_data = find_max_item_value_in_all_cluster(cluster_keywords_tfidf,item,cluster_index)\n",
        "        items_tmp.append(item+' ('+str(items[item])+' | '+str(max_data[0])+'/'+str(max_data[1])+')') # (item+' :'+str(items[item])+' / '+str( max of item in all other rows))\n",
        "    cluster_keywords.append(items_tmp)\n",
        "    cluster_index+=1\n",
        "pd.DataFrame(cluster_keywords).to_csv(cluster_path+\" keywords\",index=False,header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZwVZPUSFsSk"
      },
      "source": [
        "# Get term cluster labels (just terms and not scores)\n",
        "cluster_keywords_terms = []\n",
        "cluster_keywords_scores = []\n",
        "for item in cluster_keywords_tfidf:\n",
        "    cluster_keywords_terms.append(list(item.keys()))\n",
        "    cluster_keywords_scores.append(list(item.values()))\n",
        "\n",
        "pd.DataFrame(cluster_keywords_terms).T.to_csv(cluster_path+\" term_cluster\",index=False)\n",
        "pd.DataFrame(cluster_keywords_scores).T.to_csv(cluster_path+\" term_score\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejjJrhKDFtss"
      },
      "source": [
        "# Get term frequencies for each period\n",
        "terms = ' '.join(cluster_as_string).split()\n",
        "terms = [x for x in terms if x not in list(stop_words)]\n",
        "pd.DataFrame(terms,columns=['terms'])['terms'].value_counts().to_csv(cluster_path+\"  frequency\",header=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}