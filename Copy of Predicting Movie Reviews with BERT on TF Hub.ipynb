{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0a4mTk9o1Qg"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCpvgG0vwXAZ"
   },
   "source": [
    "#Predicting Movie Review Sentiment with BERT on TF Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiYrZKaHwV81"
   },
   "source": [
    "If you’ve been following Natural Language Processing over the past year, you’ve probably heard of BERT: Bidirectional Encoder Representations from Transformers. It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n",
    "\n",
    "Now that BERT's been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, [finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n",
    "\n",
    "Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in Tensorflow with tf hub. Some code was adapted from [this colab notebook](https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb). Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2517,
     "status": "ok",
     "timestamp": 1605730533800,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -660
    },
    "id": "hsZvic2YxnTz",
    "outputId": "b0dc82af-855f-42f2-fc05-7c2e37fd8e23"
   },
   "outputs": [],
   "source": [
    "# %tensorflow_version 1.x\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp5wfXDx5SPH"
   },
   "source": [
    "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4924,
     "status": "ok",
     "timestamp": 1605730536213,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -660
    },
    "id": "jviywGyWyKsA",
    "outputId": "a439ca62-3a50-4182-bfbc-402f4766f431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages (1.0.1)\r\n",
      "Requirement already satisfied: six in /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages (from bert-tensorflow) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5211,
     "status": "ok",
     "timestamp": 1605730536504,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -660
    },
    "id": "hhbGEfwgdEtw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVB3eOcjxxm1"
   },
   "source": [
    "Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n",
    "\n",
    "Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n",
    "\n",
    "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 989,
     "status": "ok",
     "timestamp": 1605730562549,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -660
    },
    "id": "US_EAnICvP7f",
    "outputId": "d57bde8d-6227-4805-c496-58e0ddf09ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: output_dir *****\n"
     ]
    }
   ],
   "source": [
    "# Set the output directory for saving model file\n",
    "# Optionally, set a GCP bucket location\n",
    "\n",
    "OUTPUT_DIR = 'output_dir'#@param {type:\"string\"}\n",
    "#@markdown Whether or not to clear/delete the directory and create a new one\n",
    "DO_DELETE = False #@param {type:\"boolean\"}\n",
    "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
    "USE_BUCKET = False #@param {type:\"boolean\"}\n",
    "BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n",
    "\n",
    "if USE_BUCKET:\n",
    "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "\n",
    "if DO_DELETE:\n",
    "  try:\n",
    "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "  except:\n",
    "    # Doesn't matter if the directory didn't exist\n",
    "    pass\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmFYvkylMwXn"
   },
   "source": [
    "#Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MC_w8SRqN0fr"
   },
   "source": [
    "First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 790,
     "status": "ok",
     "timestamp": 1605730567154,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -660
    },
    "id": "fom_ff20gyy6"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in os.listdir(directory):\n",
    "    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "  pos_df[\"polarity\"] = 1\n",
    "  neg_df[\"polarity\"] = 0\n",
    "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "  dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "  \n",
    "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "  \n",
    "  return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32505,
     "status": "ok",
     "timestamp": 1605730603151,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -660
    },
    "id": "2abfwdn-g135",
    "outputId": "a46e5f29-59b2-4306-8b38-9a0fb6237ac4"
   },
   "outputs": [],
   "source": [
    "train, test = download_and_load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Salvage is the worst so called horror film I'v...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I fell asleep on my couch at 7:35pm last night...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was lucky enough to have seen this on a whim...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm all for the idea of a grand epic of the Am...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boy, this was one lousy movie! While I haven't...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>This movie will kick your ass! Powerful acting...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>First one was much better, I had enjoyed it a ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>It's a great American martial arts movie. The ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>Alone in The Dark is one of my favorite role-p...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>This is one of the greatest sports movies ever...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence sentiment  polarity\n",
       "0      Salvage is the worst so called horror film I'v...         1         0\n",
       "1      I fell asleep on my couch at 7:35pm last night...         1         0\n",
       "2      I was lucky enough to have seen this on a whim...        10         1\n",
       "3      I'm all for the idea of a grand epic of the Am...         1         0\n",
       "4      Boy, this was one lousy movie! While I haven't...         2         0\n",
       "...                                                  ...       ...       ...\n",
       "24995  This movie will kick your ass! Powerful acting...        10         1\n",
       "24996  First one was much better, I had enjoyed it a ...         2         0\n",
       "24997  It's a great American martial arts movie. The ...        10         1\n",
       "24998  Alone in The Dark is one of my favorite role-p...         1         0\n",
       "24999  This is one of the greatest sports movies ever...        10         1\n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XA8WHJgzhIZf"
   },
   "source": [
    "To keep training fast, we'll take a sample of 5000 train and test examples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 22766,
     "status": "ok",
     "timestamp": 1605730603152,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -660
    },
    "id": "lw_F488eixTV"
   },
   "outputs": [],
   "source": [
    "train = train.sample(5000)\n",
    "test = test.sample(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfRnHSz3iSXz"
   },
   "source": [
    "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 805,
     "status": "ok",
     "timestamp": 1605730632631,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -660
    },
    "id": "IuMOGwFui4it"
   },
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'sentence'\n",
    "LABEL_COLUMN = 'polarity'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V399W0rqNJ-Z"
   },
   "source": [
    "#Data Preprocessing\n",
    "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
    "\n",
    "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
    "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
    "- `label` is the label for our example, i.e. True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 799,
     "status": "ok",
     "timestamp": 1605730638662,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -660
    },
    "id": "p9gEt5SmM6i6"
   },
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCZWZtKxObjh"
   },
   "source": [
    "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "\n",
    "\n",
    "1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "4. Map our words to indexes using a vocab file that BERT provides\n",
    "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "\n",
    "Happily, we don't have to worry about most of these details.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMWiDtpyQSoU"
   },
   "source": [
    "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6558,
     "status": "ok",
     "timestamp": 1605730651059,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -660
    },
    "id": "IhJSe0QHNG7U",
    "outputId": "5f19d70a-cde1-4d18-f46d-88cf664cae6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4oFkhpZBDKm"
   },
   "source": [
    "Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "dsBo6RCtQmwx",
    "outputId": "9af8c917-90ec-4fe9-897b-79dc89ca88e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'using',\n",
       " 'the',\n",
       " 'bert',\n",
       " 'token',\n",
       " '##izer']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OEzfFIt6GIc"
   },
   "source": [
    "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1261
    },
    "id": "LL5W8gEGRTAf",
    "outputId": "65001dda-155b-48fc-b5fc-1e4cabc8dfbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i am glad other people enjoyed this movie , cause i know it doesn ' t have the greatest reputation and it made no money at the box office . i thought it was terrific and there are several reasons why - bog ##dan ##ovich directs with the light ##est of touches , the cast ( especially cole ##en camp ) is perfect and the big bad apple never looked better on film . you ' ve seen worse movies ! [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i am glad other people enjoyed this movie , cause i know it doesn ' t have the greatest reputation and it made no money at the box office . i thought it was terrific and there are several reasons why - bog ##dan ##ovich directs with the light ##est of touches , the cast ( especially cole ##en camp ) is perfect and the big bad apple never looked better on film . you ' ve seen worse movies ! [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2572 5580 2060 2111 5632 2023 3185 1010 3426 1045 2113 2009 2987 1005 1056 2031 1996 4602 5891 1998 2009 2081 2053 2769 2012 1996 3482 2436 1012 1045 2245 2009 2001 27547 1998 2045 2024 2195 4436 2339 1011 22132 7847 12303 23303 2007 1996 2422 4355 1997 12817 1010 1996 3459 1006 2926 5624 2368 3409 1007 2003 3819 1998 1996 2502 2919 6207 2196 2246 2488 2006 2143 1012 2017 1005 2310 2464 4788 5691 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2572 5580 2060 2111 5632 2023 3185 1010 3426 1045 2113 2009 2987 1005 1056 2031 1996 4602 5891 1998 2009 2081 2053 2769 2012 1996 3482 2436 1012 1045 2245 2009 2001 27547 1998 2045 2024 2195 4436 2339 1011 22132 7847 12303 23303 2007 1996 2422 4355 1997 12817 1010 1996 3459 1006 2926 5624 2368 3409 1007 2003 3819 1998 1996 2502 2919 6207 2196 2246 2488 2006 2143 1012 2017 1005 2310 2464 4788 5691 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this is only related to the first movie by the name . the plot has nothing to do with the first and the whole movie stink ##s ! ! ! i have no idea what they were thinking but this movie is so bad . avoid this at all costs , the first movie in the series is acceptable as a slash ##er flick and so is the fourth but this one and the 3rd are rubbish ! ! [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this is only related to the first movie by the name . the plot has nothing to do with the first and the whole movie stink ##s ! ! ! i have no idea what they were thinking but this movie is so bad . avoid this at all costs , the first movie in the series is acceptable as a slash ##er flick and so is the fourth but this one and the 3rd are rubbish ! ! [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2003 2069 3141 2000 1996 2034 3185 2011 1996 2171 1012 1996 5436 2038 2498 2000 2079 2007 1996 2034 1998 1996 2878 3185 27136 2015 999 999 999 1045 2031 2053 2801 2054 2027 2020 3241 2021 2023 3185 2003 2061 2919 1012 4468 2023 2012 2035 5366 1010 1996 2034 3185 1999 1996 2186 2003 11701 2004 1037 18296 2121 17312 1998 2061 2003 1996 2959 2021 2023 2028 1998 1996 3822 2024 29132 999 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2003 2069 3141 2000 1996 2034 3185 2011 1996 2171 1012 1996 5436 2038 2498 2000 2079 2007 1996 2034 1998 1996 2878 3185 27136 2015 999 999 999 1045 2031 2053 2801 2054 2027 2020 3241 2021 2023 3185 2003 2061 2919 1012 4468 2023 2012 2035 5366 1010 1996 2034 3185 1999 1996 2186 2003 11701 2004 1037 18296 2121 17312 1998 2061 2003 1996 2959 2021 2023 2028 1998 1996 3822 2024 29132 999 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this is the best sci - fi that i have seen in my 29 years of watching sci - fi . i also believe that dark angel will become a cult favorite . the action is great but jessica alba is the best and most gorgeous star on tv today . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this is the best sci - fi that i have seen in my 29 years of watching sci - fi . i also believe that dark angel will become a cult favorite . the action is great but jessica alba is the best and most gorgeous star on tv today . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2003 1996 2190 16596 1011 10882 2008 1045 2031 2464 1999 2026 2756 2086 1997 3666 16596 1011 10882 1012 1045 2036 2903 2008 2601 4850 2097 2468 1037 8754 5440 1012 1996 2895 2003 2307 2021 8201 18255 2003 1996 2190 1998 2087 9882 2732 2006 2694 2651 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2003 1996 2190 16596 1011 10882 2008 1045 2031 2464 1999 2026 2756 2086 1997 3666 16596 1011 10882 1012 1045 2036 2903 2008 2601 4850 2097 2468 1037 8754 5440 1012 1996 2895 2003 2307 2021 8201 18255 2003 1996 2190 1998 2087 9882 2732 2006 2694 2651 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] poor robert eng ##lund makes another flop and to the expense of to ##be hooper who usually makes pretty good horror movies but he failed pretty bad at this one . eng ##lund plays the well known mar ##que de sad ##e who in the 17th century was en ##pr ##ison ##ed for his obsession of pain and the pleasure of bringing pain upon himself as well as watching others also be in pain . the story is so confusing with the flip flop from one century to another and i became confused as to what was going on and what was the purpose of this movie . all i saw was a young lady that became en ##tra ##pped by a strange lesbian who des [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] poor robert eng ##lund makes another flop and to the expense of to ##be hooper who usually makes pretty good horror movies but he failed pretty bad at this one . eng ##lund plays the well known mar ##que de sad ##e who in the 17th century was en ##pr ##ison ##ed for his obsession of pain and the pleasure of bringing pain upon himself as well as watching others also be in pain . the story is so confusing with the flip flop from one century to another and i became confused as to what was going on and what was the purpose of this movie . all i saw was a young lady that became en ##tra ##pped by a strange lesbian who des [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3532 2728 25540 18028 3084 2178 28583 1998 2000 1996 10961 1997 2000 4783 23717 2040 2788 3084 3492 2204 5469 5691 2021 2002 3478 3492 2919 2012 2023 2028 1012 25540 18028 3248 1996 2092 2124 9388 4226 2139 6517 2063 2040 1999 1996 5550 2301 2001 4372 18098 10929 2098 2005 2010 17418 1997 3255 1998 1996 5165 1997 5026 3255 2588 2370 2004 2092 2004 3666 2500 2036 2022 1999 3255 1012 1996 2466 2003 2061 16801 2007 1996 11238 28583 2013 2028 2301 2000 2178 1998 1045 2150 5457 2004 2000 2054 2001 2183 2006 1998 2054 2001 1996 3800 1997 2023 3185 1012 2035 1045 2387 2001 1037 2402 3203 2008 2150 4372 6494 11469 2011 1037 4326 11690 2040 4078 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3532 2728 25540 18028 3084 2178 28583 1998 2000 1996 10961 1997 2000 4783 23717 2040 2788 3084 3492 2204 5469 5691 2021 2002 3478 3492 2919 2012 2023 2028 1012 25540 18028 3248 1996 2092 2124 9388 4226 2139 6517 2063 2040 1999 1996 5550 2301 2001 4372 18098 10929 2098 2005 2010 17418 1997 3255 1998 1996 5165 1997 5026 3255 2588 2370 2004 2092 2004 3666 2500 2036 2022 1999 3255 1012 1996 2466 2003 2061 16801 2007 1996 11238 28583 2013 2028 2301 2000 2178 1998 1045 2150 5457 2004 2000 2054 2001 2183 2006 1998 2054 2001 1996 3800 1997 2023 3185 1012 2035 1045 2387 2001 1037 2402 3203 2008 2150 4372 6494 11469 2011 1037 4326 11690 2040 4078 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] jim carr ##ey is one of the fun ##nies ##t and most gifted comedians in film today . with his hyper ##active sp ##ont ##ane ##ity and his rubber face he can just go crazy , and we love him for it . he has the ability to make med ##io ##cre comedies ( ala ace ventura ) , and turn them into decent comedic outing ##s . or , in the case of ' liar liar ' , make them some of the most hilarious contemporary comedies around . carr ##ey has also proven himself capable of tack ##ling dramas . he was excellent in both ' man on the moon ' and ' the truman show . ' the guy is remarkable . < [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] jim carr ##ey is one of the fun ##nies ##t and most gifted comedians in film today . with his hyper ##active sp ##ont ##ane ##ity and his rubber face he can just go crazy , and we love him for it . he has the ability to make med ##io ##cre comedies ( ala ace ventura ) , and turn them into decent comedic outing ##s . or , in the case of ' liar liar ' , make them some of the most hilarious contemporary comedies around . carr ##ey has also proven himself capable of tack ##ling dramas . he was excellent in both ' man on the moon ' and ' the truman show . ' the guy is remarkable . < [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3958 12385 3240 2003 2028 1997 1996 4569 15580 2102 1998 2087 12785 25119 1999 2143 2651 1012 2007 2010 23760 19620 11867 12162 7231 3012 1998 2010 8903 2227 2002 2064 2074 2175 4689 1010 1998 2057 2293 2032 2005 2009 1012 2002 2038 1996 3754 2000 2191 19960 3695 16748 22092 1006 21862 9078 21151 1007 1010 1998 2735 2068 2046 11519 21699 26256 2015 1012 2030 1010 1999 1996 2553 1997 1005 16374 16374 1005 1010 2191 2068 2070 1997 1996 2087 26316 3824 22092 2105 1012 12385 3240 2038 2036 10003 2370 5214 1997 26997 2989 16547 1012 2002 2001 6581 1999 2119 1005 2158 2006 1996 4231 1005 1998 1005 1996 15237 2265 1012 1005 1996 3124 2003 9487 1012 1026 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3958 12385 3240 2003 2028 1997 1996 4569 15580 2102 1998 2087 12785 25119 1999 2143 2651 1012 2007 2010 23760 19620 11867 12162 7231 3012 1998 2010 8903 2227 2002 2064 2074 2175 4689 1010 1998 2057 2293 2032 2005 2009 1012 2002 2038 1996 3754 2000 2191 19960 3695 16748 22092 1006 21862 9078 21151 1007 1010 1998 2735 2068 2046 11519 21699 26256 2015 1012 2030 1010 1999 1996 2553 1997 1005 16374 16374 1005 1010 2191 2068 2070 1997 1996 2087 26316 3824 22092 2105 1012 12385 3240 2038 2036 10003 2370 5214 1997 26997 2989 16547 1012 2002 2001 6581 1999 2119 1005 2158 2006 1996 4231 1005 1998 1005 1996 15237 2265 1012 1005 1996 3124 2003 9487 1012 1026 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] ridiculous , nausea ##ting dogg ##ere ##l with terrible acting ; in ##ept ##ly , superficial ##ly , and conde ##sc ##ending ##ly tr ##aw ##ling all the most ban ##al cl ##iche ##s about tuscany and italy , divorce and mid ##life . the main actor nervously grimace ##s her way through the film , struggling to portray the appropriate level of smug , self - cong ##rat ##ulator ##y self - pity the worthless character and script call for . i ' m sure the book was bad , but it can ' t have been this bad ! the camera is permanently fitted with a vomit - yellow \" tu ##scan \" lens ##e filter ( perhaps the tu ##scan sun wasn ' [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] ridiculous , nausea ##ting dogg ##ere ##l with terrible acting ; in ##ept ##ly , superficial ##ly , and conde ##sc ##ending ##ly tr ##aw ##ling all the most ban ##al cl ##iche ##s about tuscany and italy , divorce and mid ##life . the main actor nervously grimace ##s her way through the film , struggling to portray the appropriate level of smug , self - cong ##rat ##ulator ##y self - pity the worthless character and script call for . i ' m sure the book was bad , but it can ' t have been this bad ! the camera is permanently fitted with a vomit - yellow \" tu ##scan \" lens ##e filter ( perhaps the tu ##scan sun wasn ' [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 9951 1010 19029 3436 28844 7869 2140 2007 6659 3772 1025 1999 23606 2135 1010 23105 2135 1010 1998 24707 11020 18537 2135 19817 10376 2989 2035 1996 2087 7221 2389 18856 17322 2015 2055 23322 1998 3304 1010 8179 1998 3054 15509 1012 1996 2364 3364 12531 25898 2015 2014 2126 2083 1996 2143 1010 8084 2000 17279 1996 6413 2504 1997 20673 1010 2969 1011 26478 8609 20350 2100 2969 1011 12063 1996 22692 2839 1998 5896 2655 2005 1012 1045 1005 1049 2469 1996 2338 2001 2919 1010 2021 2009 2064 1005 1056 2031 2042 2023 2919 999 1996 4950 2003 8642 7130 2007 1037 23251 1011 3756 1000 10722 29378 1000 10014 2063 11307 1006 3383 1996 10722 29378 3103 2347 1005 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 9951 1010 19029 3436 28844 7869 2140 2007 6659 3772 1025 1999 23606 2135 1010 23105 2135 1010 1998 24707 11020 18537 2135 19817 10376 2989 2035 1996 2087 7221 2389 18856 17322 2015 2055 23322 1998 3304 1010 8179 1998 3054 15509 1012 1996 2364 3364 12531 25898 2015 2014 2126 2083 1996 2143 1010 8084 2000 17279 1996 6413 2504 1997 20673 1010 2969 1011 26478 8609 20350 2100 2969 1011 12063 1996 22692 2839 1998 5896 2655 2005 1012 1045 1005 1049 2469 1996 2338 2001 2919 1010 2021 2009 2064 1005 1056 2031 2042 2023 2919 999 1996 4950 2003 8642 7130 2007 1037 23251 1011 3756 1000 10722 29378 1000 10014 2063 11307 1006 3383 1996 10722 29378 3103 2347 1005 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this could have been a really good movie if someone would just have known how to finish the film . < br / > < br / > the story was going along just fine and heading towards that point in every movie like this where the \" gray \" characters turn \" good \" and the \" bad \" guys get their just dessert ##s and * boom * . . . it ' s like they ran out of script and the cast just started to make things up . < br / > < br / > which wouldn ' t have been so bad . . . if the cast had just continued with the character development they had already put in [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this could have been a really good movie if someone would just have known how to finish the film . < br / > < br / > the story was going along just fine and heading towards that point in every movie like this where the \" gray \" characters turn \" good \" and the \" bad \" guys get their just dessert ##s and * boom * . . . it ' s like they ran out of script and the cast just started to make things up . < br / > < br / > which wouldn ' t have been so bad . . . if the cast had just continued with the character development they had already put in [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2071 2031 2042 1037 2428 2204 3185 2065 2619 2052 2074 2031 2124 2129 2000 3926 1996 2143 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2466 2001 2183 2247 2074 2986 1998 5825 2875 2008 2391 1999 2296 3185 2066 2023 2073 1996 1000 3897 1000 3494 2735 1000 2204 1000 1998 1996 1000 2919 1000 4364 2131 2037 2074 18064 2015 1998 1008 8797 1008 1012 1012 1012 2009 1005 1055 2066 2027 2743 2041 1997 5896 1998 1996 3459 2074 2318 2000 2191 2477 2039 1012 1026 7987 1013 1028 1026 7987 1013 1028 2029 2876 1005 1056 2031 2042 2061 2919 1012 1012 1012 2065 1996 3459 2018 2074 2506 2007 1996 2839 2458 2027 2018 2525 2404 1999 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2071 2031 2042 1037 2428 2204 3185 2065 2619 2052 2074 2031 2124 2129 2000 3926 1996 2143 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2466 2001 2183 2247 2074 2986 1998 5825 2875 2008 2391 1999 2296 3185 2066 2023 2073 1996 1000 3897 1000 3494 2735 1000 2204 1000 1998 1996 1000 2919 1000 4364 2131 2037 2074 18064 2015 1998 1008 8797 1008 1012 1012 1012 2009 1005 1055 2066 2027 2743 2041 1997 5896 1998 1996 3459 2074 2318 2000 2191 2477 2039 1012 1026 7987 1013 1028 1026 7987 1013 1028 2029 2876 1005 1056 2031 2042 2061 2919 1012 1012 1012 2065 1996 3459 2018 2074 2506 2007 1996 2839 2458 2027 2018 2525 2404 1999 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] muscular ' scientists ' , unpleasant ##ly thin females in swim ##suit ##s , lots of beer drinking . . yet it ' s too long to be a beer commercial . oh , okay , there ' s some plot about a big shark - like monster that ' s killing people and stuff . but it ' s nothing you haven ' t seen before . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] muscular ' scientists ' , unpleasant ##ly thin females in swim ##suit ##s , lots of beer drinking . . yet it ' s too long to be a beer commercial . oh , okay , there ' s some plot about a big shark - like monster that ' s killing people and stuff . but it ' s nothing you haven ' t seen before . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 13472 1005 6529 1005 1010 16010 2135 4857 3801 1999 9880 28880 2015 1010 7167 1997 5404 5948 1012 1012 2664 2009 1005 1055 2205 2146 2000 2022 1037 5404 3293 1012 2821 1010 3100 1010 2045 1005 1055 2070 5436 2055 1037 2502 11420 1011 2066 6071 2008 1005 1055 4288 2111 1998 4933 1012 2021 2009 1005 1055 2498 2017 4033 1005 1056 2464 2077 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 13472 1005 6529 1005 1010 16010 2135 4857 3801 1999 9880 28880 2015 1010 7167 1997 5404 5948 1012 1012 2664 2009 1005 1055 2205 2146 2000 2022 1037 5404 3293 1012 2821 1010 3100 1010 2045 1005 1055 2070 5436 2055 1037 2502 11420 1011 2066 6071 2008 1005 1055 4288 2111 1998 4933 1012 2021 2009 1005 1055 2498 2017 4033 1005 1056 2464 2077 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] alright , let me break it down for ya . . . haggard is probably one of the fun ##nies ##t pointless movies you ' ll ever see . it ' s got a mixture of a unique storyline about a guy having girl troubles and everything going backwards for him mixed in with countless humorous scenes that will keep you laughing throughout the whole movie , basically , if you ' ve seen jack ##ass or the ck ##y series , you ' d know what to expect for humor , considering it has most of the people from those movies . overall . . . i just had to give it a 10 / 10 because its one of my favorite movies of all [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] alright , let me break it down for ya . . . haggard is probably one of the fun ##nies ##t pointless movies you ' ll ever see . it ' s got a mixture of a unique storyline about a guy having girl troubles and everything going backwards for him mixed in with countless humorous scenes that will keep you laughing throughout the whole movie , basically , if you ' ve seen jack ##ass or the ck ##y series , you ' d know what to expect for humor , considering it has most of the people from those movies . overall . . . i just had to give it a 10 / 10 because its one of my favorite movies of all [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 10303 1010 2292 2033 3338 2009 2091 2005 8038 1012 1012 1012 27912 2003 2763 2028 1997 1996 4569 15580 2102 23100 5691 2017 1005 2222 2412 2156 1012 2009 1005 1055 2288 1037 8150 1997 1037 4310 9994 2055 1037 3124 2383 2611 13460 1998 2673 2183 11043 2005 2032 3816 1999 2007 14518 14742 5019 2008 2097 2562 2017 5870 2802 1996 2878 3185 1010 10468 1010 2065 2017 1005 2310 2464 2990 12054 2030 1996 23616 2100 2186 1010 2017 1005 1040 2113 2054 2000 5987 2005 8562 1010 6195 2009 2038 2087 1997 1996 2111 2013 2216 5691 1012 3452 1012 1012 1012 1045 2074 2018 2000 2507 2009 1037 2184 1013 2184 2138 2049 2028 1997 2026 5440 5691 1997 2035 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 10303 1010 2292 2033 3338 2009 2091 2005 8038 1012 1012 1012 27912 2003 2763 2028 1997 1996 4569 15580 2102 23100 5691 2017 1005 2222 2412 2156 1012 2009 1005 1055 2288 1037 8150 1997 1037 4310 9994 2055 1037 3124 2383 2611 13460 1998 2673 2183 11043 2005 2032 3816 1999 2007 14518 14742 5019 2008 2097 2562 2017 5870 2802 1996 2878 3185 1010 10468 1010 2065 2017 1005 2310 2464 2990 12054 2030 1996 23616 2100 2186 1010 2017 1005 1040 2113 2054 2000 5987 2005 8562 1010 6195 2009 2038 2087 1997 1996 2111 2013 2216 5691 1012 3452 1012 1012 1012 1045 2074 2018 2000 2507 2009 1037 2184 1013 2184 2138 2049 2028 1997 2026 5440 5691 1997 2035 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] the efficacy of this picture was best proven on the intended target audience , namely teens . my 14 - year - old son became so eng ##ross ##ed in this film that i rate it considerably higher than its im ##ita ##tor \" mad city . \" it sparked debate in our household on issues such as peer pressure and loyalty vs . doing the right thing . for that alone , i rate this film a 10 ! parents should watch it with their teens and discuss it afterwards . < br / > < br / > i very much liked the smart dialogue and consistent acting . i thought that james re ##mar was adequate in his role , but the teenage [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] the efficacy of this picture was best proven on the intended target audience , namely teens . my 14 - year - old son became so eng ##ross ##ed in this film that i rate it considerably higher than its im ##ita ##tor \" mad city . \" it sparked debate in our household on issues such as peer pressure and loyalty vs . doing the right thing . for that alone , i rate this film a 10 ! parents should watch it with their teens and discuss it afterwards . < br / > < br / > i very much liked the smart dialogue and consistent acting . i thought that james re ##mar was adequate in his role , but the teenage [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1996 21150 1997 2023 3861 2001 2190 10003 2006 1996 3832 4539 4378 1010 8419 13496 1012 2026 2403 1011 2095 1011 2214 2365 2150 2061 25540 25725 2098 1999 2023 2143 2008 1045 3446 2009 9839 3020 2084 2049 10047 6590 4263 1000 5506 2103 1012 1000 2009 13977 5981 1999 2256 4398 2006 3314 2107 2004 8152 3778 1998 9721 5443 1012 2725 1996 2157 2518 1012 2005 2008 2894 1010 1045 3446 2023 2143 1037 2184 999 3008 2323 3422 2009 2007 2037 13496 1998 6848 2009 5728 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2200 2172 4669 1996 6047 7982 1998 8335 3772 1012 1045 2245 2008 2508 2128 7849 2001 11706 1999 2010 2535 1010 2021 1996 9454 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1996 21150 1997 2023 3861 2001 2190 10003 2006 1996 3832 4539 4378 1010 8419 13496 1012 2026 2403 1011 2095 1011 2214 2365 2150 2061 25540 25725 2098 1999 2023 2143 2008 1045 3446 2009 9839 3020 2084 2049 10047 6590 4263 1000 5506 2103 1012 1000 2009 13977 5981 1999 2256 4398 2006 3314 2107 2004 8152 3778 1998 9721 5443 1012 2725 1996 2157 2518 1012 2005 2008 2894 1010 1045 3446 2023 2143 1037 2184 999 3008 2323 3422 2009 2007 2037 13496 1998 6848 2009 5728 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2200 2172 4669 1996 6047 7982 1998 8335 3772 1012 1045 2245 2008 2508 2128 7849 2001 11706 1999 2010 2535 1010 2021 1996 9454 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    }
   ],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bert.run_classifier.InputFeatures at 0x7fa3e8945110>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e92d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d13d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f58d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e8688190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1ad0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e38450d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5e90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f59d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e6395c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e6395510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1b50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e6395690>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e89452d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5850>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e8688250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa590>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa46d4cfa50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e8688210>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5c10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e86881d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e98d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa46979b0d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa310>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa790>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e8581d10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1d90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e8581ed0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa46cb93210>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e8581c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d14d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e877c7d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e8581dd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e877cb90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5790>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e877c090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37facd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e8581e90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9350>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e6395790>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa469798f50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fab10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37faa90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37faf10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e877cc90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa469798090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fac90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e6395fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e877cfd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1a50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e8581c10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e877c650>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1410>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1d10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a8d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e8581e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e6395b10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9b90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e8964f50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1490>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380ad50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e8581e50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e6395890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a290>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1990>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa210>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa290>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa950>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e8581f90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fadd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fae10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa7d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9d50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e8964a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380ae90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a1d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9950>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5590>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1dd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f57d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5f10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2410>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1650>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e8964b90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380af50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2750>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9310>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1950>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1b90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380af90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa4d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fad90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9750>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2e50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9990>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e91d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9f90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d18d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1810>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a810>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1c90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2b10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f92d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37faf50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fae50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f52d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1f90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fae90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9950>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9b90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9650>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9850>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37faed0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d19d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d15d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2d10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1bd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2490>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f50d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1590>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fac50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5750>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9a10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa990>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2a50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380ae10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9cd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5410>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1cd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f51d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9ed0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d16d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a850>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e24d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1b10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800b50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fac10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2210>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1a10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37d1f50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a7d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a410>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2f50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e23d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9dd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f95d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800a10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9110>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2b50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2850>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2310>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2f10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2110>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e22d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f91d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9490>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800e90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380ab50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fab50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9210>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380aa10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e21d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e97d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9610>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9650>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800290>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380ad10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380aad0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa350>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800d50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9810>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9e50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9ad0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a590>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380ac10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f98d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800850>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800350>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9e90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fad10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f56d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9f10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9690>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800210>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a950>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e90d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2dd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e38008d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5e50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2c90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800790>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9ad0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2bd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9b50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380abd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9790>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a490>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800750>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5490>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9e90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f55d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9c90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9c90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9b10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e99d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800ad0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5bd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f53d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5290>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fabd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5350>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a5d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2990>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e38005d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800bd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800690>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800f10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a990>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2ad0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800f50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5110>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800990>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800b10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5210>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800e50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800a50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380af10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa750>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9a50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2c10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa4d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa910>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa950>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e38003d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e25d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5d50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5310>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5f50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800dd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa0d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa9d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a210>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2590>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9290>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa8d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aabd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aac10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800950>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2050>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e28d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2cd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a6d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9110>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a2d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a690>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aab50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800610>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800b90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2e90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9f50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a050>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9a10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9310>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa3d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa610>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380ab90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5650>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa050>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e38000d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9c10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2650>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800050>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f94d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9690>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9d10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9f10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa990>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa810>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa110>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9410>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800910>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e26d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a4d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380aed0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa690>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bce10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a310>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5f90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5ed0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fafd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa1d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2790>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9d90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa850>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9590>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc3d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa5d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aad90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bccd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800f90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9ed0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e94d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2950>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2610>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e38006d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc9d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bcdd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bcf90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9cd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa050>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa110>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa410>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5050>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2690>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc0d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bcf10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa690>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380acd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a350>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc8d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a750>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc290>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc950>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bcfd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a650>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9350>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9d90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa490>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2d90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa290>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800810>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fad50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa750>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37faf90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa0d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa2d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800650>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e20d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2290>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e38007d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2ed0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bce50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2910>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aab90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc1d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bca90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380aa90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9210>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800c10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a610>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800110>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800590>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9590>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e38002d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aacd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aad10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380aa50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e29d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc810>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e96d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc910>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab610>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aba50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800310>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e38004d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800d90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a0d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aac50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa810>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e38009d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa410>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9e50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa210>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e9990>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800c90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9b50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab950>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5d10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a110>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800410>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2810>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e38001d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa490>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa6d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37faa10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aaa10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f54d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aad50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a3d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab8d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aba10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aadd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f5910>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9d10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa850>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9b10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aac90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380afd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa2d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bcbd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380ac50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fab90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab050>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abb50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9a50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9d50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aaa50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aaf90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab1d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800ed0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9810>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796290>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abd90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bca10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a9d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800490>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2d50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37faad0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37faa50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a910>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37fa650>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380ac90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab9d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380ab10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa790>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa6d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa8d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e27d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800d10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2350>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2b90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2f90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380ae50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab6d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa910>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aae10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800cd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796d10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9290>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7810>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abf90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3800510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a790>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aae50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9dd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aaf10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380add0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bca50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc7d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380a390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796210>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc5d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e77e2a10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e380ad90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bcc10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a76d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9410>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f99d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab110>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9050>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7c90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9790>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796350>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9910>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7410>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37beb50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa5d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be310>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f97d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be990>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aaed0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc310>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc990>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa1d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aaf50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9610>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796c10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796d50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796b90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab790>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abc10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc2d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f90d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796690>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796750>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796490>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab690>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796910>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc590>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc4d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796110>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abd10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37961d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7e90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7b10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a75d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aab10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab850>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab0d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab5d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab750>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7350>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa610>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa7d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aaad0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa9d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be3d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be5d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bed10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796590>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796c90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37960d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37967d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f93d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bef90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f96d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7a10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37becd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bec90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bcc90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc350>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bed90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bcd50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abb90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be050>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796b50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7290>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7110>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7cd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf3d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab310>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796050>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aae90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf5d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc610>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa310>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7850>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7dd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bfd90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bfdd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37beb10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9bd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a73d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab410>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bcc50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa590>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37f9c10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796dd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abe10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab210>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abc50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796f90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc850>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa3d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bced0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab490>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf310>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bcad0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bce90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc490>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37965d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7b90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796b10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bfa10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abad0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796ad0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc750>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab810>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab350>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bfd50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2650>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bcb10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2850>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bfed0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab590>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab990>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7bd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7b50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abdd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7d90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37beb90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37969d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2990>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bfc90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa650>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abe90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7050>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bea50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab290>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2d90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a72d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be9d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7f10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bebd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7750>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a70d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796f10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be910>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aaa90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4a50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4e90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aa350>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf9d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc650>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bcd90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abed0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aafd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be410>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bed50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2b90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b26d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37befd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bcd10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7610>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796850>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abc90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7d50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c43d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c42d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bfcd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796950>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a77d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc110>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796d90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be110>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bea10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bef50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bee50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be8d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc790>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc6d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bcb50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796650>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be1d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7f50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be6d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be650>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf810>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4f50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2690>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4cd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bff90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be610>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37aba90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc690>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796310>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be690>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4d90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bcf50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c44d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc050>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc410>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7990>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2c10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2750>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be490>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c49d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf4d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bfb10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be2d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4350>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4050>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4850>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bee90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37966d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796cd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4c90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4810>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c45d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374b510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374b450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7790>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2a10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4dd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b23d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796e50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7490>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bead0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be210>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2c90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bcb90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abe50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf2d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37beed0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bc210>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bfc50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abf50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374b8d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be350>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bee10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796410>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab2d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab3d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37964d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796ed0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37963d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bedd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be590>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a71d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2b50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4bd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4690>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4b10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374b290>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374b310>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4750>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4ed0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2790>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be810>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4e50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374b550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374bc50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7690>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abfd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bfd10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf290>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf410>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796610>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bfbd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7f90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab4d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bffd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c46d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374b850>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e376e850>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2f50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf6d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7950>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a74d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2cd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2450>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abd50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796810>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4ad0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796e90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be290>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bec50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374bf50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374bfd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374b110>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bfb50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2b10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2d50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2610>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374ba90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374bbd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e376e890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab650>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be0d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374b250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374bcd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374bed0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abf10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be790>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf110>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796a50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e376ef10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e376e350>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374b190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2dd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be750>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374b150>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e376eb10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b24d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2350>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2f10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4290>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4310>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4950>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796bd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37968d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7ed0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be7d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf990>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2510>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c40d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be850>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bfe10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374bd90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374b710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e376e250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e376ee10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bf0d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e376e610>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e376ef90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7d10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7c10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7a50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37b2390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796190>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7210>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4210>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7310>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7e50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796a10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796f50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37962d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c4650>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7890>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37a7590>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e376e4d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e376ef50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bea90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374b810>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37c47d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e374b750>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37abb10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796790>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab250>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e376e490>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e376ee50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab910>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37ab7d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e376e090>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37bef10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e3796990>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be4d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7fa3e37be950>,\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccp5trMwRtmr"
   },
   "source": [
    "#Creating a model\n",
    "\n",
    "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6o2a5ZIvRcJq"
   },
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "  bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "  bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "  # Use \"sequence_outputs\" for token-level output.\n",
    "  output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "  # Create our own layer to tune for politeness data.\n",
    "  output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    # Dropout helps prevent overfitting\n",
    "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    # Convert labels into one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "    if is_predicting:\n",
    "      return (predicted_labels, log_probs)\n",
    "\n",
    "    # If we're train/eval, compute loss between predicted and actual label\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, predicted_labels, log_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpE0ZIDOCQzE"
   },
   "source": [
    "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FnH-AnOQ9KKW"
   },
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(label_ids, predicted_labels):\n",
    "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "        f1_score = tf.contrib.metrics.f1_score(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        auc = tf.metrics.auc(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        recall = tf.metrics.recall(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        precision = tf.metrics.precision(\n",
    "            label_ids,\n",
    "            predicted_labels) \n",
    "        true_pos = tf.metrics.true_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        true_neg = tf.metrics.true_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)   \n",
    "        false_pos = tf.metrics.false_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)  \n",
    "        false_neg = tf.metrics.false_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"auc\": auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"true_positives\": true_pos,\n",
    "            \"true_negatives\": true_neg,\n",
    "            \"false_positives\": false_pos,\n",
    "            \"false_negatives\": false_neg\n",
    "        }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OjwJ4bTeWXD8"
   },
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "emHf9GhfWBZ_"
   },
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oEJldMr3WYZa"
   },
   "outputs": [],
   "source": [
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "q_WebpS1X97v",
    "outputId": "1648932a-7391-49d3-8af7-52d514e226e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'output_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa3e033df90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'output_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa3e033df90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOO3RfG1DYLo"
   },
   "source": [
    "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1Pv2bAlOX_-K"
   },
   "outputs": [],
   "source": [
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6Nukby2EB6-"
   },
   "source": [
    "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "nucD4gluYJmK",
    "outputId": "5d728e72-4631-42bf-c48d-3f51d4b968ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-ca03218f28a6>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-ca03218f28a6>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "/home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/tensorflow_core/contrib/metrics/python/metrics/classification.py:162: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sahand/anaconda3/envs/tf-1/lib/python3.7/site-packages/tensorflow_core/contrib/metrics/python/metrics/classification.py:162: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into output_dir/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into output_dir/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmbLTVniARy3"
   },
   "source": [
    "Now let's use our test data to see how well our model did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIhejfpyJ8Bx"
   },
   "outputs": [],
   "source": [
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "PPVEXhNjYXC-",
    "outputId": "dd5482cd-c558-465f-c854-ec11a0175316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-12T21:04:20Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from gs://bert-tfhub/aclImdb_v1/model.ckpt-468\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-12-21:06:05\n",
      "INFO:tensorflow:Saving dict for global step 468: auc = 0.86659324, eval_accuracy = 0.8664, f1_score = 0.8659711, false_negatives = 375.0, false_positives = 293.0, global_step = 468, loss = 0.51870537, precision = 0.880457, recall = 0.8519542, true_negatives = 2174.0, true_positives = 2158.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 468: gs://bert-tfhub/aclImdb_v1/model.ckpt-468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'auc': 0.86659324,\n",
       " 'eval_accuracy': 0.8664,\n",
       " 'f1_score': 0.8659711,\n",
       " 'false_negatives': 375.0,\n",
       " 'false_positives': 293.0,\n",
       " 'global_step': 468,\n",
       " 'loss': 0.51870537,\n",
       " 'precision': 0.880457,\n",
       " 'recall': 0.8519542,\n",
       " 'true_negatives': 2174.0,\n",
       " 'true_positives': 2158.0}"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueKsULteiz1B"
   },
   "source": [
    "Now let's write code to make predictions on new sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsrbTD2EJTVl"
   },
   "outputs": [],
   "source": [
    "def getPrediction(in_sentences):\n",
    "  labels = [\"Negative\", \"Positive\"]\n",
    "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "  predictions = estimator.predict(predict_input_fn)\n",
    "  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-thbodgih_VJ"
   },
   "outputs": [],
   "source": [
    "pred_sentences = [\n",
    "  \"That movie was absolutely awful\",\n",
    "  \"The acting was a bit lacking\",\n",
    "  \"The film was creative and surprising\",\n",
    "  \"Absolutely fantastic!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "QrZmvZySKQTm",
    "outputId": "3891fafb-a460-4eb8-fa6c-335a5bbc10e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 4\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: \n",
      "INFO:tensorflow:tokens: [CLS] that movie was absolutely awful [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2008 3185 2001 7078 9643 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: \n",
      "INFO:tensorflow:tokens: [CLS] the acting was a bit lacking [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1996 3772 2001 1037 2978 11158 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: \n",
      "INFO:tensorflow:tokens: [CLS] the film was creative and surprising [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1996 2143 2001 5541 1998 11341 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: \n",
      "INFO:tensorflow:tokens: [CLS] absolutely fantastic ! [SEP]\n",
      "INFO:tensorflow:input_ids: 101 7078 10392 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from gs://bert-tfhub/aclImdb_v1/model.ckpt-468\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predictions = getPrediction(pred_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXkRiEBUqN3n"
   },
   "source": [
    "Voila! We have a sentiment classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "ERkTE8-7oQLZ",
    "outputId": "26c33224-dc2c-4b3d-f7b4-ac3ef0a58b27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('That movie was absolutely awful',\n",
       "  array([-4.9142293e-03, -5.3180690e+00], dtype=float32),\n",
       "  'Negative'),\n",
       " ('The acting was a bit lacking',\n",
       "  array([-0.03325794, -3.4200459 ], dtype=float32),\n",
       "  'Negative'),\n",
       " ('The film was creative and surprising',\n",
       "  array([-5.3589125e+00, -4.7171740e-03], dtype=float32),\n",
       "  'Positive'),\n",
       " ('Absolutely fantastic!',\n",
       "  array([-5.0434084 , -0.00647258], dtype=float32),\n",
       "  'Positive')]"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Predicting Movie Reviews with BERT on TF Hub.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb",
     "timestamp": 1605734027933
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
