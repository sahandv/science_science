{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "clustering_benchmark.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahandv/science_science/blob/master/clustering_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gg40GIClXPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "728200c1-b630-44a4-fabd-c8609ad98224"
      },
      "source": [
        "!rm -rf 'science_science'\n",
        "username = \"sahandv\"#@param {type:\"string\"}\n",
        "# password = \"\"#@param {type:\"string\"} \n",
        "!git clone https://github.com/$username/science_science.git\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'science_science'...\n",
            "remote: Enumerating objects: 108, done.\u001b[K\n",
            "remote: Counting objects: 100% (108/108), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 1110 (delta 50), reused 61 (delta 8), pack-reused 1002\u001b[K\n",
            "Receiving objects: 100% (1110/1110), 102.65 MiB | 37.08 MiB/s, done.\n",
            "Resolving deltas: 100% (645/645), done.\n",
            "sample_data  science_science\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIuk5KrtjB3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d705ed3-5288-41d0-f78a-23018d545dd8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!pip install -r 'science_science/requirements.txt'\n",
        "# !pip install tensorflow\n",
        "# !pip install keras"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 3)) (1.1.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 4)) (3.2.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 5)) (2.2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 6)) (0.22.2.post1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 7)) (0.16.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 9)) (0.11.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 10)) (1.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 11)) (3.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 12)) (7.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 13)) (0.8.7)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 14)) (3.6.0)\n",
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 14.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 16)) (2.5)\n",
            "Collecting netgraph\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/ed/1e163a923cc58feab143656f2eefd69e5a1d2e323423f62c08b5100a4cbe/netgraph-3.1.8.tar.gz (40kB)\n",
            "\u001b[K     |████████████████████████████████| 40kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: yellowbrick in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 18)) (0.9.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 19)) (4.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r science_science/requirements.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r science_science/requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r science_science/requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (2.0.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (50.3.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (0.8.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (7.4.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r science_science/requirements.txt (line 6)) (0.17.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r science_science/requirements.txt (line 7)) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r science_science/requirements.txt (line 7)) (1.1.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r science_science/requirements.txt (line 11)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r science_science/requirements.txt (line 11)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r science_science/requirements.txt (line 11)) (1.3.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->-r science_science/requirements.txt (line 14)) (3.0.0)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r science_science/requirements.txt (line 15)) (0.35.1)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r science_science/requirements.txt (line 15)) (2.11.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r science_science/requirements.txt (line 15)) (2.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r science_science/requirements.txt (line 15)) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r science_science/requirements.txt (line 15)) (0.16.0)\n",
            "Collecting funcy\n",
            "  Downloading https://files.pythonhosted.org/packages/66/89/479de0afbbfb98d1c4b887936808764627300208bb771fcd823403645a36/funcy-1.15-py2.py3-none-any.whl\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->-r science_science/requirements.txt (line 16)) (4.4.2)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->-r science_science/requirements.txt (line 19)) (1.3.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->-r science_science/requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r science_science/requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r science_science/requirements.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r science_science/requirements.txt (line 5)) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r science_science/requirements.txt (line 5)) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis->-r science_science/requirements.txt (line 15)) (1.1.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis->-r science_science/requirements.txt (line 15)) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis->-r science_science/requirements.txt (line 15)) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis->-r science_science/requirements.txt (line 15)) (1.9.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis->-r science_science/requirements.txt (line 15)) (8.6.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis->-r science_science/requirements.txt (line 15)) (20.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r science_science/requirements.txt (line 5)) (3.4.0)\n",
            "Building wheels for collected packages: pyLDAvis, netgraph\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=18add09f8838eada733ded4218b8e0bb4ec8d7ac6e7a318c85f7f8e512e78bee\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "  Building wheel for netgraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for netgraph: filename=netgraph-3.1.8-cp36-none-any.whl size=45928 sha256=d5fbb1dc5693718cfd51580f4c7289ee78448d1ddba4a6623757ae29025cc9a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/c2/49/30a980fcf7c864676d6184e46ea8860fff2b7fa6f324cad69a\n",
            "Successfully built pyLDAvis netgraph\n",
            "Installing collected packages: funcy, pyLDAvis, netgraph\n",
            "Successfully installed funcy-1.15 netgraph-3.1.8 pyLDAvis-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh7k1URJ7UVR"
      },
      "source": [
        "# datapath = '/mnt/6016589416586D52/Users/z5204044/GoogleDrive/GoogleDrive/Data/' # Local C1314\n",
        "# datapath = '/mnt/16A4A9BCA4A99EAD/GoogleDrive/Data/' # Local Ryzen\n",
        "datapath = 'drive/My Drive/Data/' # Remote"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QunjBk8t7UVW"
      },
      "source": [
        "# Remote\n",
        "from science_science.sciosci.assets import text_assets as ta\n",
        "from science_science.DEC.DEC_keras import DEC_simple_run\n",
        "\n",
        "# Local\n",
        "# from sciosci.assets import text_assets as ta\n",
        "# from DEC.DEC_keras import DEC_simple_run"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ST9wwa7iqRr"
      },
      "source": [
        "import sys\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics.cluster import silhouette_score,homogeneity_score,adjusted_rand_score\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score,adjusted_mutual_info_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfTransformer , TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Load data and init\n",
        "# =============================================================================\n",
        "data_address =  datapath+\"Corpus/KPRIS/embeddings/deflemm/bert scibert corpus.csv\"\n",
        "label_address =  datapath+\"Corpus/KPRIS/labels\"\n",
        "\n",
        "vectors = pd.read_csv(data_address)\n",
        "labels = pd.read_csv(label_address,names=['label'])\n",
        "labels_f = pd.factorize(labels.label)\n",
        "X = vectors.values\n",
        "Y = labels_f[0]\n",
        "n_clusters = 5\n",
        "\n",
        "labels_task_1 = labels[(labels['label']=='car') | (labels['label']=='memory')]\n",
        "vectors_task_1 = vectors.iloc[labels_task_1.index]\n",
        "labels_task_1_f = pd.factorize(labels_task_1.label)\n",
        "X_task_1 = vectors_task_1.values\n",
        "Y_task_1 = labels_task_1_f[0]\n",
        "n_clusters_task_1 = 2\n",
        "\n",
        "results = pd.DataFrame([],columns=['Method','parameter','Silhouette','Homogeneity','NMI','AMI','ARI'])\n",
        "# =============================================================================\n",
        "# Evaluation method\n",
        "# =============================================================================\n",
        "def evaluate(X,Y,predicted_labels):\n",
        "    \n",
        "    df = pd.DataFrame(predicted_labels,columns=['label'])\n",
        "    if len(df.groupby('label').groups)<2:\n",
        "        return [0,0,0,0,0]\n",
        "    \n",
        "    return [silhouette_score(X, predicted_labels, metric='euclidean'),\n",
        "                    homogeneity_score(Y, predicted_labels),\n",
        "                    normalized_mutual_info_score(Y, predicted_labels),\n",
        "                    adjusted_mutual_info_score(Y, predicted_labels),\n",
        "                    adjusted_rand_score(Y, predicted_labels)]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWpVWF_DisHc"
      },
      "source": [
        "\n",
        "# =============================================================================\n",
        "# K-means\n",
        "# =============================================================================\n",
        "print('\\n- k-means random -----------------------')\n",
        "for fold in tqdm(range(20)):\n",
        "    seed = randint(0,10**5)\n",
        "    model = KMeans(n_clusters=n_clusters,n_init=20, init='random', random_state=seed).fit(X)\n",
        "    predicted_labels = model.labels_\n",
        "    tmp_results = ['k-means random','seed '+str(seed)]+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# K-means with init='k-means++'\n",
        "# =============================================================================\n",
        "print('\\n- k-means++ -----------------------')\n",
        "for fold in tqdm(range(20)):\n",
        "    seed = randint(0,10**5)\n",
        "    model = KMeans(n_clusters=n_clusters,n_init=20,init='k-means++', random_state=seed).fit(X)\n",
        "    predicted_labels = model.labels_\n",
        "    tmp_results = ['k-means++','seed '+str(seed)]+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# Agglomerative\n",
        "# =============================================================================\n",
        "print('\\n- Agglomerative -----------------------')\n",
        "for fold in tqdm(range(4)):\n",
        "    model = AgglomerativeClustering(n_clusters=n_clusters,linkage='ward').fit(X)\n",
        "    predicted_labels = model.labels_\n",
        "    tmp_results = ['Agglomerative','ward']+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# DBSCAN\n",
        "# =============================================================================\n",
        "eps=0.000001\n",
        "print('\\n- DBSCAN -----------------------')\n",
        "for fold in tqdm(range(19)):\n",
        "    eps = eps+0.05\n",
        "    model = DBSCAN(eps=eps, min_samples=10,n_jobs=15).fit(X)\n",
        "    predicted_labels = model.labels_\n",
        "    tmp_results = ['DBSCAN','eps '+str(eps)]+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# Deep no min_max_scaling\n",
        "# =============================================================================\n",
        "archs = [[500, 500, 2000, 10],[500, 1000, 2000, 10],[500, 1000, 1000, 10],\n",
        "         [500, 500, 2000, 100],[500, 1000, 2000, 100],[500, 1000, 1000, 100],\n",
        "         [100, 300, 600, 10],[300, 500, 2000, 10],[700, 1000, 2000, 10],\n",
        "         [200, 500, 10],[500, 1000, 10],[1000, 2000, 10],\n",
        "         [200, 500, 100],[500, 1000, 100],[1000, 2000, 100],\n",
        "         [1000, 500, 10],[500, 200, 10],[200, 100, 10],\n",
        "         [1000, 1000, 2000, 10],[1000, 1500, 2000, 10],[1000, 1500, 1000, 10],\n",
        "         [1000, 1000, 2000,500, 10],[1000, 1500, 2000,500, 10],[1000, 1500, 1000, 500, 10],\n",
        "         [500, 500, 2000, 500, 10],[500, 1000, 2000, 500, 10],[500, 1000, 1000, 500, 10]]\n",
        "print('\\n- DEC -----------------------')\n",
        "for fold in tqdm(archs):\n",
        "    seed = randint(0,10**4)\n",
        "    np.random.seed(seed)\n",
        "    predicted_labels = DEC_simple_run(X,minmax_scale_custom_data=False,n_clusters=5,architecture=fold,pretrain_epochs=400)\n",
        "    tmp_results = ['DEC',str(seed)+' '+str(fold)]+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# Deep with min_max_scaling\n",
        "# =============================================================================\n",
        "archs = [[500, 500, 2000, 10],[500, 1000, 2000, 10],[500, 1000, 1000, 10],\n",
        "         [500, 500, 2000, 100],[500, 1000, 2000, 100],[500, 1000, 1000, 100],\n",
        "         [100, 300, 600, 10],[300, 500, 2000, 10],[700, 1000, 2000, 10],\n",
        "         [200, 500, 10],[500, 1000, 10],[1000, 2000, 10],\n",
        "         [200, 500, 100],[500, 1000, 100],[1000, 2000, 100],\n",
        "         [1000, 500, 10],[500, 200, 10],[200, 100, 10],\n",
        "         [1000, 1000, 2000, 10],[1000, 1500, 2000, 10],[1000, 1500, 1000, 10],\n",
        "         [1000, 1000, 2000,500, 10],[1000, 1500, 2000,500, 10],[1000, 1500, 1000, 500, 10],\n",
        "         [500, 500, 2000, 500, 10],[500, 1000, 2000, 500, 10],[500, 1000, 1000, 500, 10]]\n",
        "print('\\n- DEC -----------------------')\n",
        "for fold in tqdm(archs):\n",
        "    seed = randint(0,10**4)\n",
        "    np.random.seed(seed)\n",
        "    predicted_labels = DEC_simple_run(X,minmax_scale_custom_data=False,n_clusters=5,architecture=fold,pretrain_epochs=400)\n",
        "    tmp_results = ['DEC',str(seed)+' '+str(fold)]+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAlZI5NwiuHG"
      },
      "source": [
        "# =============================================================================\n",
        "# Save to disk\n",
        "# =============================================================================\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(data_address+' clustering results _ new',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfxkBGq9HBO3"
      },
      "source": [
        "Mini test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hILmmi_qG9IH"
      },
      "source": [
        "# =============================================================================\n",
        "# Deep no min_max_scaling\n",
        "# =============================================================================\n",
        "archs = [[200, 1000, 2000,100],[200, 1000, 2000,200],[200, 1000, 2000, 500],\n",
        "            [200, 500, 1000, 500],[200, 500, 1000, 200],[200, 500, 1000, 100],\n",
        "            [200, 1000, 2000, 10],[200, 1000, 2000, 10],[200, 1000, 2000, 10],\n",
        "            [1536,3072,1536,100],[1536,3072,1536,100],[1536,3072,1536,100],\n",
        "            [1536,3072,1536,10],[1536,3072,1536,10],[1536,3072,1536,10],\n",
        "            [1536,3072,1536,100,10],[1536,3072,1536,100,10],[1536,3072,1536,100,10],\n",
        "            [1536,3072,1536,100],[1536,3072,1536,100],[1536,3072,1536,100],\n",
        "            [1536,3072,1536,10],[1536,3072,1536,10],[1536,3072,1536,10],\n",
        "            [1536,3072,1536,100,10],[1536,3072,1536,100,10],[1536,3072,1536,100,10]]\n",
        "\n",
        "print('\\n- DEC -----------------------')\n",
        "for fold in tqdm(archs):\n",
        "    seed = randint(0,10**4)\n",
        "    np.random.seed(seed)\n",
        "    try:\n",
        "        predicted_labels = DEC_simple_run(X,minmax_scale_custom_data=False,n_clusters=5,architecture=fold,pretrain_epochs=400)\n",
        "        tmp_results = ['DEC',str(seed)+' '+str(fold)]+evaluate(X,Y,predicted_labels)\n",
        "        tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "        results = results.append(tmp_results, ignore_index=True)\n",
        "    except:\n",
        "        print('Some error happened, skipping ',fold)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# Save to disk\n",
        "# =============================================================================\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(data_address+' clustering results _ [200, 1000, 2000,100]',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}