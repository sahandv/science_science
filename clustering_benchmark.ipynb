{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clustering_benchmark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP7yTTti5DXz1BFPnLSIV6b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahandv/science_science/blob/master/clustering_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIuk5KrtjB3Q"
      },
      "source": [
        "import os\n",
        "print(os.environ['CONDA_DEFAULT_ENV'])\n",
        "# datapath = '/mnt/6016589416586D52/Users/z5204044/GoogleDrive/GoogleDrive/Data/' # Local\n",
        "datapath = 'drive/My Drive/Data/' # Remote\n",
        "!rm -rf 'science_science'\n",
        "username = \"sahandv\"#@param {type:\"string\"}\n",
        "# password = \"\"#@param {type:\"string\"} \n",
        "!git clone https://github.com/$username/science_science.git\n",
        "!ls\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!pip install -r 'science_science/requirements.txt'\n",
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ST9wwa7iqRr"
      },
      "source": [
        "import sys\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics.cluster import silhouette_score,homogeneity_score,adjusted_rand_score\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score,adjusted_mutual_info_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfTransformer , TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from science_science.sciosci.assets import text_assets as ta\n",
        "from science_science.DEC.DEC_keras import DEC_simple_run\n",
        "\n",
        "# =============================================================================\n",
        "# Load data and init\n",
        "# =============================================================================\n",
        "data_address =  datapath+\"Corpus/KPRIS/embeddings/deflemm/Doc2Vec patent corpus\"\n",
        "label_address =  datapath+\"Corpus/KPRIS/labels\"\n",
        "\n",
        "vectors = pd.read_csv(data_address)\n",
        "labels = pd.read_csv(label_address,names=['label'])\n",
        "labels_f = pd.factorize(labels.label)\n",
        "X = vectors.values\n",
        "Y = labels_f[0]\n",
        "n_clusters = 5\n",
        "\n",
        "labels_task_1 = labels[(labels['label']=='car') | (labels['label']=='memory')]\n",
        "vectors_task_1 = vectors.iloc[labels_task_1.index]\n",
        "labels_task_1_f = pd.factorize(labels_task_1.label)\n",
        "X_task_1 = vectors_task_1.values\n",
        "Y_task_1 = labels_task_1_f[0]\n",
        "n_clusters_task_1 = 2\n",
        "\n",
        "results = pd.DataFrame([],columns=['Method','parameter','Silhouette','Homogeneity','NMI','AMI','ARI'])\n",
        "# =============================================================================\n",
        "# Evaluation method\n",
        "# =============================================================================\n",
        "def evaluate(X,Y,predicted_labels):\n",
        "    \n",
        "    df = pd.DataFrame(predicted_labels,columns=['label'])\n",
        "    if len(df.groupby('label').groups)<2:\n",
        "        return [0,0,0,0,0]\n",
        "    \n",
        "    return [silhouette_score(X, predicted_labels, metric='euclidean'),\n",
        "                    homogeneity_score(Y, predicted_labels),\n",
        "                    normalized_mutual_info_score(Y, predicted_labels),\n",
        "                    adjusted_mutual_info_score(Y, predicted_labels),\n",
        "                    adjusted_rand_score(Y, predicted_labels)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWpVWF_DisHc"
      },
      "source": [
        "\n",
        "# =============================================================================\n",
        "# K-means\n",
        "# =============================================================================\n",
        "print('\\n- k-means random -----------------------')\n",
        "for fold in tqdm(range(20)):\n",
        "    seed = randint(0,10**5)\n",
        "    model = KMeans(n_clusters=n_clusters,n_init=20, init='random', random_state=seed).fit(X)\n",
        "    predicted_labels = model.labels_\n",
        "    tmp_results = ['k-means random','seed '+str(seed)]+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# K-means with init='k-means++'\n",
        "# =============================================================================\n",
        "print('\\n- k-means++ -----------------------')\n",
        "for fold in tqdm(range(20)):\n",
        "    seed = randint(0,10**5)\n",
        "    model = KMeans(n_clusters=n_clusters,n_init=20,init='k-means++', random_state=seed).fit(X)\n",
        "    predicted_labels = model.labels_\n",
        "    tmp_results = ['k-means++','seed '+str(seed)]+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# Agglomerative\n",
        "# =============================================================================\n",
        "print('\\n- Agglomerative -----------------------')\n",
        "for fold in tqdm(range(4)):\n",
        "    model = AgglomerativeClustering(n_clusters=n_clusters,linkage='ward').fit(X)\n",
        "    predicted_labels = model.labels_\n",
        "    tmp_results = ['Agglomerative','ward']+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# DBSCAN\n",
        "# =============================================================================\n",
        "eps=0.000001\n",
        "print('\\n- DBSCAN -----------------------')\n",
        "for fold in tqdm(range(19)):\n",
        "    eps = eps+0.05\n",
        "    model = DBSCAN(eps=eps, min_samples=10,n_jobs=15).fit(X)\n",
        "    predicted_labels = model.labels_\n",
        "    tmp_results = ['DBSCAN','eps '+str(eps)]+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# Deep no min_max_scaling\n",
        "# =============================================================================\n",
        "archs = [[500, 500, 2000, 10],[500, 1000, 2000, 10],[500, 1000, 1000, 10],\n",
        "         [500, 500, 2000, 100],[500, 1000, 2000, 100],[500, 1000, 1000, 100],\n",
        "         [100, 300, 600, 10],[300, 500, 2000, 10],[700, 1000, 2000, 10],\n",
        "         [200, 500, 10],[500, 1000, 10],[1000, 2000, 10],\n",
        "         [200, 500, 100],[500, 1000, 100],[1000, 2000, 100],\n",
        "         [1000, 500, 10],[500, 200, 10],[200, 100, 10],\n",
        "         [1000, 1000, 2000, 10],[1000, 1500, 2000, 10],[1000, 1500, 1000, 10],\n",
        "         [1000, 1000, 2000,500, 10],[1000, 1500, 2000,500, 10],[1000, 1500, 1000, 500, 10],\n",
        "         [500, 500, 2000, 500, 10],[500, 1000, 2000, 500, 10],[500, 1000, 1000, 500, 10]]\n",
        "print('\\n- DEC -----------------------')\n",
        "for fold in tqdm(archs):\n",
        "    predicted_labels = DEC_simple_run(X,minmax_scale_custom_data=False,n_clusters=5,architecture=fold,pretrain_epochs=300)\n",
        "    tmp_results = ['DEC',str(fold)]+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# Deep with min_max_scaling\n",
        "# =============================================================================\n",
        "archs = [[500, 500, 2000, 10],[500, 1000, 2000, 10],[500, 1000, 1000, 10],\n",
        "         [500, 500, 2000, 100],[500, 1000, 2000, 100],[500, 1000, 1000, 100],\n",
        "         [100, 300, 600, 10],[300, 500, 2000, 10],[700, 1000, 2000, 10],\n",
        "         [200, 500, 10],[500, 1000, 10],[1000, 2000, 10],\n",
        "         [200, 500, 100],[500, 1000, 100],[1000, 2000, 100],\n",
        "         [1000, 500, 10],[500, 200, 10],[200, 100, 10],\n",
        "         [1000, 1000, 2000, 10],[1000, 1500, 2000, 10],[1000, 1500, 1000, 10],\n",
        "         [1000, 1000, 2000,500, 10],[1000, 1500, 2000,500, 10],[1000, 1500, 1000, 500, 10],\n",
        "         [500, 500, 2000, 500, 10],[500, 1000, 2000, 500, 10],[500, 1000, 1000, 500, 10]]\n",
        "print('\\n- DEC -----------------------')\n",
        "for fold in tqdm(archs):\n",
        "    predicted_labels = DEC_simple_run(X,minmax_scale_custom_data=True,n_clusters=5,architecture=fold,pretrain_epochs=300)\n",
        "    tmp_results = ['DEC minmax scaler',str(fold)]+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAlZI5NwiuHG"
      },
      "source": [
        "# =============================================================================\n",
        "# Save to disk\n",
        "# =============================================================================\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(data_address+' clustering results',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}