{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "clustering_benchmark.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahandv/science_science/blob/master/clustering_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gg40GIClXPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c32a06d-fe2a-4ae5-e888-7e39171b157d"
      },
      "source": [
        "!rm -rf 'science_science'\n",
        "username = \"sahandv\"#@param {type:\"string\"}\n",
        "# password = \"\"#@param {type:\"string\"} \n",
        "!git clone https://github.com/$username/science_science.git\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'science_science'...\n",
            "remote: Enumerating objects: 317, done.\u001b[K\n",
            "remote: Counting objects: 100% (317/317), done.\u001b[K\n",
            "remote: Compressing objects: 100% (279/279), done.\u001b[K\n",
            "remote: Total 1319 (delta 178), reused 162 (delta 37), pack-reused 1002\u001b[K\n",
            "Receiving objects: 100% (1319/1319), 102.74 MiB | 41.22 MiB/s, done.\n",
            "Resolving deltas: 100% (773/773), done.\n",
            "sample_data  science_science\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIuk5KrtjB3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "933afbc5-a1f3-4ff6-8d8e-d4a9aa08f2cb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!pip install -r 'science_science/requirements.txt'\n",
        "# !pip install tensorflow\n",
        "# !pip install keras"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 1)) (1.19.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 3)) (1.1.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 4)) (3.2.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 5)) (2.2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 6)) (0.22.2.post1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 7)) (0.16.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 9)) (0.11.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 10)) (1.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 11)) (3.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 12)) (7.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 13)) (0.8.7)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 14)) (3.6.0)\n",
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 16.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 16)) (2.5)\n",
            "Collecting netgraph\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/ed/1e163a923cc58feab143656f2eefd69e5a1d2e323423f62c08b5100a4cbe/netgraph-3.1.8.tar.gz (40kB)\n",
            "\u001b[K     |████████████████████████████████| 40kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: yellowbrick in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 18)) (0.9.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 19)) (4.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r science_science/requirements.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r science_science/requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r science_science/requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (0.8.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (51.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r science_science/requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r science_science/requirements.txt (line 7)) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r science_science/requirements.txt (line 7)) (1.1.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r science_science/requirements.txt (line 11)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r science_science/requirements.txt (line 11)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r science_science/requirements.txt (line 11)) (1.3.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->-r science_science/requirements.txt (line 14)) (4.0.1)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r science_science/requirements.txt (line 15)) (0.36.2)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r science_science/requirements.txt (line 15)) (2.11.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r science_science/requirements.txt (line 15)) (2.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r science_science/requirements.txt (line 15)) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r science_science/requirements.txt (line 15)) (0.16.0)\n",
            "Collecting funcy\n",
            "  Downloading https://files.pythonhosted.org/packages/66/89/479de0afbbfb98d1c4b887936808764627300208bb771fcd823403645a36/funcy-1.15-py2.py3-none-any.whl\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->-r science_science/requirements.txt (line 16)) (4.4.2)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->-r science_science/requirements.txt (line 19)) (1.3.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->-r science_science/requirements.txt (line 5)) (3.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r science_science/requirements.txt (line 5)) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r science_science/requirements.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r science_science/requirements.txt (line 5)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r science_science/requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis->-r science_science/requirements.txt (line 15)) (1.1.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis->-r science_science/requirements.txt (line 15)) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis->-r science_science/requirements.txt (line 15)) (1.10.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis->-r science_science/requirements.txt (line 15)) (8.6.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis->-r science_science/requirements.txt (line 15)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis->-r science_science/requirements.txt (line 15)) (20.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r science_science/requirements.txt (line 5)) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r science_science/requirements.txt (line 5)) (3.4.0)\n",
            "Building wheels for collected packages: pyLDAvis, netgraph\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=703d43501249594b447aa6e7f8d588c0ab5c17937f441d0a3369baefdb284b15\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "  Building wheel for netgraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for netgraph: filename=netgraph-3.1.8-cp36-none-any.whl size=45928 sha256=daa4b6a34f841e201bf4c563e7022f46fad13770c83ec5cd3d28f3353d450931\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/c2/49/30a980fcf7c864676d6184e46ea8860fff2b7fa6f324cad69a\n",
            "Successfully built pyLDAvis netgraph\n",
            "Installing collected packages: funcy, pyLDAvis, netgraph\n",
            "Successfully installed funcy-1.15 netgraph-3.1.8 pyLDAvis-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh7k1URJ7UVR"
      },
      "source": [
        "# datapath = '/mnt/6016589416586D52/Users/z5204044/GoogleDrive/GoogleDrive/Data/' # Local C1314\n",
        "# datapath = '/mnt/16A4A9BCA4A99EAD/GoogleDrive/Data/' # Local Ryzen\n",
        "datapath = 'drive/My Drive/Data/' # Remote"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QunjBk8t7UVW"
      },
      "source": [
        "# Remote\n",
        "from science_science.sciosci.assets import text_assets as ta\n",
        "from science_science.DEC.DEC_keras import DEC_simple_run\n",
        "\n",
        "# Local\n",
        "# from sciosci.assets import text_assets as ta\n",
        "# from DEC.DEC_keras import DEC_simple_run"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B-UsLBFge1d"
      },
      "source": [
        "## Sequential test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ST9wwa7iqRr"
      },
      "source": [
        "import sys\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics.cluster import silhouette_score,homogeneity_score,adjusted_rand_score\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score,adjusted_mutual_info_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfTransformer , TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Load data and init\n",
        "# =============================================================================\n",
        "data_address =  datapath+\"Corpus/KPRIS/embeddings/deflemm/bert scibert corpus.csv\"\n",
        "label_address =  datapath+\"Corpus/KPRIS/labels\"\n",
        "\n",
        "vectors = pd.read_csv(data_address)\n",
        "labels = pd.read_csv(label_address,names=['label'])\n",
        "labels_f = pd.factorize(labels.label)\n",
        "X = vectors.values\n",
        "Y = labels_f[0]\n",
        "n_clusters = 5\n",
        "\n",
        "labels_task_1 = labels[(labels['label']=='car') | (labels['label']=='memory')]\n",
        "vectors_task_1 = vectors.iloc[labels_task_1.index]\n",
        "labels_task_1_f = pd.factorize(labels_task_1.label)\n",
        "X_task_1 = vectors_task_1.values\n",
        "Y_task_1 = labels_task_1_f[0]\n",
        "n_clusters_task_1 = 2\n",
        "\n",
        "results = pd.DataFrame([],columns=['Method','parameter','Silhouette','Homogeneity','NMI','AMI','ARI'])\n",
        "# =============================================================================\n",
        "# Evaluation method\n",
        "# =============================================================================\n",
        "def evaluate(X,Y,predicted_labels):\n",
        "    \n",
        "    df = pd.DataFrame(predicted_labels,columns=['label'])\n",
        "    if len(df.groupby('label').groups)<2:\n",
        "        return [0,0,0,0,0]\n",
        "    \n",
        "    return [silhouette_score(X, predicted_labels, metric='euclidean'),\n",
        "                    homogeneity_score(Y, predicted_labels),\n",
        "                    normalized_mutual_info_score(Y, predicted_labels),\n",
        "                    adjusted_mutual_info_score(Y, predicted_labels),\n",
        "                    adjusted_rand_score(Y, predicted_labels)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuALxBDaXnCg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWpVWF_DisHc"
      },
      "source": [
        "\n",
        "# =============================================================================\n",
        "# K-means\n",
        "# =============================================================================\n",
        "print('\\n- k-means random -----------------------')\n",
        "for fold in tqdm(range(20)):\n",
        "    seed = randint(0,10**5)\n",
        "    model = KMeans(n_clusters=n_clusters,n_init=20, init='random', random_state=seed).fit(X)\n",
        "    predicted_labels = model.labels_\n",
        "    tmp_results = ['k-means random','seed '+str(seed)]+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# K-means with init='k-means++'\n",
        "# =============================================================================\n",
        "print('\\n- k-means++ -----------------------')\n",
        "for fold in tqdm(range(20)):\n",
        "    seed = randint(0,10**5)\n",
        "    model = KMeans(n_clusters=n_clusters,n_init=20,init='k-means++', random_state=seed).fit(X)\n",
        "    predicted_labels = model.labels_\n",
        "    tmp_results = ['k-means++','seed '+str(seed)]+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# Agglomerative\n",
        "# =============================================================================\n",
        "print('\\n- Agglomerative -----------------------')\n",
        "for fold in tqdm(range(4)):\n",
        "    model = AgglomerativeClustering(n_clusters=n_clusters,linkage='ward').fit(X)\n",
        "    predicted_labels = model.labels_\n",
        "    tmp_results = ['Agglomerative','ward']+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# DBSCAN\n",
        "# =============================================================================\n",
        "eps=0.000001\n",
        "print('\\n- DBSCAN -----------------------')\n",
        "for fold in tqdm(range(19)):\n",
        "    eps = eps+0.05\n",
        "    model = DBSCAN(eps=eps, min_samples=10,n_jobs=15).fit(X)\n",
        "    predicted_labels = model.labels_\n",
        "    tmp_results = ['DBSCAN','eps '+str(eps)]+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# Deep no min_max_scaling\n",
        "# =============================================================================\n",
        "archs = [[500, 500, 2000, 10],[500, 1000, 2000, 10],[500, 1000, 1000, 10],\n",
        "         [500, 500, 2000, 100],[500, 1000, 2000, 100],[500, 1000, 1000, 100],\n",
        "         [100, 300, 600, 10],[300, 500, 2000, 10],[700, 1000, 2000, 10],\n",
        "         [200, 500, 10],[500, 1000, 10],[1000, 2000, 10],\n",
        "         [200, 500, 100],[500, 1000, 100],[1000, 2000, 100],\n",
        "         [1000, 500, 10],[500, 200, 10],[200, 100, 10],\n",
        "         [1000, 1000, 2000, 10],[1000, 1500, 2000, 10],[1000, 1500, 1000, 10],\n",
        "         [1000, 1000, 2000,500, 10],[1000, 1500, 2000,500, 10],[1000, 1500, 1000, 500, 10],\n",
        "         [500, 500, 2000, 500, 10],[500, 1000, 2000, 500, 10],[500, 1000, 1000, 500, 10]]\n",
        "print('\\n- DEC -----------------------')\n",
        "for fold in tqdm(archs):\n",
        "    seed = randint(0,10**4)\n",
        "    np.random.seed(seed)\n",
        "    predicted_labels = DEC_simple_run(X,minmax_scale_custom_data=False,n_clusters=5,architecture=fold,pretrain_epochs=400)\n",
        "    tmp_results = ['DEC',str(seed)+' '+str(fold)]+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# Deep with min_max_scaling\n",
        "# =============================================================================\n",
        "archs = [[500, 500, 2000, 10],[500, 1000, 2000, 10],[500, 1000, 1000, 10],\n",
        "         [500, 500, 2000, 100],[500, 1000, 2000, 100],[500, 1000, 1000, 100],\n",
        "         [100, 300, 600, 10],[300, 500, 2000, 10],[700, 1000, 2000, 10],\n",
        "         [200, 500, 10],[500, 1000, 10],[1000, 2000, 10],\n",
        "         [200, 500, 100],[500, 1000, 100],[1000, 2000, 100],\n",
        "         [1000, 500, 10],[500, 200, 10],[200, 100, 10],\n",
        "         [1000, 1000, 2000, 10],[1000, 1500, 2000, 10],[1000, 1500, 1000, 10],\n",
        "         [1000, 1000, 2000,500, 10],[1000, 1500, 2000,500, 10],[1000, 1500, 1000, 500, 10],\n",
        "         [500, 500, 2000, 500, 10],[500, 1000, 2000, 500, 10],[500, 1000, 1000, 500, 10]]\n",
        "print('\\n- DEC -----------------------')\n",
        "for fold in tqdm(archs):\n",
        "    seed = randint(0,10**4)\n",
        "    np.random.seed(seed)\n",
        "    predicted_labels = DEC_simple_run(X,minmax_scale_custom_data=False,n_clusters=5,architecture=fold,pretrain_epochs=400)\n",
        "    tmp_results = ['DEC',str(seed)+' '+str(fold)]+evaluate(X,Y,predicted_labels)\n",
        "    tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    results = results.append(tmp_results, ignore_index=True)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAlZI5NwiuHG"
      },
      "source": [
        "# =============================================================================\n",
        "# Save to disk\n",
        "# =============================================================================\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(data_address+' clustering results _ new',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfxkBGq9HBO3"
      },
      "source": [
        "Mini test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hILmmi_qG9IH"
      },
      "source": [
        "# =============================================================================\n",
        "# Deep no min_max_scaling\n",
        "# =============================================================================\n",
        "archs = [[200, 1000, 2000,100],[200, 1000, 2000,200],[200, 1000, 2000, 500],\n",
        "            [200, 500, 1000, 500],[200, 500, 1000, 200],[200, 500, 1000, 100],\n",
        "            [200, 1000, 2000, 10],[200, 1000, 2000, 10],[200, 1000, 2000, 10],\n",
        "            [1536,3072,1536,100],[1536,3072,1536,100],[1536,3072,1536,100],\n",
        "            [1536,3072,1536,10],[1536,3072,1536,10],[1536,3072,1536,10],\n",
        "            [1536,3072,1536,100,10],[1536,3072,1536,100,10],[1536,3072,1536,100,10],\n",
        "            [1536,3072,1536,100],[1536,3072,1536,100],[1536,3072,1536,100],\n",
        "            [1536,3072,1536,10],[1536,3072,1536,10],[1536,3072,1536,10],\n",
        "            [1536,3072,1536,100,10],[1536,3072,1536,100,10],[1536,3072,1536,100,10]]\n",
        "\n",
        "print('\\n- DEC -----------------------')\n",
        "for fold in tqdm(archs):\n",
        "    seed = randint(0,10**4)\n",
        "    np.random.seed(seed)\n",
        "    try:\n",
        "        predicted_labels = DEC_simple_run(X,minmax_scale_custom_data=False,n_clusters=5,architecture=fold,pretrain_epochs=400)\n",
        "        tmp_results = ['DEC',str(seed)+' '+str(fold)]+evaluate(X,Y,predicted_labels)\n",
        "        tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "        results = results.append(tmp_results, ignore_index=True)\n",
        "    except:\n",
        "        print('Some error happened, skipping ',fold)\n",
        "mean = results.mean(axis=0)\n",
        "maxx = results.max(axis=0)\n",
        "print(mean)\n",
        "print(maxx)\n",
        "# =============================================================================\n",
        "# Save to disk\n",
        "# =============================================================================\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(data_address+' clustering results _ [200, 1000, 2000,100]',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2vILz8TYFyT"
      },
      "source": [
        "## Functional test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSjcX-nRZW9f"
      },
      "source": [
        "import sys\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics.cluster import silhouette_score,homogeneity_score,adjusted_rand_score\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score,adjusted_mutual_info_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfTransformer , TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from science_science.sciosci.assets import text_assets as ta\n",
        "from science_science.DEC.DEC_keras import DEC_simple_run"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFDsr2OQYO0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fcf0e28-dd1d-4be6-f5a3-142ad77c0528"
      },
      "source": [
        "data_dir =  datapath+\"Corpus/cora-classify/cora/\"\n",
        "label_address =  datapath+\"Corpus/cora-classify/cora/corpus classes1 with citations\"\n",
        "\n",
        "vec_file_names = ['embeddings/node2vec deepwalk-100-6-100 p1q1','embeddings/node2ve deepwalk 80-10-128']#,'Doc2Vec patent corpus',\n",
        "                #   ,'embeddings/node2vec-80-10-128 p1q0.5','embeddings/node2vec-80-10-128 p4q1'],\n",
        "                  # 'FastText SIF patent_wos corpus','FastText SIF wos corpus']\n",
        "labels = pd.read_csv(label_address,names=['label'])\n",
        "\n",
        "clusters = labels.groupby('label').groups\n",
        "\n",
        "for file_name in vec_file_names:\n",
        "    gc.collect()\n",
        "    output_file_name = data_dir+file_name+' clustering results'\n",
        "    run_all_tests(data_dir+file_name,output_file_name,labels,len(list(clusters.keys())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Will write the results to drive/My Drive/Data/Corpus/cora-classify/cora/embeddings/Doc2Vec cora corpus dm1 with citations clustering results\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/39 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "- DEC -----------------------\n",
            "Model: \"AE\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "encoder_0 (Dense)            (None, 500)               50500     \n",
            "_________________________________________________________________\n",
            "encoder_1 (Dense)            (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "encoder_2 (Dense)            (None, 2000)              1002000   \n",
            "_________________________________________________________________\n",
            "encoder_3 (Dense)            (None, 500)               1000500   \n",
            "_________________________________________________________________\n",
            "decoder_3 (Dense)            (None, 2000)              1002000   \n",
            "_________________________________________________________________\n",
            "decoder_2 (Dense)            (None, 500)               1000500   \n",
            "_________________________________________________________________\n",
            "decoder_1 (Dense)            (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "decoder_0 (Dense)            (None, 100)               50100     \n",
            "=================================================================\n",
            "Total params: 4,606,600\n",
            "Trainable params: 4,606,600\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "...Pretraining...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBojz1AZYJMg"
      },
      "source": [
        "\n",
        "# =============================================================================\n",
        "# Evaluation method\n",
        "# =============================================================================\n",
        "def evaluate(X,Y,predicted_labels):\n",
        "    \n",
        "    df = pd.DataFrame(predicted_labels,columns=['label'])\n",
        "    if len(df.groupby('label').groups)<2:\n",
        "        return [0,0,0,0,0]\n",
        "    \n",
        "    try:\n",
        "        sil = silhouette_score(X, predicted_labels, metric='euclidean')\n",
        "    except:\n",
        "        sil = 0\n",
        "        \n",
        "    return [sil,\n",
        "            homogeneity_score(Y, predicted_labels),\n",
        "            normalized_mutual_info_score(Y, predicted_labels),\n",
        "            adjusted_mutual_info_score(Y, predicted_labels),\n",
        "            adjusted_rand_score(Y, predicted_labels)]\n",
        "# =============================================================================\n",
        "# Cluster and evaluate\n",
        "# =============================================================================\n",
        "def run_all_tests(data_address,output_file_name,labels,k):\n",
        "    print('k=',k)\n",
        "    print('Will write the results to',output_file_name)\n",
        "    \n",
        "    column_names = ['Method','parameter','Silhouette','Homogeneity','NMI','AMI','ARI']\n",
        "    vectors = pd.read_csv(data_address)\n",
        "    # data_dir+file_name+' dm_concat'\n",
        "    labels_f = pd.factorize(labels.label)\n",
        "\n",
        "    X = vectors.values\n",
        "    Y = labels_f[0]\n",
        "    n_clusters = k\n",
        "    \n",
        "    labels_task_1 = labels[(labels['label']=='car') | (labels['label']=='memory')]\n",
        "    vectors_task_1 = vectors.iloc[labels_task_1.index]\n",
        "    labels_task_1_f = pd.factorize(labels_task_1.label)\n",
        "    X_task_1 = vectors_task_1.values\n",
        "    Y_task_1 = labels_task_1_f[0]\n",
        "    n_clusters_task_1 = 2\n",
        "    \n",
        "    results = pd.DataFrame([],columns=column_names)\n",
        "    results_template = results.copy()\n",
        "    # =============================================================================\n",
        "    # Deep no min_max_scaling\n",
        "    # =============================================================================\n",
        "    archs = [\n",
        "            [500, 500, 2000, 500],[500, 500, 2000, 500],[500, 500, 2000, 500],\n",
        "            # [500, 500, 2000, 500],[500, 500, 2000, 500],[500, 500, 2000, 500],\n",
        "            [500, 1000, 2000, 100],[500, 1000, 2000, 100],[500, 1000, 2000, 100],\n",
        "            [200, 1000, 2000,100, 10],[200, 1000, 2000,200, 10],[200, 1000, 2000, 500, 10],\n",
        "            # [200, 500, 1000, 500, 10],[200, 500, 1000, 200, 10],[200, 500, 1000, 100, 10],\n",
        "            # [200, 1000, 2000,100, 10],[200, 1000, 2000,200, 10],[200, 1000, 2000, 500, 10],\n",
        "            [200, 500, 1000, 500, 10],[200, 500, 1000, 200, 10],[200, 500, 1000, 100, 10],\n",
        "            # [200, 1000, 2000,100],[200, 1000, 2000,200],[200, 1000, 2000, 500],\n",
        "            # [200, 500, 1000, 500],[200, 500, 1000, 200],[200, 500, 1000, 100],\n",
        "            # [200, 1000, 2000, 10],[200, 1000, 2000, 10],[200, 1000, 2000, 10],\n",
        "            [1536,3072,1536,100],[1536,3072,1536,100],[1536,3072,1536,100],\n",
        "            [256,1024,512,10],[256,1024,512,10],[256,1024,512,10],\n",
        "            [1024,1024,2048,256,10],[1024,1024,2048,256,10],\n",
        "            [512,1024,2048,128,10],[512,1024,2048,128,10],\n",
        "            [512,1024,2048,10],[512,1024,2048,10],\n",
        "            [1024,1024,2048,10],[1024,1024,2048,10],\n",
        "            # [1536,768,384,192,10],[1536,768,384,192,10],[1536,768,192,10],\n",
        "            [1536,3072,1536,100,10],[1536,3072,1536,100,10],[1536,3072,1536,100,10],\n",
        "            # [1536,3072,1536,100],[1536,3072,1536,100],[1536,3072,1536,100],\n",
        "            # [1536,3072,1536,10],[1536,3072,1536,10],[1536,3072,1536,10],\n",
        "            # [1536,3072,1536,100,10],[1536,3072,1536,100,10],[1536,3072,1536,100,10],\n",
        "            # [200,200,100],[200,200,100],[200,200,100],\n",
        "            [200,500,20],[200,500,200],[200,500,200],\n",
        "            # [200,200,10],[200,200,10],[200,200,10],\n",
        "            # [400,400,10],[400,400,10],[400,400,10],\n",
        "            # [400,400,10],[400,400,10],[400,400,10],\n",
        "            [400,1000,10],[400,1000,10],[400,1000,100,10],[400,1000,100,10],\n",
        "            [400,500,10],[400,500,10],[400,500,10],\n",
        "            # [200,200,10],[200,200,10],[200,200,10],\n",
        "            # [200,200,10],[200,200,10],[200,200,10],\n",
        "            [200,200,10],[200,200,10],[200,200,10],\n",
        "            # [200,500,10],[200,500,10],[200,500,10],\n",
        "            # [200,500,10],[200,500,10],[200,500,10],\n",
        "            # [200,500,10],[200,500,10],[200,500,10],\n",
        "            # [200,500,10],[200,500,10],[200,500,10],\n",
        "            [200,500,10],[200,500,10],[200,500,10]]\n",
        "    print('\\n- DEC -----------------------')\n",
        "    for fold in tqdm(archs):\n",
        "        gc.collect()\n",
        "        seed = randint(0,10**4)\n",
        "        np.random.seed(seed)\n",
        "        try:\n",
        "            predicted_labels = DEC_simple_run(X,minmax_scale_custom_data=False,n_clusters=n_clusters,architecture=fold,pretrain_epochs=1000)\n",
        "            tmp_results = ['DEC',str(seed)+' '+str(fold)]+evaluate(X,Y,predicted_labels)\n",
        "            tmp_results_s = pd.Series(tmp_results, index = results.columns)\n",
        "            tmp_results_df = results_template.copy()\n",
        "            tmp_results_df = tmp_results_df.append(tmp_results_s, ignore_index=True)\n",
        "            results = results.append(tmp_results_s, ignore_index=True)\n",
        "        except:\n",
        "            print('Some error happened, skipping ',fold)\n",
        "        \n",
        "        print('writing the fold results to file')\n",
        "        # if file does not exist write header \n",
        "        try:\n",
        "            if not os.path.isfile(output_file_name):\n",
        "                tmp_results_df.to_csv(output_file_name, header=column_names,index=False)\n",
        "            else: # else it exists so append without writing the header\n",
        "                tmp_results_df.to_csv(output_file_name, mode='a', header=False,index=False)\n",
        "        except:\n",
        "            print('something went wrong and could not write the results to file!\\n',\n",
        "                 'You may abort and see what can be done.\\n',\n",
        "                 'Or wait to see the the final results in memory.')\n",
        "        \n",
        "    mean = results.mean(axis=0)\n",
        "    maxx = results.max(axis=0)\n",
        "    print(mean)\n",
        "    print(maxx)\n",
        "    \n",
        "    # # =============================================================================\n",
        "    # # Deep with min_max_scaling\n",
        "    # # =============================================================================\n",
        "    # archs = [[500, 500, 2000, 10],[500, 1000, 2000, 10],[500, 1000, 1000, 10],\n",
        "    #             [500, 500, 2000, 100],[500, 1000, 2000, 100],[500, 1000, 1000, 100],\n",
        "    #             [100, 300, 600, 10],[300, 500, 2000, 10],[700, 1000, 2000, 10],\n",
        "    #             [200, 500, 10],[500, 1000, 10],[1000, 2000, 10],\n",
        "    #             [200, 500, 100],[500, 1000, 100],[1000, 2000, 100],\n",
        "    #             [1000, 500, 10],[500, 200, 10],[200, 100, 10],\n",
        "    #             [1000, 1000, 2000, 10],[1000, 1500, 2000, 10],[1000, 1500, 1000, 10],\n",
        "    #             [1000, 1000, 2000,500, 10],[1000, 1500, 2000,500, 10],[1000, 1500, 1000, 500, 10],\n",
        "    #             [500, 500, 2000, 500, 10],[500, 1000, 2000, 500, 10],[500, 1000, 1000, 500, 10],\n",
        "    #             [200,200,10],[200,200,10],[200,200,10],\n",
        "    #             [200,200,10],[200,200,10],[200,200,10],\n",
        "    #             [200,200,10],[200,200,10],[200,200,10],\n",
        "    #             [200,500,10],[200,500,10],[200,500,10],\n",
        "    #             [200,500,10],[200,500,10],[200,500,10]]\n",
        "    # print('\\n- DEC 2-----------------------')\n",
        "    # for fold in tqdm(archs):\n",
        "    #     predicted_labels = DEC_simple_run(X,minmax_scale_custom_data=True,n_clusters=5,architecture=fold,pretrain_epochs=300)\n",
        "    #     tmp_results = ['DEC minmax scaler',str(fold)]+evaluate(X,Y,predicted_labels)\n",
        "    #     tmp_results = pd.Series(tmp_results, index = results.columns)\n",
        "    #     results = results.append(tmp_results, ignore_index=True)\n",
        "    # mean = results.mean(axis=0)\n",
        "    # maxx = results.max(axis=0)\n",
        "    # print(mean)\n",
        "    # print(maxx)\n",
        "    \n",
        "\n",
        "    # =============================================================================\n",
        "    # K-means\n",
        "    # =============================================================================\n",
        "    print('\\n- k-means random -----------------------')\n",
        "    for fold in tqdm(range(20)):\n",
        "        seed = randint(0,10**5)\n",
        "        model = KMeans(n_clusters=n_clusters,n_init=20, init='random', random_state=seed).fit(X)\n",
        "        predicted_labels = model.labels_\n",
        "        try:\n",
        "            tmp_results = ['k-means random','seed '+str(seed)]+evaluate(X,Y,predicted_labels)\n",
        "            tmp_results_s = pd.Series(tmp_results, index = results.columns)\n",
        "            tmp_results_df = results_template.copy()\n",
        "            tmp_results_df = tmp_results_df.append(tmp_results_s, ignore_index=True)\n",
        "            results = results.append(tmp_results_s, ignore_index=True)\n",
        "        except:\n",
        "            print('Some error happened, skipping ',fold)\n",
        "        \n",
        "        print('writing the fold results to file')\n",
        "        # if file does not exist write header \n",
        "        try:\n",
        "            if not os.path.isfile(output_file_name):\n",
        "                tmp_results_df.to_csv(output_file_name, header=column_names,index=False)\n",
        "            else: # else it exists so append without writing the header\n",
        "                tmp_results_df.to_csv(output_file_name, mode='a', header=False,index=False)\n",
        "        except:\n",
        "            print('something went wrong and could not write the results to file!\\n',\n",
        "                 'You may abort and see what can be done.\\n',\n",
        "                 'Or wait to see the the final results in memory.')\n",
        "    mean = results.mean(axis=0)\n",
        "    maxx = results.max(axis=0)\n",
        "    print(mean)\n",
        "    print(maxx)\n",
        "    # =============================================================================\n",
        "    # K-means with init='k-means++'\n",
        "    # =============================================================================\n",
        "    print('\\n- k-means++ -----------------------')\n",
        "    for fold in tqdm(range(20)):\n",
        "        gc.collect()\n",
        "        seed = randint(0,10**5)\n",
        "        model = KMeans(n_clusters=n_clusters,n_init=20,init='k-means++', random_state=seed).fit(X)\n",
        "        predicted_labels = model.labels_\n",
        "        try:\n",
        "            tmp_results = ['k-means++','seed '+str(seed)]+evaluate(X,Y,predicted_labels)\n",
        "            tmp_results_s = pd.Series(tmp_results, index = results.columns)\n",
        "            tmp_results_df = results_template.copy()\n",
        "            tmp_results_df = tmp_results_df.append(tmp_results_s, ignore_index=True)\n",
        "            results = results.append(tmp_results_s, ignore_index=True)\n",
        "        except:\n",
        "            print('Some error happened, skipping ',fold)\n",
        "        \n",
        "        print('writing the fold results to file')\n",
        "        # if file does not exist write header \n",
        "        try:\n",
        "            if not os.path.isfile(output_file_name):\n",
        "                tmp_results_df.to_csv(output_file_name, header=column_names,index=False)\n",
        "            else: # else it exists so append without writing the header\n",
        "                tmp_results_df.to_csv(output_file_name, mode='a', header=False,index=False)\n",
        "        except:\n",
        "            print('something went wrong and could not write the results to file!\\n',\n",
        "                 'You may abort and see what can be done.\\n',\n",
        "                 'Or wait to see the the final results in memory.')\n",
        "    mean = results.mean(axis=0)\n",
        "    maxx = results.max(axis=0)\n",
        "    print(mean)\n",
        "    print(maxx)\n",
        "    # =============================================================================\n",
        "    # Agglomerative\n",
        "    # =============================================================================\n",
        "    print('\\n- Agglomerative -----------------------')\n",
        "    for fold in tqdm(range(4)):\n",
        "        gc.collect()\n",
        "        model = AgglomerativeClustering(n_clusters=n_clusters,linkage='ward').fit(X)\n",
        "        predicted_labels = model.labels_\n",
        "        try:\n",
        "            tmp_results = ['Agglomerative','ward']+evaluate(X,Y,predicted_labels)\n",
        "            tmp_results_s = pd.Series(tmp_results, index = results.columns)\n",
        "            tmp_results_df = results_template.copy()\n",
        "            tmp_results_df = tmp_results_df.append(tmp_results_s, ignore_index=True)\n",
        "            results = results.append(tmp_results_s, ignore_index=True)\n",
        "        except:\n",
        "            print('Some error happened, skipping ',fold)\n",
        "        \n",
        "        print('writing the fold results to file')\n",
        "        # if file does not exist write header \n",
        "        try:\n",
        "            if not os.path.isfile(output_file_name):\n",
        "                tmp_results_df.to_csv(output_file_name, header=column_names,index=False)\n",
        "            else: # else it exists so append without writing the header\n",
        "                tmp_results_df.to_csv(output_file_name, mode='a', header=False,index=False)\n",
        "        except:\n",
        "            print('something went wrong and could not write the results to file!\\n',\n",
        "                 'You may abort and see what can be done.\\n',\n",
        "                 'Or wait to see the the final results in memory.')\n",
        "    mean = results.mean(axis=0)\n",
        "    maxx = results.max(axis=0)\n",
        "    print(mean)\n",
        "    print(maxx)\n",
        "    # =============================================================================\n",
        "    # DBSCAN\n",
        "    # =============================================================================\n",
        "    eps=0.000001\n",
        "    print('\\n- DBSCAN -----------------------')\n",
        "    for fold in tqdm(range(19)):\n",
        "        gc.collect()\n",
        "        eps = eps+0.05\n",
        "        model = DBSCAN(eps=eps, min_samples=10,n_jobs=15).fit(X)\n",
        "        predicted_labels = model.labels_\n",
        "        try:\n",
        "            tmp_results = ['DBSCAN','eps '+str(eps)]+evaluate(X,Y,predicted_labels)\n",
        "            tmp_results_s = pd.Series(tmp_results, index = results.columns)\n",
        "            tmp_results_df = results_template.copy()\n",
        "            tmp_results_df = tmp_results_df.append(tmp_results_s, ignore_index=True)\n",
        "            results = results.append(tmp_results_s, ignore_index=True)\n",
        "        except:\n",
        "            print('Some error happened, skipping ',fold)\n",
        "        \n",
        "        print('writing the fold results to file')\n",
        "        # if file does not exist write header \n",
        "        try:\n",
        "            if not os.path.isfile(output_file_name):\n",
        "                tmp_results_df.to_csv(output_file_name, header=column_names,index=False)\n",
        "            else: # else it exists so append without writing the header\n",
        "                tmp_results_df.to_csv(output_file_name, mode='a', header=False,index=False)\n",
        "        except:\n",
        "            print('something went wrong and could not write the results to file!\\n',\n",
        "                 'You may abort and see what can be done.\\n',\n",
        "                 'Or wait to see the the final results in memory.')\n",
        "    mean = results.mean(axis=0)\n",
        "    maxx = results.max(axis=0)\n",
        "    print(mean)\n",
        "    print(maxx)\n",
        "\n",
        "    # =============================================================================\n",
        "    # Save to disk\n",
        "    # =============================================================================\n",
        "    # print('Writing to disk...')\n",
        "    results_df = pd.DataFrame(results)\n",
        "    # results_df.to_csv(output_file_name,index=False)\n",
        "    print('Done.')\n",
        "    return results_df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF41GR9GYVHd"
      },
      "source": [
        "## Evaluation of the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3R89kc_YVeJ"
      },
      "source": [
        "import pandas as pd\n",
        "datapath = '/mnt/6016589416586D52/Users/z5204044/GoogleDrive/GoogleDrive/Data/' #C1314\n",
        "# datapath = '/mnt/16A4A9BCA4A99EAD/GoogleDrive/Data/' #Zen\n",
        "data_address =  datapath+\"Corpus/KPRIS/embeddings/deflemm/performance/bert orig-large corpus.csv clustering results\"\n",
        "df = pd.read_csv(data_address)\n",
        "max1 = df.groupby(['Method'], sort=False).max()\n",
        "max2 = df.groupby(['Method']).agg({'NMI': 'max','AMI':'max','ARI':'max'})\n",
        "max3 = df[df.groupby(['Method'])['NMI'].transform(max) == df['NMI']]\n",
        "min3 = df[df.groupby(['Method'])['NMI'].transform(min) == df['NMI']]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}