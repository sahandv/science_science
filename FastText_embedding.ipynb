{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fasttext_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "r_m00tKKkKNG"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahandv/science_science/blob/master/FastText_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B43KQKxsZqK2"
      },
      "source": [
        "# FASTTEXT EMBEDDING\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyakRMAwbUq2"
      },
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8klZfiWyEt98"
      },
      "source": [
        "Local OR Colab?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsEwsoJSEtKx"
      },
      "source": [
        "# datapath = '/mnt/16A4A9BCA4A99EAD/GoogleDrive/Data/' # Local\n",
        "datapath = 'drive/My Drive/Data/' # Remote"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIPuXPla6BKh"
      },
      "source": [
        "### Clone Project Git Repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-YQSyFe6Bgv",
        "outputId": "6da203ff-88ac-4a17-e09b-93ecc4867e1a"
      },
      "source": [
        "!rm -rf 'science_science'\n",
        "username = \"sahandv\"#@param {type:\"string\"}\n",
        "# password = \"\"#@param {type:\"string\"} \n",
        "token = \"\"#@param {type:\"string\"}\n",
        "\n",
        "!git clone https://$username:$token@github.com/$username/science_science.git\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'science_science'...\n",
            "remote: Enumerating objects: 2204, done.\u001b[K\n",
            "remote: Counting objects: 100% (868/868), done.\u001b[K\n",
            "remote: Compressing objects: 100% (741/741), done.\u001b[K\n",
            "remote: Total 2204 (delta 559), reused 420 (delta 120), pack-reused 1336\u001b[K\n",
            "Receiving objects: 100% (2204/2204), 169.01 MiB | 33.58 MiB/s, done.\n",
            "Resolving deltas: 100% (1358/1358), done.\n",
            "Checking out files: 100% (218/218), done.\n",
            "sample_data  science_science\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8crXh0Ek1FA"
      },
      "source": [
        "### Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulliMrpjkKAd",
        "outputId": "74963b77-4704-4ad3-c348-3c60d6add53a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhV7OflrnOhI"
      },
      "source": [
        "# Check files!\n",
        "!ls 'drive/My Drive/Data-Permenant/FastText-crawl-300d-2M-subword'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiBuMInGmypx"
      },
      "source": [
        "### Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OwQMHvQmy3A",
        "outputId": "7aada7ee-917e-4f6c-ea7e-8c22ad946395"
      },
      "source": [
        "!pip install fasttext\n",
        "!pip install -r 'science_science/requirements.txt'\n",
        "# !pip install gensim==3.8.1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20 kB 30.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30 kB 15.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 68 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.7.1-py2.py3-none-any.whl (200 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3094594 sha256=8e0f4c7cecefa956a542af2caa008c7a4e97161ad415bfccaee8edcc43911a81\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.7.1\n",
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow_gpu-2.6.0-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 458.3 MB 9.2 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 2)) (4.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 3)) (4.62.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 4)) (1.1.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 5)) (3.2.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 6)) (2.2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 7)) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 9)) (0.11.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 10)) (1.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 11)) (3.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 12)) (7.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 13)) (0.8.9)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 14)) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 15)) (2.6.3)\n",
            "Collecting netgraph\n",
            "  Downloading netgraph-4.0.5.tar.gz (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: yellowbrick in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 17)) (0.9.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 18)) (4.4.1)\n",
            "Collecting stellargraph\n",
            "  Downloading stellargraph-1.2.1-py3-none-any.whl (435 kB)\n",
            "\u001b[K     |████████████████████████████████| 435 kB 52.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 65.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 21)) (1.3.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 22)) (0.10.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (2.6.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (0.37.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (3.17.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (5.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.40.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (2.6.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements.txt (line 2)) (0.3.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements.txt (line 2)) (0.1.6)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements.txt (line 2)) (21.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements.txt (line 2)) (0.16.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements.txt (line 2)) (5.2.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements.txt (line 2)) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r science_science/requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r science_science/requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (0.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r science_science/requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r science_science/requirements.txt (line 11)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r science_science/requirements.txt (line 11)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r science_science/requirements.txt (line 11)) (2.4.7)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->-r science_science/requirements.txt (line 14)) (5.2.1)\n",
            "Collecting rectangle-packer\n",
            "  Downloading rectangle_packer-2.0.1-cp37-cp37m-manylinux2010_x86_64.whl (246 kB)\n",
            "\u001b[K     |████████████████████████████████| 246 kB 59.9 MB/s \n",
            "\u001b[?25hCollecting grandalf\n",
            "  Downloading grandalf-0.7-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 662 kB/s \n",
            "\u001b[?25hRequirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->-r science_science/requirements.txt (line 18)) (1.3.3)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from stellargraph->-r science_science/requirements.txt (line 19)) (2.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->-r science_science/requirements.txt (line 2)) (1.53.0)\n",
            "Building wheels for collected packages: netgraph\n",
            "  Building wheel for netgraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for netgraph: filename=netgraph-4.0.5-py3-none-any.whl size=72596 sha256=3aa2afa8cf2193527952c50059330adc629561a98cc26b78b679b744d4accf10\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/b1/fc/b83b257e7318cf4b8bde770dc0f5bdd2e8c79a2fcc27bd6f8a\n",
            "Successfully built netgraph\n",
            "Installing collected packages: rectangle-packer, grandalf, tokenizers, tensorflow-gpu, stellargraph, netgraph\n",
            "Successfully installed grandalf-0.7 netgraph-4.0.5 rectangle-packer-2.0.1 stellargraph-1.2.1 tensorflow-gpu-2.6.0 tokenizers-0.10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9l5YuBnkKFB"
      },
      "source": [
        "### Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKZ3hKboHU8y",
        "outputId": "e5d942e4-10f2-41ea-aa88-b0a0b407d029"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data  science_science\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4sdVZl-kKI3"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import json\n",
        "import re\n",
        "\n",
        "# import fasttext\n",
        "import gensim\n",
        "from gensim.models import FastText as fasttext_gensim\n",
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# from sciosci.assets import text_assets as kw\n",
        "# from sciosci.assets import generic_assets as sci\n",
        "# from sciosci.assets import advanced_assets as aa\n",
        "from itertools import chain\n",
        "\n",
        "from science_science.sciosci.assets import text_assets as kw\n",
        "from science_science.sciosci.assets import generic_assets as sci\n",
        "from science_science.sciosci.assets import advanced_assets as aa"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu17lpjOg7F2"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "draRfEpf8A5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b35672-1bef-4374-c762-c33b3d68ffb9"
      },
      "source": [
        "stops = ['a','an','we','result','however','yet','since','previously','although','propose','proposed','e_g','method',\n",
        "         'published_elsevier','b','v','problem','paper','approach','within','with','by','via','way','t','case','issue','level','area','system',\n",
        "         'work','discussed','seen','put','usually','take','make','author','versus','enables','result','research','design','based']\n",
        "punkts = [' ','','(',')','[',']','{','}','.',',','!','?','<','>','-','_',':',';','\\\\','/','|','&','%',\"'s\",\"`s\",'#','$','@']\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = list(set(stopwords.words(\"english\")))+stops+punkts\n",
        "np.random.seed(50)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLK4cDhVkKbs"
      },
      "source": [
        "# Get embeddings from a pre-trained model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhKPivNafWBf"
      },
      "source": [
        "### Load Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQEzlLSQ6g0E"
      },
      "source": [
        "period = '2017-2018'\n",
        "percentile = 97"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6XYOAVzkKsR"
      },
      "source": [
        "#### Option A - Load corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkWEF6DBkKzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c9a6ffe-b5d8-4d62-f67c-0ba24e6a33bc"
      },
      "source": [
        "# directory = datapath+'Corpus/cora-classify/cora/clean/with citations/'\n",
        "# directory = datapath+'Corpus/Dimensions AI unlimited citations/clean/'\n",
        "directory = datapath+'Corpus/Dimensions All/clean/'\n",
        "\n",
        "# file_name = 'cora deflemm'#corpus abstract-title - with n-grams'\n",
        "file_name = 'data with abstract'#corpus abstract-title - with n-grams'\n",
        "corpus = pd.read_csv(directory+file_name)\n",
        "# keywords = [item.lower().replace(';;;',' ') for sublist in corpus['DE-n'].values.tolist() for item in sublist.split()]\n",
        "keywords = [item.lower().split(';;;') for item in tqdm(corpus['DE-n'].values.tolist())]\n",
        "ids = corpus['id']\n",
        "gc.collect()\n",
        "keywords_flat = list(chain.from_iterable(keywords))\n",
        "keywords_unique = list(set(keywords_flat))\n",
        "len(keywords_unique)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 566309/566309 [00:08<00:00, 69678.47it/s] \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2415657"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG1-O4D26irL"
      },
      "source": [
        "#### Option B - Load keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7qJSz6l6mVt"
      },
      "source": [
        "directory = datapath+'LDA/'\n",
        "file_name = period+' top_90-percentile_keywords_terms.csv'\n",
        "corpus = pd.read_csv(directory+file_name)\n",
        "corpus = corpus.fillna('this_is_null')\n",
        "corpus_tokens = []\n",
        "for idx,row in tqdm(corpus.iterrows(),total=corpus.shape[0]):\n",
        "    for token in row.values.tolist():\n",
        "        if token != 'this_is_null': \n",
        "            corpus_tokens.append(token) \n",
        "del corpus\n",
        "print(\"\\nNumber of unique tokens:\",len(corpus_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YXZPTZrKjhr"
      },
      "source": [
        "#### Option C - Load author keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq2HreIeKqP0"
      },
      "source": [
        "directory = datapath+'Author keywords - 29 Oct 2019/'\n",
        "file_name = period+' keyword frequency'\n",
        "corpus = pd.read_csv(directory+file_name,names=['keyword','frequency'])\n",
        "corpus = corpus.fillna('this_is_null')\n",
        "threshold = np.percentile(corpus['frequency'].values,percentile)\n",
        "corpus = corpus[corpus['frequency']>threshold]\n",
        "\n",
        "corpus_tokens = []\n",
        "for idx,row in tqdm(corpus.iterrows(),total=corpus.shape[0]):\n",
        "    if row['keyword'] != 'this_is_null': \n",
        "        corpus_tokens.append(row['keyword']) \n",
        "print(\"\\nNumber of unique tokens:\",len(corpus_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6UjbqehEOFA"
      },
      "source": [
        "#### Option D - Load author keywords and lemmatze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJvpKLFvENUj"
      },
      "source": [
        "file_name = 'Corpus/Taxonomy/CSO.3.3-with-labels-US-lem.csv'\n",
        "corpus = pd.read_csv(datapath+file_name)#,names=['keyword','frequency'])\n",
        "corpus = corpus.a.values.tolist()+corpus.b.values.tolist()\n",
        "keywords = list(set(corpus))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXqmug-0Ww4x",
        "outputId": "b78e1e6f-403b-4fbc-9670-a4740fcc82fa"
      },
      "source": [
        "print(len(keywords),len(corpus))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12351 242150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqlZHLo1e40r"
      },
      "source": [
        "## Facebook Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeC8SkXPkKjx"
      },
      "source": [
        "#### Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjSQ5qVnS7CQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f814d8ad-57f4-459a-d587-56e1b1c21459"
      },
      "source": [
        "!wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz'\n",
        "# https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.vec.zip"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-26 16:12:28--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4503593528 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.en.300.bin.gz’\n",
            "\n",
            "cc.en.300.bin.gz    100%[===================>]   4.19G  11.9MB/s    in 6m 3s   \n",
            "\n",
            "2021-09-26 16:18:32 (11.8 MB/s) - ‘cc.en.300.bin.gz’ saved [4503593528/4503593528]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klvGhIVlTK37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c27d58-1e89-4c35-d65e-dcce1b17eb74"
      },
      "source": [
        "!gunzip 'cc.en.300.bin.gz'\n",
        "!ls"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cc.en.300.bin  drive  sample_data  science_science\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv3ACHBtexEI"
      },
      "source": [
        "fb_model_address = datapath+'FastText-crawl-300d-2M-subword/crawl-300d-2M-subword.bin'"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv8rOhd7AHkE",
        "outputId": "1ac942d0-c03e-4762-b4be-20d8909df817"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cc.en.300.bin  drive  sample_data  science_science\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6K74cHMkKnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a35e39ba-034b-4c58-967f-20179b578b77"
      },
      "source": [
        "!pip install fasttext\n",
        "import fasttext\n",
        "model = fasttext.load_model('cc.en.300.bin')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.7/dist-packages (0.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.7.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2648m7e8kk0n"
      },
      "source": [
        "#### Get embeddings\n",
        "\n",
        "*   Save to dictionary and json\n",
        "*   This takes much less space on disk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mq87DiZxVvY"
      },
      "source": [
        "##### No n-gram handle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiJcbZqHkkLl"
      },
      "source": [
        "# Save in a dict\n",
        "output_dict = {}\n",
        "comment_embedding = ''\n",
        "for token in tqdm(corpus_tokens[:],total=len(corpus_tokens[:])):\n",
        "    output_dict[token] = str(model.get_word_vector(token))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOMUto5AxVDy"
      },
      "source": [
        "##### Manual n-gram handle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4UJAF3exSLm"
      },
      "source": [
        "# Save in a dict\n",
        "comment_embedding = 'average_manual '\n",
        "output_dict = {}\n",
        "for token in tqdm(corpus_tokens[:],total=len(corpus_tokens[:])):\n",
        "    token_split = token.split(' ')\n",
        "    if len(token_split) > 0:\n",
        "        tmp_vector_grams = []\n",
        "        for item in token_split:\n",
        "            tmp_vector_grams.append(model.get_word_vector(item))\n",
        "        output_dict[token] = str(list(np.array(tmp_vector_grams).mean(axis=0)))\n",
        "    else:\n",
        "        output_dict[token] = str(model.get_word_vector(item))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8nMUTU0xXo8"
      },
      "source": [
        "##### Save to disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJk-lqjemSky"
      },
      "source": [
        "# Save embeddings to disk\n",
        "with open(directory+'vectors/100D/FastText vector '+comment_embedding+period+'.json', 'w') as json_file:\n",
        "    json.dump(output_dict, json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXkwEnj4lnJi"
      },
      "source": [
        "#### Get embeddings (alternative) : \n",
        "\n",
        "*   save to a list instead of a dicktionary and csv\n",
        "*   Will have many redundant words in it and will take lots of disk space\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GndJ3Q1OlnSy"
      },
      "source": [
        "# Save in a list\n",
        "batches = 1000\n",
        "batch_size = len(corpus_tokens)/batches\n",
        "\n",
        "for step in tqdm(range(batches),total=batches):\n",
        "    batch_tokens = corpus_tokens[int(step*batch_size):int((step+1)*batch_size)]\n",
        "    corpus_vectors = [model.get_word_vector(x) for x in batch_tokens]\n",
        "    corpus_vectors = pd.DataFrame(corpus_vectors)\n",
        "    corpus_vectors['tokens'] = batch_tokens\n",
        "\n",
        "    # Save embeddings to disk\n",
        "    with open(directory+'vector '+period,'a') as f:\n",
        "        corpus_vectors.to_csv(f,index=False,header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Xcww-Mfdaj"
      },
      "source": [
        "## Gensim Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPd9g4E4fgTi"
      },
      "source": [
        "#### Load model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06depR4llpmB"
      },
      "source": [
        "Load gensim model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_UYlrcCfgxw"
      },
      "source": [
        "gensim_model_address = datapath+'Corpus/Dimensions All/models/Fasttext/FastText100D-dim-scopus-update.model'\n",
        "model_AI = fasttext_gensim.load(gensim_model_address)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx1x_twuQCbQ"
      },
      "source": [
        "gensim_model_address = datapath+\"Corpus/cora-classify/cora/models/fasttext cora corpus 128d5w.model\"\n",
        "model = fasttext_gensim.load(gensim_model_address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhKlmoX6gPE-"
      },
      "source": [
        "Simple Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7oeEkj_gNLg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "980c009f-f3ae-4a8f-d09d-60540aa49b89"
      },
      "source": [
        "print('intelligence' in model.wv.vocab)\n",
        "print(model_AI.similarity(\"anns\", \"ann\"))\n",
        "print(model.most_similar(['eye','vision','processing'], ['computer']))\n",
        "print(model.wmdistance(['stop', 'word', 'removed', 'tokens', 'of', 'sentence 1'], ['stop word removed tokens of sentence 2']))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "0.7752938\n",
            "[('vision.This', 0.5317180156707764), ('acuity', 0.5243748426437378), ('vison', 0.5201269388198853), ('vision.The', 0.5188624262809753), ('vision.It', 0.5166184902191162), ('vision.What', 0.5062639713287354), ('vision-', 0.5017036199569702), ('vision.A', 0.4983903169631958), ('binocularity', 0.4980439841747284), ('eye-', 0.4922565221786499)]\n",
            "1.2832011783002162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Gdwsz_glA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38acf839-8862-4cba-fda0-90b62f255e32"
      },
      "source": [
        "print(model.most_similar(['math','vector','statistic'], ['psychology']))\n",
        "print(model_AI.most_similar(['math','vector','statistic'], ['psychology']))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('dot-product', 0.5507299900054932), ('vectors', 0.5373679399490356), ('arithmetic', 0.506456732749939), ('arithmatic', 0.5047929883003235), ('scatterplot', 0.49137625098228455), ('calculation', 0.4910370111465454), ('log-scale', 0.48583289980888367), ('real-number', 0.4845273494720459), ('non-vector', 0.48128536343574524), ('calulator', 0.4809798002243042)]\n",
            "[('vectorizer', 0.727868914604187), ('vectorized', 0.6785148978233337), ('ldf', 0.6675050854682922), ('subvector', 0.6655511856079102), ('vectored', 0.6642667055130005), ('vectorial', 0.6612221002578735), ('vectorize', 0.6548691987991333), ('eigenvector', 0.6539524793624878), ('bitvector', 0.6513363122940063), ('rametrix', 0.6496775150299072)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCuEFwIMbND_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca5a453-5b07-4497-c4aa-5df1643931e7"
      },
      "source": [
        "# get distance of two words\n",
        "from scipy import spatial,sparse,sign\n",
        "vec_a = np.array([model_AI.wv['car'],model_AI.wv['wash']]).mean(axis=0)\n",
        "vec_b = np.array([model_AI.wv['data'],model_AI.wv['science']]).mean(axis=0)#(model_AI['data']+model_AI['science'])\n",
        "distance_tmp = spatial.distance.cosine(vec_a, vec_b)\n",
        "similarity_tmp = 1 - distance_tmp\n",
        "similarity_tmp\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.10761483013629913"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG_E-eFDEqNV",
        "outputId": "708fd87c-7203-4af8-fda8-33d8a9c11c4e"
      },
      "source": [
        "# get distance of two words\n",
        "from scipy import spatial,sparse,sign\n",
        "vec_a = np.array([model['car'],model['wash']]).mean(axis=0)\n",
        "vec_b = (model['data']+model['science'])\n",
        "distance_tmp = spatial.distance.cosine(vec_a, vec_b)\n",
        "similarity_tmp = 1 - distance_tmp\n",
        "similarity_tmp\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.08714380115270615"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5516KiWbelyF"
      },
      "source": [
        "#### Compare vectors to ACM categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AxNRKUidrCk"
      },
      "source": [
        "AI_categories = [\n",
        "              'Artificial Intelligence Applications - Expert Systems',\n",
        "              'Automatic Programming',\n",
        "              'Deduction - Theorem Proving',\n",
        "              'Knowledge Representation Formalisms - Knowledge Representation Methods',\n",
        "              'Programming Languages - Software',\n",
        "              'Learning',\n",
        "              'Natural Language Processing',\n",
        "              'Problem Solving - Control Methods - Search',\n",
        "              'Robotics',\n",
        "              'Vision - Scene Understanding',\n",
        "              'Distributed Artificial Intelligence',\n",
        "              'ARTIFICIAL INTELLIGENCE'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QeWKP-k1qpy"
      },
      "source": [
        "categories = [\n",
        "              'Natural language processing - Information extraction - Machine translation - Discourse, dialogue  pragmatics - Natural language generation - Speech recognition - Lexical semantics - Phonology / morphology',\n",
        "              'Knowledge representation reasoning - Description logics - Semantic networks Nonmonotonic default reasoning  belief revision - Probabilistic reasoning - Vagueness fuzzy logic - Causal reasoning  diagnostics - Temporal reasoning - Cognitive robotics - Ontology engineering - Logic programming answer set programming - Spatial  physical reasoning - Reasoning about belief  knowledge',\n",
        "              'Planning  scheduling - Planning for deterministic actions - Planning under uncertainty - Multi-agent planning - Planning  abstraction  generalization - Robotic planning - Evolutionary robotics',\n",
        "              'Search methodologies - Heuristic function construction - Discrete space search - Continuous space search - Randomized search - Game tree search - Abstraction  micro-operators - Search with partial observations - ',\n",
        "              'Control methods - Robotic planning - Evolutionary robotics - Computational control theory - Motion path planning',\n",
        "              'Philosophical theoretical foundations artificial intelligence - Cognitive science - Theory mind',\n",
        "              'Distributed artificial intelligence - Multi agent systems - Intelligent agents - Mobile agents - Cooperation  coordination',\n",
        "              'Computer vision - Biometrics - Scene understanding - Activity recognition  understanding - Video summarization - Visual content based indexing  retrieval - Visual inspection - Vision for robotics - Scene anomaly detection - Image  video acquisition - Camera calibration - Epipolar geometry - Computational photography - Hyperspectral imaging - Motion capture - 3D imaging - Active vision - Image representations - Shape representations - Appearance  texture - Hierarchical representations - Computer vision problems - Interest point  salient region detections - Image segmentation  - Video segmentation - Shape inference - Object detection - Object recognition - Object identification - Tracking - Reconstruction - Matching',\n",
        "              \n",
        "              'Learning paradigms - Supervised learning - Ranking - Learning to rank - Supervised learning  classification - Supervised learning  regression - Structured outputs - Cost sensitive learning - Unsupervised learning - Cluster analysis - Anomaly detection - Mixture modeling - Topic modeling - Source separation - Motif discovery - Dimensionality reduction  manifold learning - Reinforcement learning - Sequential decision making - Inverse reinforcement learning - Apprenticeship learning - Multi-agent reinforcement learning - Adversarial learning - Multi-task learning - Transfer learning - Lifelong machine learning - Learning under covariate shift',\n",
        "              'Learning settings - Batch learning - Online learning settings - Learning from demonstrations - Learning from critiques - Learning from implicit feedback - Active learning settings - Semi supervised learning settings',\n",
        "              'Machine learning approaches - Classification  regression trees - Kernel methods - Support vector machines - Gaussian processes - Neural networks - Logical  relational learning - Inductive logic learning - Statistical relational learning - Learning in probabilistic graphical models - Maximum likelihood modeling - Maximum entropy modeling - Maximum a posteriori modeling - Mixture models - Latent variable models - Bayesian network models - Learning linear models - Perceptron algorithm - Factorization methods - Non-negative matrix factorization - Factor analysis - Principal component analysis - Canonical correlation analysis - Latent Dirichlet allocation - Rule learning - Instance-based learning - Markov decision processes -  Markov decision processes - Stochastic games - Learning latent representations - Deep belief networks - Bio inspired approaches - Artificial life - Evolvable hardware - Genetic algorithms - Genetic programming - Evolutionary robotics - Generative  developmental approaches',\n",
        "              'Machine learning algorithms - Dynamic programming Markov decision processes - Value iteration - Q learning - Policy iteration - Temporal difference learning - Approximate dynamic programming methods - Ensemble methods - Boosting - Bagging - Spectral methods - Feature selection - Regularization',\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rtaQ7KbfQ_y"
      },
      "source": [
        "AI_vectors = []\n",
        "labels = []\n",
        "for item in categories:\n",
        "    vector_tmp = []\n",
        "    label = item.split('-')[0]\n",
        "    for phrase in item.split('-'):\n",
        "        phrase = phrase.lower().strip()\n",
        "        # print(phrase)\n",
        "        vector_tmp.append(model.wv[phrase])\n",
        "    # print('---')\n",
        "    AI_vectors.append(list(np.array(vector_tmp).mean(axis=0)))\n",
        "    labels.append(label)\n",
        "print(AI_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brCodssfgEgt"
      },
      "source": [
        "pd.DataFrame(AI_vectors).to_csv(datapath+'FastText doc clusters - SIP/50D/classification/ACM_classifications_vectors')\n",
        "pd.DataFrame(labels,columns=['label']).to_csv(datapath+'FastText doc clusters - SIP/50D/classification/ACM_classifications_labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMAHWCgX6kEz"
      },
      "source": [
        "### Get Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LPLO5z4cER3"
      },
      "source": [
        "##### No n-gram handle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP8pYSjf6kiR"
      },
      "source": [
        "# Save in a dict\n",
        "comment_embedding = ''\n",
        "output_dict = {}\n",
        "for token in tqdm(corpus_tokens[:],total=len(corpus_tokens[:])):\n",
        "    output_dict[token] = str(model.wv[token])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGLF7jepcI83"
      },
      "source": [
        "##### Manual n-gram handle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8OC3uOovyM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb95059-eeda-4f49-a829-510b18d537fd"
      },
      "source": [
        "# Save in a dict\n",
        "comment_embedding = 'average_manual '\n",
        "sentence_corpus = pd.read_csv(datapath+\"Corpus/Taxonomy/AI kw mergedAI kw merged preprocessed for similarity measure\")\n",
        "corpus_tokens = sentence_corpus[pd.notna(sentence_corpus['sentence'])]['sentence'].values.tolist()\n",
        "output_dict = {}\n",
        "for token in tqdm(corpus_tokens,total=len(corpus_tokens)):\n",
        "    token_split = token.split(' ')\n",
        "    if len(token_split) > 0:\n",
        "        tmp_vector_grams = []\n",
        "        for item in token_split:\n",
        "            tmp_vector_grams.append(model.wv[item])\n",
        "        output_dict[token] = str(list(np.array(tmp_vector_grams).mean(axis=0)))\n",
        "    else:\n",
        "        output_dict[token] = str(model.wv[token])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971871/971871 [01:50<00:00, 8789.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq52ru_cKBuK"
      },
      "source": [
        "##### Manual n-gram handle for keywords - for Option D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8kzOvxDJ_Fr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "0160a4fb-ab12-4e4c-966f-e72b289cd453"
      },
      "source": [
        "vectors = []\n",
        "for keyword in tqdm(keywords_unique,total=len(keywords_unique)):\n",
        "    tokens = keyword.split(' ')\n",
        "    keyword_vec = []\n",
        "    if len(tokens)>1:\n",
        "        for token in tokens:\n",
        "            if len(token)>1:\n",
        "                keyword_vec.append(model_AI.wv['neural'])\n",
        "        if len(keyword_vec)>0:\n",
        "            keyword_vec = np.array(keyword_vec).mean(axis=0)\n",
        "        else:\n",
        "            keyword_vec = np.zeros(100)\n",
        "            keyword_vec[:] = np.nan\n",
        "    else:\n",
        "        keyword_vec = model_AI.wv['neural']\n",
        "    vectors.append(keyword_vec)\n",
        "vectors = pd.DataFrame(vectors)\n",
        "vectors"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2415657/2415657 [00:50<00:00, 48050.29it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-4.497859</td>\n",
              "      <td>-9.942174</td>\n",
              "      <td>3.103759</td>\n",
              "      <td>-1.652349</td>\n",
              "      <td>-2.584312</td>\n",
              "      <td>-1.776602</td>\n",
              "      <td>-3.870975</td>\n",
              "      <td>4.701572</td>\n",
              "      <td>-1.922743</td>\n",
              "      <td>0.782408</td>\n",
              "      <td>6.416571</td>\n",
              "      <td>-7.64352</td>\n",
              "      <td>3.64921</td>\n",
              "      <td>-7.158574</td>\n",
              "      <td>-0.545283</td>\n",
              "      <td>1.014253</td>\n",
              "      <td>-0.39179</td>\n",
              "      <td>5.457388</td>\n",
              "      <td>-6.126346</td>\n",
              "      <td>1.019077</td>\n",
              "      <td>-0.865525</td>\n",
              "      <td>1.340499</td>\n",
              "      <td>1.486618</td>\n",
              "      <td>3.259309</td>\n",
              "      <td>-5.427164</td>\n",
              "      <td>0.570631</td>\n",
              "      <td>3.151731</td>\n",
              "      <td>8.084656</td>\n",
              "      <td>6.90467</td>\n",
              "      <td>-3.016809</td>\n",
              "      <td>1.005586</td>\n",
              "      <td>-3.242914</td>\n",
              "      <td>-1.807679</td>\n",
              "      <td>-2.873847</td>\n",
              "      <td>-7.858417</td>\n",
              "      <td>4.226807</td>\n",
              "      <td>11.046021</td>\n",
              "      <td>7.15528</td>\n",
              "      <td>5.37511</td>\n",
              "      <td>1.274791</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.95264</td>\n",
              "      <td>4.344461</td>\n",
              "      <td>6.376086</td>\n",
              "      <td>2.445218</td>\n",
              "      <td>8.955554</td>\n",
              "      <td>4.103342</td>\n",
              "      <td>-2.638663</td>\n",
              "      <td>-1.186178</td>\n",
              "      <td>3.001384</td>\n",
              "      <td>-7.051963</td>\n",
              "      <td>6.906458</td>\n",
              "      <td>-3.431726</td>\n",
              "      <td>-0.519207</td>\n",
              "      <td>-8.974524</td>\n",
              "      <td>-0.807257</td>\n",
              "      <td>-0.456233</td>\n",
              "      <td>-5.885994</td>\n",
              "      <td>-4.973614</td>\n",
              "      <td>4.490455</td>\n",
              "      <td>-2.57421</td>\n",
              "      <td>0.339056</td>\n",
              "      <td>1.826858</td>\n",
              "      <td>-8.489288</td>\n",
              "      <td>5.520897</td>\n",
              "      <td>5.356351</td>\n",
              "      <td>-6.357804</td>\n",
              "      <td>-1.609869</td>\n",
              "      <td>3.560576</td>\n",
              "      <td>-5.086923</td>\n",
              "      <td>4.107792</td>\n",
              "      <td>-5.0108</td>\n",
              "      <td>-7.981196</td>\n",
              "      <td>-5.111307</td>\n",
              "      <td>6.368404</td>\n",
              "      <td>-3.654854</td>\n",
              "      <td>-0.052845</td>\n",
              "      <td>-5.485974</td>\n",
              "      <td>-8.187818</td>\n",
              "      <td>-4.258795</td>\n",
              "      <td>6.62896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-4.497859</td>\n",
              "      <td>-9.942174</td>\n",
              "      <td>3.103759</td>\n",
              "      <td>-1.652349</td>\n",
              "      <td>-2.584312</td>\n",
              "      <td>-1.776602</td>\n",
              "      <td>-3.870975</td>\n",
              "      <td>4.701572</td>\n",
              "      <td>-1.922743</td>\n",
              "      <td>0.782408</td>\n",
              "      <td>6.416571</td>\n",
              "      <td>-7.64352</td>\n",
              "      <td>3.64921</td>\n",
              "      <td>-7.158574</td>\n",
              "      <td>-0.545283</td>\n",
              "      <td>1.014253</td>\n",
              "      <td>-0.39179</td>\n",
              "      <td>5.457388</td>\n",
              "      <td>-6.126346</td>\n",
              "      <td>1.019077</td>\n",
              "      <td>-0.865525</td>\n",
              "      <td>1.340499</td>\n",
              "      <td>1.486618</td>\n",
              "      <td>3.259309</td>\n",
              "      <td>-5.427164</td>\n",
              "      <td>0.570631</td>\n",
              "      <td>3.151731</td>\n",
              "      <td>8.084656</td>\n",
              "      <td>6.90467</td>\n",
              "      <td>-3.016809</td>\n",
              "      <td>1.005586</td>\n",
              "      <td>-3.242914</td>\n",
              "      <td>-1.807679</td>\n",
              "      <td>-2.873847</td>\n",
              "      <td>-7.858417</td>\n",
              "      <td>4.226807</td>\n",
              "      <td>11.046021</td>\n",
              "      <td>7.15528</td>\n",
              "      <td>5.37511</td>\n",
              "      <td>1.274791</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.95264</td>\n",
              "      <td>4.344461</td>\n",
              "      <td>6.376086</td>\n",
              "      <td>2.445218</td>\n",
              "      <td>8.955554</td>\n",
              "      <td>4.103342</td>\n",
              "      <td>-2.638663</td>\n",
              "      <td>-1.186178</td>\n",
              "      <td>3.001384</td>\n",
              "      <td>-7.051963</td>\n",
              "      <td>6.906458</td>\n",
              "      <td>-3.431726</td>\n",
              "      <td>-0.519207</td>\n",
              "      <td>-8.974524</td>\n",
              "      <td>-0.807257</td>\n",
              "      <td>-0.456233</td>\n",
              "      <td>-5.885994</td>\n",
              "      <td>-4.973614</td>\n",
              "      <td>4.490455</td>\n",
              "      <td>-2.57421</td>\n",
              "      <td>0.339056</td>\n",
              "      <td>1.826858</td>\n",
              "      <td>-8.489288</td>\n",
              "      <td>5.520897</td>\n",
              "      <td>5.356351</td>\n",
              "      <td>-6.357804</td>\n",
              "      <td>-1.609869</td>\n",
              "      <td>3.560576</td>\n",
              "      <td>-5.086923</td>\n",
              "      <td>4.107792</td>\n",
              "      <td>-5.0108</td>\n",
              "      <td>-7.981196</td>\n",
              "      <td>-5.111307</td>\n",
              "      <td>6.368404</td>\n",
              "      <td>-3.654854</td>\n",
              "      <td>-0.052845</td>\n",
              "      <td>-5.485974</td>\n",
              "      <td>-8.187818</td>\n",
              "      <td>-4.258795</td>\n",
              "      <td>6.62896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-4.497859</td>\n",
              "      <td>-9.942174</td>\n",
              "      <td>3.103759</td>\n",
              "      <td>-1.652349</td>\n",
              "      <td>-2.584312</td>\n",
              "      <td>-1.776602</td>\n",
              "      <td>-3.870975</td>\n",
              "      <td>4.701572</td>\n",
              "      <td>-1.922743</td>\n",
              "      <td>0.782408</td>\n",
              "      <td>6.416571</td>\n",
              "      <td>-7.64352</td>\n",
              "      <td>3.64921</td>\n",
              "      <td>-7.158574</td>\n",
              "      <td>-0.545283</td>\n",
              "      <td>1.014253</td>\n",
              "      <td>-0.39179</td>\n",
              "      <td>5.457388</td>\n",
              "      <td>-6.126346</td>\n",
              "      <td>1.019077</td>\n",
              "      <td>-0.865525</td>\n",
              "      <td>1.340499</td>\n",
              "      <td>1.486618</td>\n",
              "      <td>3.259309</td>\n",
              "      <td>-5.427164</td>\n",
              "      <td>0.570631</td>\n",
              "      <td>3.151731</td>\n",
              "      <td>8.084656</td>\n",
              "      <td>6.90467</td>\n",
              "      <td>-3.016809</td>\n",
              "      <td>1.005586</td>\n",
              "      <td>-3.242914</td>\n",
              "      <td>-1.807679</td>\n",
              "      <td>-2.873847</td>\n",
              "      <td>-7.858417</td>\n",
              "      <td>4.226807</td>\n",
              "      <td>11.046021</td>\n",
              "      <td>7.15528</td>\n",
              "      <td>5.37511</td>\n",
              "      <td>1.274791</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.95264</td>\n",
              "      <td>4.344461</td>\n",
              "      <td>6.376086</td>\n",
              "      <td>2.445218</td>\n",
              "      <td>8.955554</td>\n",
              "      <td>4.103342</td>\n",
              "      <td>-2.638663</td>\n",
              "      <td>-1.186178</td>\n",
              "      <td>3.001384</td>\n",
              "      <td>-7.051963</td>\n",
              "      <td>6.906458</td>\n",
              "      <td>-3.431726</td>\n",
              "      <td>-0.519207</td>\n",
              "      <td>-8.974524</td>\n",
              "      <td>-0.807257</td>\n",
              "      <td>-0.456233</td>\n",
              "      <td>-5.885994</td>\n",
              "      <td>-4.973614</td>\n",
              "      <td>4.490455</td>\n",
              "      <td>-2.57421</td>\n",
              "      <td>0.339056</td>\n",
              "      <td>1.826858</td>\n",
              "      <td>-8.489288</td>\n",
              "      <td>5.520897</td>\n",
              "      <td>5.356351</td>\n",
              "      <td>-6.357804</td>\n",
              "      <td>-1.609869</td>\n",
              "      <td>3.560576</td>\n",
              "      <td>-5.086923</td>\n",
              "      <td>4.107792</td>\n",
              "      <td>-5.0108</td>\n",
              "      <td>-7.981196</td>\n",
              "      <td>-5.111307</td>\n",
              "      <td>6.368404</td>\n",
              "      <td>-3.654854</td>\n",
              "      <td>-0.052845</td>\n",
              "      <td>-5.485974</td>\n",
              "      <td>-8.187818</td>\n",
              "      <td>-4.258795</td>\n",
              "      <td>6.62896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-4.497859</td>\n",
              "      <td>-9.942174</td>\n",
              "      <td>3.103759</td>\n",
              "      <td>-1.652349</td>\n",
              "      <td>-2.584312</td>\n",
              "      <td>-1.776602</td>\n",
              "      <td>-3.870975</td>\n",
              "      <td>4.701572</td>\n",
              "      <td>-1.922743</td>\n",
              "      <td>0.782408</td>\n",
              "      <td>6.416571</td>\n",
              "      <td>-7.64352</td>\n",
              "      <td>3.64921</td>\n",
              "      <td>-7.158574</td>\n",
              "      <td>-0.545283</td>\n",
              "      <td>1.014253</td>\n",
              "      <td>-0.39179</td>\n",
              "      <td>5.457388</td>\n",
              "      <td>-6.126346</td>\n",
              "      <td>1.019077</td>\n",
              "      <td>-0.865525</td>\n",
              "      <td>1.340499</td>\n",
              "      <td>1.486618</td>\n",
              "      <td>3.259309</td>\n",
              "      <td>-5.427164</td>\n",
              "      <td>0.570631</td>\n",
              "      <td>3.151731</td>\n",
              "      <td>8.084656</td>\n",
              "      <td>6.90467</td>\n",
              "      <td>-3.016809</td>\n",
              "      <td>1.005586</td>\n",
              "      <td>-3.242914</td>\n",
              "      <td>-1.807679</td>\n",
              "      <td>-2.873847</td>\n",
              "      <td>-7.858417</td>\n",
              "      <td>4.226807</td>\n",
              "      <td>11.046021</td>\n",
              "      <td>7.15528</td>\n",
              "      <td>5.37511</td>\n",
              "      <td>1.274791</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.95264</td>\n",
              "      <td>4.344461</td>\n",
              "      <td>6.376086</td>\n",
              "      <td>2.445218</td>\n",
              "      <td>8.955554</td>\n",
              "      <td>4.103342</td>\n",
              "      <td>-2.638663</td>\n",
              "      <td>-1.186178</td>\n",
              "      <td>3.001384</td>\n",
              "      <td>-7.051963</td>\n",
              "      <td>6.906458</td>\n",
              "      <td>-3.431726</td>\n",
              "      <td>-0.519207</td>\n",
              "      <td>-8.974524</td>\n",
              "      <td>-0.807257</td>\n",
              "      <td>-0.456233</td>\n",
              "      <td>-5.885994</td>\n",
              "      <td>-4.973614</td>\n",
              "      <td>4.490455</td>\n",
              "      <td>-2.57421</td>\n",
              "      <td>0.339056</td>\n",
              "      <td>1.826858</td>\n",
              "      <td>-8.489288</td>\n",
              "      <td>5.520897</td>\n",
              "      <td>5.356351</td>\n",
              "      <td>-6.357804</td>\n",
              "      <td>-1.609869</td>\n",
              "      <td>3.560576</td>\n",
              "      <td>-5.086923</td>\n",
              "      <td>4.107792</td>\n",
              "      <td>-5.0108</td>\n",
              "      <td>-7.981196</td>\n",
              "      <td>-5.111307</td>\n",
              "      <td>6.368404</td>\n",
              "      <td>-3.654854</td>\n",
              "      <td>-0.052845</td>\n",
              "      <td>-5.485974</td>\n",
              "      <td>-8.187818</td>\n",
              "      <td>-4.258795</td>\n",
              "      <td>6.62896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-4.497859</td>\n",
              "      <td>-9.942174</td>\n",
              "      <td>3.103759</td>\n",
              "      <td>-1.652349</td>\n",
              "      <td>-2.584312</td>\n",
              "      <td>-1.776602</td>\n",
              "      <td>-3.870975</td>\n",
              "      <td>4.701572</td>\n",
              "      <td>-1.922743</td>\n",
              "      <td>0.782408</td>\n",
              "      <td>6.416571</td>\n",
              "      <td>-7.64352</td>\n",
              "      <td>3.64921</td>\n",
              "      <td>-7.158574</td>\n",
              "      <td>-0.545283</td>\n",
              "      <td>1.014253</td>\n",
              "      <td>-0.39179</td>\n",
              "      <td>5.457388</td>\n",
              "      <td>-6.126345</td>\n",
              "      <td>1.019077</td>\n",
              "      <td>-0.865525</td>\n",
              "      <td>1.340499</td>\n",
              "      <td>1.486618</td>\n",
              "      <td>3.259309</td>\n",
              "      <td>-5.427164</td>\n",
              "      <td>0.570631</td>\n",
              "      <td>3.151731</td>\n",
              "      <td>8.084656</td>\n",
              "      <td>6.90467</td>\n",
              "      <td>-3.016809</td>\n",
              "      <td>1.005586</td>\n",
              "      <td>-3.242914</td>\n",
              "      <td>-1.807679</td>\n",
              "      <td>-2.873847</td>\n",
              "      <td>-7.858416</td>\n",
              "      <td>4.226807</td>\n",
              "      <td>11.046021</td>\n",
              "      <td>7.15528</td>\n",
              "      <td>5.37511</td>\n",
              "      <td>1.274791</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.95264</td>\n",
              "      <td>4.344461</td>\n",
              "      <td>6.376086</td>\n",
              "      <td>2.445218</td>\n",
              "      <td>8.955554</td>\n",
              "      <td>4.103342</td>\n",
              "      <td>-2.638663</td>\n",
              "      <td>-1.186178</td>\n",
              "      <td>3.001384</td>\n",
              "      <td>-7.051963</td>\n",
              "      <td>6.906458</td>\n",
              "      <td>-3.431726</td>\n",
              "      <td>-0.519207</td>\n",
              "      <td>-8.974524</td>\n",
              "      <td>-0.807257</td>\n",
              "      <td>-0.456233</td>\n",
              "      <td>-5.885994</td>\n",
              "      <td>-4.973614</td>\n",
              "      <td>4.490455</td>\n",
              "      <td>-2.57421</td>\n",
              "      <td>0.339056</td>\n",
              "      <td>1.826858</td>\n",
              "      <td>-8.489288</td>\n",
              "      <td>5.520897</td>\n",
              "      <td>5.356351</td>\n",
              "      <td>-6.357804</td>\n",
              "      <td>-1.609869</td>\n",
              "      <td>3.560576</td>\n",
              "      <td>-5.086923</td>\n",
              "      <td>4.107792</td>\n",
              "      <td>-5.0108</td>\n",
              "      <td>-7.981196</td>\n",
              "      <td>-5.111307</td>\n",
              "      <td>6.368404</td>\n",
              "      <td>-3.654854</td>\n",
              "      <td>-0.052845</td>\n",
              "      <td>-5.485975</td>\n",
              "      <td>-8.187818</td>\n",
              "      <td>-4.258795</td>\n",
              "      <td>6.62896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2415652</th>\n",
              "      <td>-4.497859</td>\n",
              "      <td>-9.942174</td>\n",
              "      <td>3.103759</td>\n",
              "      <td>-1.652349</td>\n",
              "      <td>-2.584312</td>\n",
              "      <td>-1.776602</td>\n",
              "      <td>-3.870975</td>\n",
              "      <td>4.701572</td>\n",
              "      <td>-1.922743</td>\n",
              "      <td>0.782408</td>\n",
              "      <td>6.416571</td>\n",
              "      <td>-7.64352</td>\n",
              "      <td>3.64921</td>\n",
              "      <td>-7.158574</td>\n",
              "      <td>-0.545283</td>\n",
              "      <td>1.014253</td>\n",
              "      <td>-0.39179</td>\n",
              "      <td>5.457388</td>\n",
              "      <td>-6.126346</td>\n",
              "      <td>1.019077</td>\n",
              "      <td>-0.865525</td>\n",
              "      <td>1.340499</td>\n",
              "      <td>1.486618</td>\n",
              "      <td>3.259309</td>\n",
              "      <td>-5.427164</td>\n",
              "      <td>0.570631</td>\n",
              "      <td>3.151731</td>\n",
              "      <td>8.084656</td>\n",
              "      <td>6.90467</td>\n",
              "      <td>-3.016809</td>\n",
              "      <td>1.005586</td>\n",
              "      <td>-3.242914</td>\n",
              "      <td>-1.807679</td>\n",
              "      <td>-2.873847</td>\n",
              "      <td>-7.858417</td>\n",
              "      <td>4.226807</td>\n",
              "      <td>11.046021</td>\n",
              "      <td>7.15528</td>\n",
              "      <td>5.37511</td>\n",
              "      <td>1.274791</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.95264</td>\n",
              "      <td>4.344461</td>\n",
              "      <td>6.376086</td>\n",
              "      <td>2.445218</td>\n",
              "      <td>8.955554</td>\n",
              "      <td>4.103342</td>\n",
              "      <td>-2.638663</td>\n",
              "      <td>-1.186178</td>\n",
              "      <td>3.001384</td>\n",
              "      <td>-7.051963</td>\n",
              "      <td>6.906458</td>\n",
              "      <td>-3.431726</td>\n",
              "      <td>-0.519207</td>\n",
              "      <td>-8.974524</td>\n",
              "      <td>-0.807257</td>\n",
              "      <td>-0.456233</td>\n",
              "      <td>-5.885994</td>\n",
              "      <td>-4.973614</td>\n",
              "      <td>4.490455</td>\n",
              "      <td>-2.57421</td>\n",
              "      <td>0.339056</td>\n",
              "      <td>1.826858</td>\n",
              "      <td>-8.489288</td>\n",
              "      <td>5.520897</td>\n",
              "      <td>5.356351</td>\n",
              "      <td>-6.357804</td>\n",
              "      <td>-1.609869</td>\n",
              "      <td>3.560576</td>\n",
              "      <td>-5.086923</td>\n",
              "      <td>4.107792</td>\n",
              "      <td>-5.0108</td>\n",
              "      <td>-7.981196</td>\n",
              "      <td>-5.111307</td>\n",
              "      <td>6.368404</td>\n",
              "      <td>-3.654854</td>\n",
              "      <td>-0.052845</td>\n",
              "      <td>-5.485974</td>\n",
              "      <td>-8.187818</td>\n",
              "      <td>-4.258795</td>\n",
              "      <td>6.62896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2415653</th>\n",
              "      <td>-4.497859</td>\n",
              "      <td>-9.942174</td>\n",
              "      <td>3.103759</td>\n",
              "      <td>-1.652349</td>\n",
              "      <td>-2.584312</td>\n",
              "      <td>-1.776602</td>\n",
              "      <td>-3.870975</td>\n",
              "      <td>4.701572</td>\n",
              "      <td>-1.922743</td>\n",
              "      <td>0.782408</td>\n",
              "      <td>6.416571</td>\n",
              "      <td>-7.64352</td>\n",
              "      <td>3.64921</td>\n",
              "      <td>-7.158574</td>\n",
              "      <td>-0.545283</td>\n",
              "      <td>1.014253</td>\n",
              "      <td>-0.39179</td>\n",
              "      <td>5.457388</td>\n",
              "      <td>-6.126346</td>\n",
              "      <td>1.019077</td>\n",
              "      <td>-0.865525</td>\n",
              "      <td>1.340499</td>\n",
              "      <td>1.486618</td>\n",
              "      <td>3.259309</td>\n",
              "      <td>-5.427164</td>\n",
              "      <td>0.570631</td>\n",
              "      <td>3.151731</td>\n",
              "      <td>8.084656</td>\n",
              "      <td>6.90467</td>\n",
              "      <td>-3.016809</td>\n",
              "      <td>1.005586</td>\n",
              "      <td>-3.242914</td>\n",
              "      <td>-1.807679</td>\n",
              "      <td>-2.873847</td>\n",
              "      <td>-7.858417</td>\n",
              "      <td>4.226807</td>\n",
              "      <td>11.046021</td>\n",
              "      <td>7.15528</td>\n",
              "      <td>5.37511</td>\n",
              "      <td>1.274791</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.95264</td>\n",
              "      <td>4.344461</td>\n",
              "      <td>6.376086</td>\n",
              "      <td>2.445218</td>\n",
              "      <td>8.955554</td>\n",
              "      <td>4.103342</td>\n",
              "      <td>-2.638663</td>\n",
              "      <td>-1.186178</td>\n",
              "      <td>3.001384</td>\n",
              "      <td>-7.051963</td>\n",
              "      <td>6.906458</td>\n",
              "      <td>-3.431726</td>\n",
              "      <td>-0.519207</td>\n",
              "      <td>-8.974524</td>\n",
              "      <td>-0.807257</td>\n",
              "      <td>-0.456233</td>\n",
              "      <td>-5.885994</td>\n",
              "      <td>-4.973614</td>\n",
              "      <td>4.490455</td>\n",
              "      <td>-2.57421</td>\n",
              "      <td>0.339056</td>\n",
              "      <td>1.826858</td>\n",
              "      <td>-8.489288</td>\n",
              "      <td>5.520897</td>\n",
              "      <td>5.356351</td>\n",
              "      <td>-6.357804</td>\n",
              "      <td>-1.609869</td>\n",
              "      <td>3.560576</td>\n",
              "      <td>-5.086923</td>\n",
              "      <td>4.107792</td>\n",
              "      <td>-5.0108</td>\n",
              "      <td>-7.981196</td>\n",
              "      <td>-5.111307</td>\n",
              "      <td>6.368404</td>\n",
              "      <td>-3.654854</td>\n",
              "      <td>-0.052845</td>\n",
              "      <td>-5.485974</td>\n",
              "      <td>-8.187818</td>\n",
              "      <td>-4.258795</td>\n",
              "      <td>6.62896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2415654</th>\n",
              "      <td>-4.497859</td>\n",
              "      <td>-9.942174</td>\n",
              "      <td>3.103759</td>\n",
              "      <td>-1.652349</td>\n",
              "      <td>-2.584312</td>\n",
              "      <td>-1.776602</td>\n",
              "      <td>-3.870975</td>\n",
              "      <td>4.701572</td>\n",
              "      <td>-1.922743</td>\n",
              "      <td>0.782408</td>\n",
              "      <td>6.416571</td>\n",
              "      <td>-7.64352</td>\n",
              "      <td>3.64921</td>\n",
              "      <td>-7.158574</td>\n",
              "      <td>-0.545283</td>\n",
              "      <td>1.014253</td>\n",
              "      <td>-0.39179</td>\n",
              "      <td>5.457388</td>\n",
              "      <td>-6.126345</td>\n",
              "      <td>1.019077</td>\n",
              "      <td>-0.865525</td>\n",
              "      <td>1.340499</td>\n",
              "      <td>1.486618</td>\n",
              "      <td>3.259309</td>\n",
              "      <td>-5.427164</td>\n",
              "      <td>0.570631</td>\n",
              "      <td>3.151731</td>\n",
              "      <td>8.084656</td>\n",
              "      <td>6.90467</td>\n",
              "      <td>-3.016809</td>\n",
              "      <td>1.005586</td>\n",
              "      <td>-3.242914</td>\n",
              "      <td>-1.807679</td>\n",
              "      <td>-2.873847</td>\n",
              "      <td>-7.858416</td>\n",
              "      <td>4.226807</td>\n",
              "      <td>11.046021</td>\n",
              "      <td>7.15528</td>\n",
              "      <td>5.37511</td>\n",
              "      <td>1.274791</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.95264</td>\n",
              "      <td>4.344461</td>\n",
              "      <td>6.376086</td>\n",
              "      <td>2.445218</td>\n",
              "      <td>8.955554</td>\n",
              "      <td>4.103342</td>\n",
              "      <td>-2.638663</td>\n",
              "      <td>-1.186178</td>\n",
              "      <td>3.001384</td>\n",
              "      <td>-7.051963</td>\n",
              "      <td>6.906458</td>\n",
              "      <td>-3.431726</td>\n",
              "      <td>-0.519207</td>\n",
              "      <td>-8.974524</td>\n",
              "      <td>-0.807257</td>\n",
              "      <td>-0.456233</td>\n",
              "      <td>-5.885994</td>\n",
              "      <td>-4.973614</td>\n",
              "      <td>4.490455</td>\n",
              "      <td>-2.57421</td>\n",
              "      <td>0.339056</td>\n",
              "      <td>1.826858</td>\n",
              "      <td>-8.489288</td>\n",
              "      <td>5.520897</td>\n",
              "      <td>5.356351</td>\n",
              "      <td>-6.357804</td>\n",
              "      <td>-1.609869</td>\n",
              "      <td>3.560576</td>\n",
              "      <td>-5.086923</td>\n",
              "      <td>4.107792</td>\n",
              "      <td>-5.0108</td>\n",
              "      <td>-7.981196</td>\n",
              "      <td>-5.111307</td>\n",
              "      <td>6.368404</td>\n",
              "      <td>-3.654854</td>\n",
              "      <td>-0.052845</td>\n",
              "      <td>-5.485975</td>\n",
              "      <td>-8.187818</td>\n",
              "      <td>-4.258795</td>\n",
              "      <td>6.62896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2415655</th>\n",
              "      <td>-4.497859</td>\n",
              "      <td>-9.942174</td>\n",
              "      <td>3.103759</td>\n",
              "      <td>-1.652349</td>\n",
              "      <td>-2.584312</td>\n",
              "      <td>-1.776602</td>\n",
              "      <td>-3.870975</td>\n",
              "      <td>4.701572</td>\n",
              "      <td>-1.922743</td>\n",
              "      <td>0.782408</td>\n",
              "      <td>6.416571</td>\n",
              "      <td>-7.64352</td>\n",
              "      <td>3.64921</td>\n",
              "      <td>-7.158574</td>\n",
              "      <td>-0.545283</td>\n",
              "      <td>1.014253</td>\n",
              "      <td>-0.39179</td>\n",
              "      <td>5.457388</td>\n",
              "      <td>-6.126346</td>\n",
              "      <td>1.019077</td>\n",
              "      <td>-0.865525</td>\n",
              "      <td>1.340499</td>\n",
              "      <td>1.486618</td>\n",
              "      <td>3.259309</td>\n",
              "      <td>-5.427164</td>\n",
              "      <td>0.570631</td>\n",
              "      <td>3.151731</td>\n",
              "      <td>8.084656</td>\n",
              "      <td>6.90467</td>\n",
              "      <td>-3.016809</td>\n",
              "      <td>1.005586</td>\n",
              "      <td>-3.242914</td>\n",
              "      <td>-1.807679</td>\n",
              "      <td>-2.873847</td>\n",
              "      <td>-7.858417</td>\n",
              "      <td>4.226807</td>\n",
              "      <td>11.046021</td>\n",
              "      <td>7.15528</td>\n",
              "      <td>5.37511</td>\n",
              "      <td>1.274791</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.95264</td>\n",
              "      <td>4.344461</td>\n",
              "      <td>6.376086</td>\n",
              "      <td>2.445218</td>\n",
              "      <td>8.955554</td>\n",
              "      <td>4.103342</td>\n",
              "      <td>-2.638663</td>\n",
              "      <td>-1.186178</td>\n",
              "      <td>3.001384</td>\n",
              "      <td>-7.051963</td>\n",
              "      <td>6.906458</td>\n",
              "      <td>-3.431726</td>\n",
              "      <td>-0.519207</td>\n",
              "      <td>-8.974524</td>\n",
              "      <td>-0.807257</td>\n",
              "      <td>-0.456233</td>\n",
              "      <td>-5.885994</td>\n",
              "      <td>-4.973614</td>\n",
              "      <td>4.490455</td>\n",
              "      <td>-2.57421</td>\n",
              "      <td>0.339056</td>\n",
              "      <td>1.826858</td>\n",
              "      <td>-8.489288</td>\n",
              "      <td>5.520897</td>\n",
              "      <td>5.356351</td>\n",
              "      <td>-6.357804</td>\n",
              "      <td>-1.609869</td>\n",
              "      <td>3.560576</td>\n",
              "      <td>-5.086923</td>\n",
              "      <td>4.107792</td>\n",
              "      <td>-5.0108</td>\n",
              "      <td>-7.981196</td>\n",
              "      <td>-5.111307</td>\n",
              "      <td>6.368404</td>\n",
              "      <td>-3.654854</td>\n",
              "      <td>-0.052845</td>\n",
              "      <td>-5.485974</td>\n",
              "      <td>-8.187818</td>\n",
              "      <td>-4.258795</td>\n",
              "      <td>6.62896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2415656</th>\n",
              "      <td>-4.497859</td>\n",
              "      <td>-9.942174</td>\n",
              "      <td>3.103759</td>\n",
              "      <td>-1.652349</td>\n",
              "      <td>-2.584312</td>\n",
              "      <td>-1.776602</td>\n",
              "      <td>-3.870975</td>\n",
              "      <td>4.701572</td>\n",
              "      <td>-1.922743</td>\n",
              "      <td>0.782408</td>\n",
              "      <td>6.416571</td>\n",
              "      <td>-7.64352</td>\n",
              "      <td>3.64921</td>\n",
              "      <td>-7.158574</td>\n",
              "      <td>-0.545283</td>\n",
              "      <td>1.014253</td>\n",
              "      <td>-0.39179</td>\n",
              "      <td>5.457388</td>\n",
              "      <td>-6.126346</td>\n",
              "      <td>1.019077</td>\n",
              "      <td>-0.865525</td>\n",
              "      <td>1.340499</td>\n",
              "      <td>1.486618</td>\n",
              "      <td>3.259309</td>\n",
              "      <td>-5.427164</td>\n",
              "      <td>0.570631</td>\n",
              "      <td>3.151731</td>\n",
              "      <td>8.084656</td>\n",
              "      <td>6.90467</td>\n",
              "      <td>-3.016809</td>\n",
              "      <td>1.005586</td>\n",
              "      <td>-3.242914</td>\n",
              "      <td>-1.807679</td>\n",
              "      <td>-2.873847</td>\n",
              "      <td>-7.858417</td>\n",
              "      <td>4.226807</td>\n",
              "      <td>11.046021</td>\n",
              "      <td>7.15528</td>\n",
              "      <td>5.37511</td>\n",
              "      <td>1.274791</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.95264</td>\n",
              "      <td>4.344461</td>\n",
              "      <td>6.376086</td>\n",
              "      <td>2.445218</td>\n",
              "      <td>8.955554</td>\n",
              "      <td>4.103342</td>\n",
              "      <td>-2.638663</td>\n",
              "      <td>-1.186178</td>\n",
              "      <td>3.001384</td>\n",
              "      <td>-7.051963</td>\n",
              "      <td>6.906458</td>\n",
              "      <td>-3.431726</td>\n",
              "      <td>-0.519207</td>\n",
              "      <td>-8.974524</td>\n",
              "      <td>-0.807257</td>\n",
              "      <td>-0.456233</td>\n",
              "      <td>-5.885994</td>\n",
              "      <td>-4.973614</td>\n",
              "      <td>4.490455</td>\n",
              "      <td>-2.57421</td>\n",
              "      <td>0.339056</td>\n",
              "      <td>1.826858</td>\n",
              "      <td>-8.489288</td>\n",
              "      <td>5.520897</td>\n",
              "      <td>5.356351</td>\n",
              "      <td>-6.357804</td>\n",
              "      <td>-1.609869</td>\n",
              "      <td>3.560576</td>\n",
              "      <td>-5.086923</td>\n",
              "      <td>4.107792</td>\n",
              "      <td>-5.0108</td>\n",
              "      <td>-7.981196</td>\n",
              "      <td>-5.111307</td>\n",
              "      <td>6.368404</td>\n",
              "      <td>-3.654854</td>\n",
              "      <td>-0.052845</td>\n",
              "      <td>-5.485974</td>\n",
              "      <td>-8.187818</td>\n",
              "      <td>-4.258795</td>\n",
              "      <td>6.62896</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2415657 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               0         1         2   ...        97        98       99\n",
              "0       -4.497859 -9.942174  3.103759  ... -8.187818 -4.258795  6.62896\n",
              "1       -4.497859 -9.942174  3.103759  ... -8.187818 -4.258795  6.62896\n",
              "2       -4.497859 -9.942174  3.103759  ... -8.187818 -4.258795  6.62896\n",
              "3       -4.497859 -9.942174  3.103759  ... -8.187818 -4.258795  6.62896\n",
              "4       -4.497859 -9.942174  3.103759  ... -8.187818 -4.258795  6.62896\n",
              "...           ...       ...       ...  ...       ...       ...      ...\n",
              "2415652 -4.497859 -9.942174  3.103759  ... -8.187818 -4.258795  6.62896\n",
              "2415653 -4.497859 -9.942174  3.103759  ... -8.187818 -4.258795  6.62896\n",
              "2415654 -4.497859 -9.942174  3.103759  ... -8.187818 -4.258795  6.62896\n",
              "2415655 -4.497859 -9.942174  3.103759  ... -8.187818 -4.258795  6.62896\n",
              "2415656 -4.497859 -9.942174  3.103759  ... -8.187818 -4.258795  6.62896\n",
              "\n",
              "[2415657 rows x 100 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk6CJa6pcQC-"
      },
      "source": [
        "##### Save to disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pMtN1sRSCPl"
      },
      "source": [
        "vectors.to_csv(datapath+\"Corpus/Taxonomy/CSO.3.3-with-labels-US-lem-vectors.csv\",index=False)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoF0bLquDEpQ"
      },
      "source": [
        "vectors.to_csv(directory+'DE-n vectrors.csv',index=False)\n",
        "del vectors\n",
        "del keywords\n",
        "gc.collect()\n",
        "\n",
        "keywords_unique = pd.DataFrame(keywords_unique)\n",
        "keywords_unique.to_csv(directory+'DE-n keywords.csv',index=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4wRDXMX6tBH"
      },
      "source": [
        "# Save embeddings to disk\n",
        "with open(datapath+\"Corpus/Taxonomy/AI kw mergedAI kw merged preprocessed for similarity measure embedding.json\", 'w') as json_file:\n",
        "    json.dump(output_dict, json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohPXzf8mKUxQ",
        "outputId": "978f7b84-38da-4b31-e3e6-0c6636a2147c"
      },
      "source": [
        "print(directory)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive/My Drive/Data/Corpus/Dimensions AI unlimited citations/clean/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yYjO-JQBLE3"
      },
      "source": [
        "### Get Doc Embeddings (SIF)\n",
        "\n",
        "It is not recommended to perform this on cloud, as it is not process intesive, yet takes too long depending on the doc-count. It might take over 30 hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp1fP9z4xUGX"
      },
      "source": [
        "Make a probability dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buSIQtoWVnOz"
      },
      "source": [
        "corpus_tokens_s = pd.Series(corpus_tokens)\n",
        "corpus_tokens_probabilities = (corpus_tokens_s.groupby(corpus_tokens_s).transform('count') / len(corpus_tokens_s)).values\n",
        "corpus_tokens_probabilities = pd.DataFrame(corpus_tokens_probabilities)\n",
        "corpus_tokens_probabilities['tokens'] = corpus_tokens_s\n",
        "corpus_tokens_probabilities.columns = ['probability','token']\n",
        "corpus_tokens_probabilities = corpus_tokens_probabilities.groupby('token').mean()\n",
        "corpus_tokens_probabilities = corpus_tokens_probabilities.reset_index()\n",
        "corpus_tokens_probabilities.columns = ['token','probability']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu51jQbo9QE6"
      },
      "source": [
        "Get vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72BAU9Jm9PwE",
        "outputId": "427361b5-2231-40ab-b819-c11aa6d61144"
      },
      "source": [
        "vectors = []\n",
        "for token in tqdm(corpus_tokens_probabilities['token'],total=corpus_tokens_probabilities.shape[0]):\n",
        "    vectors.append(model.wv[token])\n",
        "corpus_tokens_probabilities['vector'] = vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 41443/41443 [00:01<00:00, 29117.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y9rI72IxZal"
      },
      "source": [
        "Calculate weighted average vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WwJtQqABN8w",
        "outputId": "1e3eab7e-f796-4978-df21-eedf6e9f4cfd"
      },
      "source": [
        "a = 1e-3\n",
        "embedding_size = 128\n",
        "\n",
        "doc_set = []\n",
        "for doc in tqdm(corpus['abstracts'].values.tolist(),total=len(corpus['abstracts'].values.tolist())):\n",
        "    vs = np.zeros(embedding_size)  # add all word2vec values into one vector for the sentence\n",
        "    doc_length = len(doc.split())\n",
        "#     print(doc.split())\n",
        "    for word in doc.lower().split():\n",
        "        a_value = a / (a + corpus_tokens_probabilities[corpus_tokens_probabilities['token']==word]['probability'].values.tolist()[0])  # smooth inverse frequency, SIF\n",
        "        vs = np.add(vs, np.multiply(a_value, corpus_tokens_probabilities[corpus_tokens_probabilities['token']==word]['vector'].values.tolist()[0]))  # vs += sif * word_vector\n",
        "\n",
        "    vs = np.divide(vs, doc_length)  # weighted average\n",
        "    doc_set.append(vs)  # add to our existing re-calculated set of sentences\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 23144/23144 [2:24:09<00:00,  2.68it/s]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3p67dBhcyDE"
      },
      "source": [
        "pd.DataFrame(doc_set).to_csv(datapath+'Corpus/cora-classify/cora/embeddings/FastText SIF cora corpus',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3C8IQiLgBJJ"
      },
      "source": [
        "##### Wikipedia embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKLjavgtgBeJ"
      },
      "source": [
        "import os\n",
        "import time\n",
        "from stat import S_ISREG, ST_CTIME, ST_MODE\n",
        "\n",
        "print(\"\\nSearching for Wiki texts...\\n\")\n",
        "dir_path = datapath+'Corpus/AI Wiki Classifications/applications/clean/'\n",
        "data = (os.path.join(dir_path, fn) for fn in os.listdir(dir_path))\n",
        "data = ((os.stat(path), path) for path in data)\n",
        "data = ((stat[ST_CTIME], path) for stat, path in data if S_ISREG(stat[ST_MODE]))\n",
        "\n",
        "names = []\n",
        "files = []\n",
        "for cdate, path in sorted(data):\n",
        "    print('   - ', time.ctime(cdate), os.path.basename(path),int(os.path.getsize(path)/1000000),'MB')\n",
        "    files.append(path)\n",
        "    names.append(os.path.basename(path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFMUH3SfgF2h"
      },
      "source": [
        "embedding_size = 50\n",
        "all = []\n",
        "for file_index,file in enumerate(files):\n",
        "    print('\\n',file,'\\n')\n",
        "    corpus = pd.read_csv(file)\n",
        "    corpus_tokens = [item.lower() for sublist in corpus['sentence'].values.tolist() if pd.notnull(sublist) for item in sublist.split()]\n",
        "    gc.collect()\n",
        "    \n",
        "    corpus_tokens_s = pd.Series(corpus_tokens)\n",
        "    corpus_tokens_probabilities = (corpus_tokens_s.groupby(corpus_tokens_s).transform('count') / len(corpus_tokens_s)).values\n",
        "    corpus_tokens_probabilities = pd.DataFrame(corpus_tokens_probabilities)\n",
        "    corpus_tokens_probabilities['tokens'] = corpus_tokens_s\n",
        "    corpus_tokens_probabilities.columns = ['probability','token']\n",
        "    corpus_tokens_probabilities = corpus_tokens_probabilities.groupby('token').mean()\n",
        "    corpus_tokens_probabilities = corpus_tokens_probabilities.reset_index()\n",
        "    corpus_tokens_probabilities.columns = ['token','probability']\n",
        "\n",
        "    vectors = []\n",
        "    for token in tqdm(corpus_tokens_probabilities['token'],total=corpus_tokens_probabilities.shape[0]):\n",
        "        vectors.append(model.wv[token])\n",
        "    corpus_tokens_probabilities['vector'] = vectors\n",
        "\n",
        "    a = 1e-3\n",
        "\n",
        "    doc_set = []\n",
        "    for doc in tqdm(corpus['sentence'].values.tolist(),total=len(corpus['sentence'].values.tolist())):\n",
        "        vs = np.zeros(embedding_size)  # add all word2vec values into one vector for the sentence\n",
        "        if pd.notnull(doc):\n",
        "            doc_length = len(doc.split())\n",
        "        #     print(doc.split())\n",
        "            for word in doc.lower().split():\n",
        "                a_value = a / (a + corpus_tokens_probabilities[corpus_tokens_probabilities['token']==word]['probability'].values.tolist()[0])  # smooth inverse frequency, SIF\n",
        "                vs = np.add(vs, np.multiply(a_value, corpus_tokens_probabilities[corpus_tokens_probabilities['token']==word]['vector'].values.tolist()[0]))  # vs += sif * word_vector\n",
        "\n",
        "            vs = np.divide(vs, doc_length)  # weighted average\n",
        "            doc_set.append(vs)  # add to our existing re-calculated set of sentences\n",
        "\n",
        "    pd.DataFrame(doc_set).to_csv(datapath+'Corpus/AI Wiki Classifications/applications/clean/vectors/'+names[file_index],index=False)\n",
        "\n",
        "    all.append(pd.DataFrame(doc_set).mean(axis=0))\n",
        "\n",
        "all_df = pd.DataFrame(all)\n",
        "all_df['clusters'] = names\n",
        "all_df.to_csv(datapath+'Corpus/AI Wiki Classifications/applications/clean/vectors/all',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBNrZ_OccyDF"
      },
      "source": [
        "### Get Doc Embedding (averaging)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6jh2g6xcyDF",
        "outputId": "7dccb7cc-47eb-4a3b-89e4-6487292e3802"
      },
      "source": [
        "doc_vectors = []\n",
        "for doc in tqdm(corpus['abstracts']):\n",
        "    tokens = doc.split()\n",
        "    doc_vectors.append(np.array([model.wv[token] for token in tokens]).mean(axis=0))\n",
        "pd.DataFrame(doc_vectors).to_csv(datapath+'Corpus/KPRIS/embeddings/FastText Avg large corpus',index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19710/19710 [00:09<00:00, 2073.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_m00tKKkKNG"
      },
      "source": [
        "# Train on a large Scopus corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz7pfl9d6R9o"
      },
      "source": [
        "#### Load Corpus Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuO0J8oxkKWz"
      },
      "source": [
        "sentence_corpus = pd.read_csv(datapath+'Corpus/patent_wos_training_very_large')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYHEYxwpEgCi"
      },
      "source": [
        "#### Preprocess and prepare corpus for FastText training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkqrGJ3cEgnX"
      },
      "source": [
        "sentences = []\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "with open(datapath+'corpus/AI ALL/1900-2019 corpus sentences abstract-title further processed.csv', 'w') as f:\n",
        "    for index,row in tqdm(sentence_corpus.iterrows(),total=sentence_corpus.shape[0]):\n",
        "        sentence = row['sentence']\n",
        "        sentence = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",sentence)\n",
        "        sentence = word_tokenize(sentence)\n",
        "        sentence = [word for word in sentence if not word in punkts] \n",
        "        sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
        "        # sentences.append(sentence)\n",
        "        f.write(\"%s\\n\" % ' '.join(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQHwYepMdKFc"
      },
      "source": [
        "#### Save sentences to disk for future use -- Not needed anymore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJZ0UwdhdOlp"
      },
      "source": [
        "pd.DataFrame([' '.join(words) for words in sentences],columns=['sentences']).to_csv(\n",
        "    datapath+'corpus/AI ALL/1900-2019 corpus sentences abstract-title further processed.csv',\n",
        "    header=True,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j6H5hsfu8p0"
      },
      "source": [
        "#### Load pre-processed sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh9_K4LmkvsH"
      },
      "source": [
        "sentence_corpus = pd.read_csv(datapath+'Corpus/Dimensions All/clean/corpus sentences abstract-title-2')\n",
        "sentence_corpus_scopus = pd.read_csv(datapath+'Corpus/Scopus new/clean/corpus sentences abstract-title')\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki6Wngdvkxyl"
      },
      "source": [
        "sentence_corpus = sentence_corpus.append(sentence_corpus_scopus)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLHxqzOPvCdV",
        "outputId": "2f6138b2-9a9a-4200-e312-9d087a3535dc"
      },
      "source": [
        "sentence_corpus[['sentence']]\n",
        "# sentence_corpus.columns = [\"sentence\"]\n",
        "sentences = []\n",
        "sentence_corpus = sentence_corpus.fillna('')\n",
        "for index,row in tqdm(sentence_corpus.iterrows(),total=sentence_corpus.shape[0]):\n",
        "    sentences.append(row['sentence'].split(' '))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7059700/7059700 [10:14<00:00, 11496.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "UA83SDYjwfrO",
        "outputId": "086d021d-e597-4a62-de01-50403152521c"
      },
      "source": [
        "sentence_corpus.head(10)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>nature consciousness</td>\n",
              "      <td>1970.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>paper existence nature consciousness distinct ...</td>\n",
              "      <td>1970.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>contended consciousness properly exists distin...</td>\n",
              "      <td>1970.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>consequence postulate consciousness real nonph...</td>\n",
              "      <td>1970.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>therefore postulated conscious reality coupled...</td>\n",
              "      <td>1970.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>connection unify single conscious experience d...</td>\n",
              "      <td>1970.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>thus must exist quantum mechanical process ope...</td>\n",
              "      <td>1970.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>detailed mathematical consideration theory lea...</td>\n",
              "      <td>1970.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>found consciousness occur minimum synaptic tra...</td>\n",
              "      <td>1970.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>equation represents physical condition necessa...</td>\n",
              "      <td>1970.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   article_index                                           sentence    year\n",
              "0              0                               nature consciousness  1970.0\n",
              "1              0  paper existence nature consciousness distinct ...  1970.0\n",
              "2              0  contended consciousness properly exists distin...  1970.0\n",
              "3              0  consequence postulate consciousness real nonph...  1970.0\n",
              "4              0  therefore postulated conscious reality coupled...  1970.0\n",
              "5              0  connection unify single conscious experience d...  1970.0\n",
              "6              0  thus must exist quantum mechanical process ope...  1970.0\n",
              "7              0  detailed mathematical consideration theory lea...  1970.0\n",
              "8              0  found consciousness occur minimum synaptic tra...  1970.0\n",
              "9              0  equation represents physical condition necessa...  1970.0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoJhFXrrcyDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b47afa0d-d94d-42e5-f230-822819a49015"
      },
      "source": [
        "sentences = [[t for t in s if len(t)>1] for s in sentences if len(s)>1]\n",
        "sentences[:5]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['nature', 'consciousness'],\n",
              " ['paper',\n",
              "  'existence',\n",
              "  'nature',\n",
              "  'consciousness',\n",
              "  'distinct',\n",
              "  'phenomenon',\n",
              "  'considered',\n",
              "  'framework',\n",
              "  'current',\n",
              "  'scientific',\n",
              "  'philosophy'],\n",
              " ['contended',\n",
              "  'consciousness',\n",
              "  'properly',\n",
              "  'exists',\n",
              "  'distinct',\n",
              "  'entity',\n",
              "  'apparently',\n",
              "  'immeasurable',\n",
              "  'quantity'],\n",
              " ['consequence',\n",
              "  'postulate',\n",
              "  'consciousness',\n",
              "  'real',\n",
              "  'nonphysical',\n",
              "  'entity',\n",
              "  'term',\n",
              "  'apply',\n",
              "  'modern',\n",
              "  'physic',\n",
              "  'consideration',\n",
              "  'possible',\n",
              "  'type',\n",
              "  'equation',\n",
              "  'relating',\n",
              "  'variable',\n",
              "  'physical',\n",
              "  'world',\n",
              "  'pi',\n",
              "  'variable',\n",
              "  'consciousness',\n",
              "  'ci',\n",
              "  'suggest',\n",
              "  'exist',\n",
              "  'certain',\n",
              "  'quantity',\n",
              "  'common',\n",
              "  'physical',\n",
              "  'equation',\n",
              "  'equation',\n",
              "  'conscious',\n",
              "  'reality'],\n",
              " ['therefore',\n",
              "  'postulated',\n",
              "  'conscious',\n",
              "  'reality',\n",
              "  'coupled',\n",
              "  'physical',\n",
              "  'reality',\n",
              "  'single',\n",
              "  'fundamental',\n",
              "  'physical',\n",
              "  'variable',\n",
              "  'consciousness',\n",
              "  'apparently',\n",
              "  'associated',\n",
              "  'way',\n",
              "  'electrochemical',\n",
              "  'process',\n",
              "  'occuring',\n",
              "  'brain',\n",
              "  'pertinent',\n",
              "  'physical',\n",
              "  'equation',\n",
              "  'satisfactorily',\n",
              "  'understood',\n",
              "  'today',\n",
              "  'possible',\n",
              "  'diskover',\n",
              "  'nature',\n",
              "  'interaction',\n",
              "  'ci',\n",
              "  'pi',\n",
              "  'variable']]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-9A4hbD7xwV"
      },
      "source": [
        "### Train Fasttext - Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYfn-ZQfhed1"
      },
      "source": [
        "#### Load a model to continue training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bnu7_aMhu-A"
      },
      "source": [
        "* If want to continue training, run this section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7eHrQg_g2ej"
      },
      "source": [
        "model = load(gensim_model_address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s07ymd4xhtWh"
      },
      "source": [
        "model.build_vocab(sentences, update=True)\n",
        "model.train(sentences, total_examples=len(sentences), epochs=model.epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9LWfxerh2DI"
      },
      "source": [
        "* Otherwise run this section\n",
        "\n",
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsDmFFxp7yJa"
      },
      "source": [
        "model = fasttext_gensim(min_n=3, max_n=6, size=15, window=5, min_count=1, seed = 50)\n",
        "model.build_vocab(sentences=sentences)\n",
        "model.train(sentences=sentences, total_examples=len(sentences), epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqV_noKRcD4b"
      },
      "source": [
        "fname = \"datapath+Models/fasttext-scopus-2.2-million_docs-gensim 15D.model\"\n",
        "model.save(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3vrtHdqHy-K"
      },
      "source": [
        "model = fasttext_gensim(min_n=3, max_n=6, size=50, window=5, min_count=1, seed = 50)\n",
        "model.build_vocab(sentences=sentences)\n",
        "model.train(sentences=sentences, total_examples=len(sentences), epochs=10)\n",
        "fname = datapath+\"Models/fasttext-scopus-2.2-million_docs-gensim 50D.model\"\n",
        "model.save(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g75Pt__ZH4RZ"
      },
      "source": [
        "model = fasttext_gensim(min_n=3, max_n=12, size=100, window=10, min_count=5, seed = 50)\n",
        "model.build_vocab(sentences=sentences)\n",
        "model.train(sentences=sentences, total_examples=len(sentences), epochs=10)#,threads=15)\n",
        "fname = datapath+\"Corpus/Dimensions All/FastText100D-dim-scopus-update-gensim4-window10.model\"\n",
        "model.save(fname)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnTG0yUGiqxw"
      },
      "source": [
        "#### Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVJoU_sLiw2E"
      },
      "source": [
        "similarities = model.wv.most_similar(positive=['logic','fuzzy','expert'],negative=['deep','neural','network','cnn','ann'])\n",
        "most_similar = similarities[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaKS52YSDcup"
      },
      "source": [
        "most_similar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkokDJdHjEFz"
      },
      "source": [
        "not_matching = model.wv.doesnt_match(\"human computer interface tree\".split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fOp26djDYk7"
      },
      "source": [
        "not_matching"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7RJl89QjMhl"
      },
      "source": [
        "sim_score = model.wv.similarity('computer', 'human')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtlAXvqWDWo4"
      },
      "source": [
        "sim_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWLLBs8UBRwy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "823bd7c7-cfcc-4727-bdd9-fd80406232e2"
      },
      "source": [
        "print(model.wv['artificial intelligence'])\n",
        "print(model.wv['artificial'])\n",
        "print(model.wv['intelligence'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.15527838 -0.8417947   0.13572344  0.9862384  -0.69965893 -1.0619401\n",
            " -0.6663257   1.6179986   1.1380848   0.20575723  1.5610282   2.1450064\n",
            " -1.0086488   1.0221347   1.4153669   0.4017173  -1.4664081   1.2477343\n",
            "  0.6175467   0.04412325 -0.02988294 -0.0195925  -1.6241038   0.09074374\n",
            " -2.1654859  -0.10280632 -2.5168698  -0.3226759   0.9145073  -0.02321676\n",
            "  1.7271574   0.15474042 -0.726134    0.73339623 -2.0182426   0.01646873\n",
            " -0.5060659   0.14963873 -1.4563788   1.1746178  -0.7265221   0.8417264\n",
            "  1.8999654   0.8506569   0.24744762  0.595496    1.0273472   0.513688\n",
            "  0.6760288  -0.22930592  0.919514   -0.53905135 -0.80066395 -1.1185676\n",
            "  1.4163915  -0.22790125 -0.29137468 -1.9458369   0.7058971  -0.24045321\n",
            " -1.1205777  -0.5625544   0.13106921 -0.95115215  0.98426855  0.24689397\n",
            "  1.8930526   0.6130692  -1.6551938   1.013349   -0.00285757  1.2419046\n",
            "  0.41582862 -0.6464679  -0.9078993   0.6697579  -1.2840121   0.5028592\n",
            " -0.5414527  -0.34156412 -0.9993528   1.4491874   0.32694387 -1.8635948\n",
            " -0.42196348  0.37646985  0.11579645  1.3988535   1.9017984  -1.9127331\n",
            " -1.2842574  -0.3089776   1.3923998  -2.6120155  -1.5734841   0.45527756\n",
            "  0.3025386   0.8743049   1.6871859  -0.55466825]\n",
            "[ 0.92674416  6.9257236  -1.7348211   2.578849   -0.4535528   2.4192502\n",
            "  0.38211876 -2.4758773   4.3903418   2.2343233   4.018081   -1.4103285\n",
            "  2.7899487   2.390545    4.961209    3.6413672  -2.3567345   1.1790022\n",
            "  1.7769686  -5.431161   -3.139113   -0.2966793  -1.3881694   1.2212404\n",
            " -4.164785   -2.8241851  -3.4258282   1.2943592   2.5705817  -3.47619\n",
            "  2.5053349  -1.8078697  -5.4559546   1.2795173  -3.306459   -3.4554715\n",
            " -0.64651304  0.30718678  3.099644   -3.118414   -2.5646617  -0.10330605\n",
            "  4.2102942   2.3888514   3.8323772   2.2059815   1.0493971   1.4824048\n",
            "  0.43462548  0.0365151  -0.06812529 -1.1575311  -0.44522688 -1.6371533\n",
            " -0.25365376 -0.856122   -2.7298186  -1.5108197  -0.16644791 -1.5536424\n",
            " -1.2648354  -7.156601   -0.2968027  -1.9933525   1.3105358  -1.6218417\n",
            "  4.1635685   3.3415658  -3.1010537   2.0742724   2.1670377   1.4898245\n",
            "  3.8310053   3.179985    0.8918676   0.20085981 -3.7180283  -0.19670711\n",
            "  0.61208576  0.10990621  0.7448788   1.1635563  -4.594825   -4.843035\n",
            "  0.2703265   0.65704256  0.28257677  1.4356182   5.7752113  -3.3349419\n",
            " -2.3843017  -1.104914    2.8790734  -1.4421624  -2.8032076  -1.099746\n",
            "  3.9274328   0.66708565  3.1387174   0.4226782 ]\n",
            "[-0.85111547 -7.0830216   1.2306664   0.82357436 -1.4399977  -4.177901\n",
            " -1.88648     5.0511665  -0.6039567  -0.99365187  1.1763303   5.7386374\n",
            " -4.0317388   0.8151518   0.23573017 -1.5417347  -2.383229    2.5784442\n",
            "  1.3374662   3.035125    1.8430384   0.62639    -3.7906845  -0.91602755\n",
            " -2.1572754   1.1865695  -4.0561347  -0.8857689   0.8187958   2.0423641\n",
            "  3.0264068   1.5818359   1.0534464   1.4613762  -2.9442725   3.2975562\n",
            " -0.89385647 -0.07426721 -5.0857754   4.4421425  -0.69599193  2.3389277\n",
            "  2.565116    0.4358644  -1.5693047   0.18379615  1.3192426  -0.04239114\n",
            "  1.9199195  -1.7720289   1.7866687  -0.91248953 -1.3471661  -1.7696134\n",
            "  3.4512858   0.08624406  1.2951971  -3.9313307   1.0788459   0.05672526\n",
            " -1.7124692   3.5667324  -0.03471302 -1.106371    1.8761657   1.9804678\n",
            "  1.1820661   0.21478917 -2.549248    0.70047015 -1.1163235   1.3785014\n",
            " -1.5625405  -4.0807962  -2.9255548   1.5288393  -1.069847    1.1401819\n",
            " -1.7131923  -0.5825283  -2.797561    2.525607    4.012831   -1.2724254\n",
            " -1.1763872   0.89250714  0.08701009  2.6708724   1.7357949  -2.7268348\n",
            " -1.5291913  -0.39953697  1.2315842  -5.140653   -0.9916628   1.6091381\n",
            " -2.015259    1.7965345   2.4041297  -1.6504326 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sSZfjEMUyMc"
      },
      "source": [
        "### Train Fasttext - Facebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK4N7jUqU1CJ"
      },
      "source": [
        "sentences_joined = ' '.join(sentences)\n",
        "model = fasttext.train_unsupervised(sentences_joined, \"cbow\", minn=2, maxn=5, dim=50, epoch=10,lr=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLNquk0BaywP"
      },
      "source": [
        "#### Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i6detxZVMUQ"
      },
      "source": [
        "model.words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o13SRSefVFDD"
      },
      "source": [
        "model.get_word_vector(\"the\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FvMFASYVNzs"
      },
      "source": [
        "model.get_nearest_neighbors('asparagus')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH9Wdf5WV5F8"
      },
      "source": [
        "model.get_analogies(\"intelligence\", \"math\", \"fuzzy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaDnem0pWcNw"
      },
      "source": [
        "#### Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wjWeyw-Wbpi"
      },
      "source": [
        "model.save_model(datapath+\"fasttext-scopus_wos-merged-310k_docs-facebook.ftz\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}