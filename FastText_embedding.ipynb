{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fasttext_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "r_m00tKKkKNG"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahandv/science_science/blob/master/FastText_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B43KQKxsZqK2",
        "colab_type": "text"
      },
      "source": [
        "# FASTTEXT EMBEDDING\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyakRMAwbUq2",
        "colab_type": "text"
      },
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIPuXPla6BKh",
        "colab_type": "text"
      },
      "source": [
        "### Clone Project Git Repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-YQSyFe6Bgv",
        "colab_type": "code",
        "outputId": "4a482077-d18f-4de0-e20f-7bf1c6b0d1ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!rm -rf 'science_science'\n",
        "username = \"sahandv\"#@param {type:\"string\"}\n",
        "# password = \"\"#@param {type:\"string\"} \n",
        "\n",
        "!git clone https://github.com/$username/science_science.git\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'science_science'...\n",
            "remote: Enumerating objects: 66, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 344 (delta 34), reused 17 (delta 0), pack-reused 278\u001b[K\n",
            "Receiving objects: 100% (344/344), 70.41 MiB | 11.40 MiB/s, done.\n",
            "Resolving deltas: 100% (143/143), done.\n",
            "sample_data  science_science\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8crXh0Ek1FA",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulliMrpjkKAd",
        "colab_type": "code",
        "outputId": "4bf13dcf-7d2d-4095-b041-b6f5ce616e4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhV7OflrnOhI",
        "colab_type": "code",
        "outputId": "d5b9f968-0618-47c2-d903-c5c34a1159da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check files!\n",
        "!ls 'drive/My Drive/Data-Permenant/FastText-crawl-300d-2M-subword'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "crawl-300d-2M-subword.bin  crawl-300d-2M-subword.vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiBuMInGmypx",
        "colab_type": "text"
      },
      "source": [
        "### Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OwQMHvQmy3A",
        "colab_type": "code",
        "outputId": "5b6ff39d-0b26-4139-acf4-6fd6688bed05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install fasttext\n",
        "!pip install -r 'science_science/requirements.txt'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 1.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.4.3)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (45.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.17.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=2386106 sha256=47b55958af7c389218d37b4f919051f23d4f5ae133c79e0b856cb1376104b6d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 1)) (1.17.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 2)) (4.28.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 3)) (0.25.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 4)) (3.2.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 5)) (2.1.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 6)) (0.22.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 7)) (0.16.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 9)) (0.9.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 10)) (1.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 11)) (3.1.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 12)) (6.2.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 13)) (0.8.6)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 14)) (3.6.0)\n",
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 16)) (2.4)\n",
            "Collecting netgraph\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/19/5db2141bdc881495055a5a05e1f86e5c70d761936a6fb2130e2a439fcac2/netgraph-3.1.6-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: yellowbrick in /usr/local/lib/python3.6/dist-packages (from -r science_science/requirements.txt (line 18)) (0.9.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r science_science/requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r science_science/requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r science_science/requirements.txt (line 4)) (1.12.0)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (0.6.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (2.21.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (0.9.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (1.0.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (2.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (2.0.3)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy->-r science_science/requirements.txt (line 5)) (7.0.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r science_science/requirements.txt (line 6)) (0.14.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r science_science/requirements.txt (line 7)) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r science_science/requirements.txt (line 7)) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r science_science/requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r science_science/requirements.txt (line 11)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r science_science/requirements.txt (line 11)) (2.4.6)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->-r science_science/requirements.txt (line 14)) (1.9.0)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r science_science/requirements.txt (line 15)) (0.34.2)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r science_science/requirements.txt (line 15)) (2.11.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r science_science/requirements.txt (line 15)) (2.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r science_science/requirements.txt (line 15)) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r science_science/requirements.txt (line 15)) (0.16.0)\n",
            "Collecting funcy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->-r science_science/requirements.txt (line 16)) (4.4.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r science_science/requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r science_science/requirements.txt (line 5)) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r science_science/requirements.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r science_science/requirements.txt (line 5)) (2019.11.28)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->-r science_science/requirements.txt (line 11)) (45.1.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r science_science/requirements.txt (line 14)) (1.11.9)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r science_science/requirements.txt (line 14)) (2.49.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis->-r science_science/requirements.txt (line 15)) (1.1.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis->-r science_science/requirements.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis->-r science_science/requirements.txt (line 15)) (8.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis->-r science_science/requirements.txt (line 15)) (19.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis->-r science_science/requirements.txt (line 15)) (1.8.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis->-r science_science/requirements.txt (line 15)) (0.7.1)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.9 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r science_science/requirements.txt (line 14)) (1.14.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r science_science/requirements.txt (line 14)) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r science_science/requirements.txt (line 14)) (0.3.2)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->smart-open>=1.2.1->gensim->-r science_science/requirements.txt (line 14)) (0.15.2)\n",
            "Building wheels for collected packages: pyLDAvis, funcy\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97711 sha256=33fa60a1482f957baddc1562d6b74df5b890a6a56e3edaa077b5893e37576833\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "  Building wheel for funcy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32042 sha256=4b1a74002e8307ab554ab481580ca5c0326e41c5f08ee923e8af360acba59d52\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\n",
            "Successfully built pyLDAvis funcy\n",
            "Installing collected packages: funcy, pyLDAvis, netgraph\n",
            "Successfully installed funcy-1.14 netgraph-3.1.6 pyLDAvis-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9l5YuBnkKFB",
        "colab_type": "text"
      },
      "source": [
        "### Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4sdVZl-kKI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import json\n",
        "import re\n",
        "\n",
        "import fasttext\n",
        "from gensim.models import FastText as fasttext_gensim\n",
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from science_science.sciosci.assets import keyword_assets as kw\n",
        "from science_science.sciosci.assets import generic_assets as sci\n",
        "from science_science.sciosci.assets import advanced_assets as aa\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu17lpjOg7F2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "draRfEpf8A5L",
        "colab_type": "code",
        "outputId": "bd54cd2a-3801-420b-f9f9-77e57fe8aee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "stops = ['a','an','we','result','however','yet','since','previously','although','propose','proposed','e_g','method',\n",
        "         'published_elsevier','b','v','problem','paper','approach','within','with','by','via','way','t','case','issue','level','area','system',\n",
        "         'work','discussed','seen','put','usually','take','make','author','versus','enables','result','research','design','based']\n",
        "punkts = [' ','','(',')','[',']','{','}','.',',','!','?','<','>','-','_',':',';','\\\\','/','|','&','%',\"'s\",\"`s\",'#','$','@']\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = list(set(stopwords.words(\"english\")))+stops+punkts\n",
        "np.random.seed(50)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLK4cDhVkKbs",
        "colab_type": "text"
      },
      "source": [
        "# Get embeddings from a pre-trained model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhKPivNafWBf",
        "colab_type": "text"
      },
      "source": [
        "### Load Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQEzlLSQ6g0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "period = '1990-2018'\n",
        "percentile = 97"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6XYOAVzkKsR",
        "colab_type": "text"
      },
      "source": [
        "#### Option A - Load corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkWEF6DBkKzd",
        "colab_type": "code",
        "outputId": "c44ff740-f959-4981-ab2b-6f79a2f2aea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "directory = 'drive/My Drive/Data/corpus/improved_copyr_lemmatized_stopwords_removed_thesaurus_n-grams/'\n",
        "file_name = period+' corpus abstract-title - with n-grams'\n",
        "corpus = pd.read_csv(directory+file_name,names=['abstracts'])\n",
        "corpus_tokens = [item for sublist in corpus['abstracts'].values.tolist() for item in sublist.split()]\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG1-O4D26irL",
        "colab_type": "text"
      },
      "source": [
        "#### Option B - Load keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7qJSz6l6mVt",
        "colab_type": "code",
        "outputId": "e65d72cb-8ee9-49ac-cab4-9ff5c0231ca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "directory = 'drive/My Drive/Data/LDA/'\n",
        "file_name = period+' top_90-percentile_keywords_terms.csv'\n",
        "corpus = pd.read_csv(directory+file_name)\n",
        "corpus = corpus.fillna('this_is_null')\n",
        "corpus_tokens = []\n",
        "for idx,row in tqdm(corpus.iterrows(),total=corpus.shape[0]):\n",
        "    for token in row.values.tolist():\n",
        "        if token != 'this_is_null': \n",
        "            corpus_tokens.append(token) \n",
        "del corpus\n",
        "print(\"\\nNumber of unique tokens:\",len(corpus_tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6546/6546 [00:00<00:00, 8418.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number of unique tokens: 52365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YXZPTZrKjhr",
        "colab_type": "text"
      },
      "source": [
        "#### Option C - Load author keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq2HreIeKqP0",
        "colab_type": "code",
        "outputId": "031b660b-100c-40b7-a563-b556d1df77d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "directory = 'drive/My Drive/Data/Author keywords - 29 Oct 2019/'\n",
        "file_name = period+' keyword frequency'\n",
        "corpus = pd.read_csv(directory+file_name,names=['keyword','frequency'])\n",
        "corpus = corpus.fillna('this_is_null')\n",
        "threshold = np.percentile(corpus['frequency'].values,percentile)\n",
        "corpus = corpus[corpus['frequency']>threshold]\n",
        "\n",
        "corpus_tokens = []\n",
        "for idx,row in tqdm(corpus.iterrows(),total=corpus.shape[0]):\n",
        "    if row['keyword'] != 'this_is_null': \n",
        "        corpus_tokens.append(row['keyword']) \n",
        "print(\"\\nNumber of unique tokens:\",len(corpus_tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1376/1376 [00:00<00:00, 9769.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number of unique tokens: 1376\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqlZHLo1e40r",
        "colab_type": "text"
      },
      "source": [
        "## Facebook Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeC8SkXPkKjx",
        "colab_type": "text"
      },
      "source": [
        "#### Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv3ACHBtexEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fb_model_address = 'drive/My Drive/Data-Permenant/FastText-crawl-300d-2M-subword/crawl-300d-2M-subword.bin'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6K74cHMkKnn",
        "colab_type": "code",
        "outputId": "640b9a1b-8888-4e67-b927-a1f803bcd589",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = fasttext.load_model(fb_model_address)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2648m7e8kk0n",
        "colab_type": "text"
      },
      "source": [
        "#### Get embeddings\n",
        "\n",
        "*   Save to dictionary and json\n",
        "*   This takes much less space on disk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mq87DiZxVvY",
        "colab_type": "text"
      },
      "source": [
        "##### No n-gram handle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiJcbZqHkkLl",
        "colab_type": "code",
        "outputId": "23f58666-ecbc-4c78-f2e8-84fa11b58948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Save in a dict\n",
        "output_dict = {}\n",
        "comment_embedding = ''\n",
        "for token in tqdm(corpus_tokens[:],total=len(corpus_tokens[:])):\n",
        "    output_dict[token] = str(model.get_word_vector(token))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 280/280 [00:00<00:00, 337.81it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOMUto5AxVDy",
        "colab_type": "text"
      },
      "source": [
        "##### Manual n-gram handle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4UJAF3exSLm",
        "colab_type": "code",
        "outputId": "65624796-56fb-468e-b923-3ff5468369f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Save in a dict\n",
        "comment_embedding = 'average_manual '\n",
        "output_dict = {}\n",
        "for token in tqdm(corpus_tokens[:],total=len(corpus_tokens[:])):\n",
        "    token_split = token.split(' ')\n",
        "    if len(token_split) > 0:\n",
        "        tmp_vector_grams = []\n",
        "        for item in token_split:\n",
        "            tmp_vector_grams.append(model.get_word_vector(item))\n",
        "        output_dict[token] = str(list(np.array(tmp_vector_grams).mean(axis=0)))\n",
        "    else:\n",
        "        output_dict[token] = str(model.get_word_vector(item))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 280/280 [00:00<00:00, 3792.22it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8nMUTU0xXo8",
        "colab_type": "text"
      },
      "source": [
        "##### Save to disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJk-lqjemSky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save embeddings to disk\n",
        "with open(directory+'vectors/100D/FastText vector '+comment_embedding+period+'.json', 'w') as json_file:\n",
        "    json.dump(output_dict, json_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXkwEnj4lnJi",
        "colab_type": "text"
      },
      "source": [
        "#### Get embeddings (alternative) : \n",
        "\n",
        "*   save to a list instead of a dicktionary and csv\n",
        "*   Will have many redundant words in it and will take lots of disk space\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GndJ3Q1OlnSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save in a list\n",
        "batches = 1000\n",
        "batch_size = len(corpus_tokens)/batches\n",
        "\n",
        "for step in tqdm(range(batches),total=batches):\n",
        "    batch_tokens = corpus_tokens[int(step*batch_size):int((step+1)*batch_size)]\n",
        "    corpus_vectors = [model.get_word_vector(x) for x in batch_tokens]\n",
        "    corpus_vectors = pd.DataFrame(corpus_vectors)\n",
        "    corpus_vectors['tokens'] = batch_tokens\n",
        "\n",
        "    # Save embeddings to disk\n",
        "    with open(directory+'vector '+period,'a') as f:\n",
        "        corpus_vectors.to_csv(f,index=False,header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Xcww-Mfdaj",
        "colab_type": "text"
      },
      "source": [
        "## Gensim Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPd9g4E4fgTi",
        "colab_type": "text"
      },
      "source": [
        "#### Load model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06depR4llpmB",
        "colab_type": "text"
      },
      "source": [
        "Load gensim model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_UYlrcCfgxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gensim_model_address = 'drive/My Drive/Data/FastText Models/50D/fasttext-scopus_wos-merged-310k_docs-gensim 50D.model'\n",
        "model = fasttext_gensim.load(gensim_model_address)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhKlmoX6gPE-",
        "colab_type": "text"
      },
      "source": [
        "Simple Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7oeEkj_gNLg",
        "colab_type": "code",
        "outputId": "fb6144c1-3338-42de-957c-ea95f3da6c28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "print('intelligence' in model.wv.vocab)\n",
        "print(model.similarity(\"anns\", \"ann\"))\n",
        "print(model.most_similar(positive=['baghdad', 'england'], negative=['london']))\n",
        "print(model.n_similarity(['neural network','deep learning'], ['ann']))\n",
        "print(model.wmdistance(['stop', 'word', 'removed', 'tokens', 'of', 'sentence 1'], ['stop word removed tokens of sentence 2']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "0.899301\n",
            "[('bagua', 0.7456873059272766), ('baselevel', 0.7439467310905457), ('bagel', 0.7432736754417419), ('category', 0.7406319975852966), ('nine-category', 0.7370117902755737), ('baget', 0.734655499458313), ('categorized-and', 0.7324073910713196), ('categorizer', 0.7313241958618164), ('category.9', 0.730506420135498), ('eight-category', 0.7295982837677002)]\n",
            "0.7487254\n",
            "21.418109944602243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCuEFwIMbND_",
        "colab_type": "code",
        "outputId": "038b21bb-96b1-45fc-d513-1939d1671df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# get distance of two words\n",
        "from scipy import spatial,sparse,sign\n",
        "vec_a = model.wv['']\n",
        "vec_b = model.wv['fpga']\n",
        "distance_tmp = spatial.distance.cosine(vec_a, vec_b)\n",
        "similarity_tmp = 1 - distance_tmp\n",
        "similarity_tmp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.33358505368232727"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPioY7Lj7BOP",
        "colab_type": "code",
        "outputId": "20aa1344-f165-4fd1-e7aa-40c57c995963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "model.wv['artificial intelligence']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-4.25525   ,  0.03749213,  1.8276842 , -3.1528432 , -3.3440664 ,\n",
              "        0.66207427,  1.0964861 , -2.038055  ,  3.0331683 , -2.1755521 ,\n",
              "        2.1063838 ,  1.6578307 ,  1.3311137 , -2.030598  , -0.69794494,\n",
              "        2.6208954 ,  1.9154872 ,  1.6715113 ,  0.23561044, -0.50721526,\n",
              "        3.1775064 , -2.069317  , -2.4310536 , -1.8514946 ,  1.3029549 ,\n",
              "        3.482592  , -2.1535952 ,  1.078043  , -3.8000522 ,  0.08382007,\n",
              "       -0.6016187 ,  3.3550935 ,  2.5037699 , -2.8812122 , -0.11693893,\n",
              "       -0.51311666,  3.1224    ,  0.46978405, -0.4427654 , -2.5400903 ,\n",
              "        2.0880878 ,  3.123557  ,  0.8703581 , -1.0431769 , -2.8512125 ,\n",
              "        2.2627175 ,  1.0080537 ,  0.1098367 ,  1.5881126 , -1.870272  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqUSjLRy_isH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(model.wv['artificial']+model.wv['intelligence'])/2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0gnOxud_m4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.wv['artificial_intelligence']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMAHWCgX6kEz",
        "colab_type": "text"
      },
      "source": [
        "### Get Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LPLO5z4cER3",
        "colab_type": "text"
      },
      "source": [
        "##### No n-gram handle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP8pYSjf6kiR",
        "colab_type": "code",
        "outputId": "3293d0e4-ff19-437d-def6-b56259717fbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Save in a dict\n",
        "comment_embedding = ''\n",
        "output_dict = {}\n",
        "for token in tqdm(corpus_tokens[:],total=len(corpus_tokens[:])):\n",
        "    output_dict[token] = str(model.wv[token])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3184174/3184174 [27:52<00:00, 1903.51it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGLF7jepcI83",
        "colab_type": "text"
      },
      "source": [
        "##### Manual n-gram handle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9cb-KpAcPWJ",
        "colab_type": "code",
        "outputId": "4f2e771b-0fb9-487f-c7db-b11e3da36156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Save in a dict\n",
        "comment_embedding = 'average_manual '\n",
        "output_dict = {}\n",
        "for token in tqdm(corpus_tokens[:],total=len(corpus_tokens[:])):\n",
        "    token_split = token.split(' ')\n",
        "    if len(token_split) > 0:\n",
        "        tmp_vector_grams = []\n",
        "        for item in token_split:\n",
        "            tmp_vector_grams.append(model.wv[item])\n",
        "        output_dict[token] = str(list(np.array(tmp_vector_grams).mean(axis=0)))\n",
        "    else:\n",
        "        output_dict[token] = str(model.wv[token])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1590/1590 [00:00<00:00, 10574.09it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk6CJa6pcQC-",
        "colab_type": "text"
      },
      "source": [
        "##### Save to disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4wRDXMX6tBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save embeddings to disk\n",
        "with open(directory+'/FastText vector with n-grams '+comment_embedding+period+'.json', 'w') as json_file:\n",
        "    json.dump(output_dict, json_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohPXzf8mKUxQ",
        "colab_type": "code",
        "outputId": "4588f6a5-d865-48bb-fb40-d90033bc007a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(directory)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/Data/corpus/improved_copyr_lemmatized_stopwords_removed_thesaurus_n-grams/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yYjO-JQBLE3",
        "colab_type": "text"
      },
      "source": [
        "### Get Doc Embeddings (SIF)\n",
        "\n",
        "It is not recommended to perform this on cloud, as it is not process intesive, yet takes too long depending on the doc-count. It might take over 30 hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp1fP9z4xUGX",
        "colab_type": "text"
      },
      "source": [
        "Make a probability dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buSIQtoWVnOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_tokens_s = pd.Series(corpus_tokens)\n",
        "corpus_tokens_probabilities = (corpus_tokens_s.groupby(corpus_tokens_s).transform('count') / len(corpus_tokens_s)).values\n",
        "corpus_tokens_probabilities = pd.DataFrame(corpus_tokens_probabilities)\n",
        "corpus_tokens_probabilities['tokens'] = corpus_tokens_s\n",
        "corpus_tokens_probabilities.columns = ['probability','token']\n",
        "corpus_tokens_probabilities = corpus_tokens_probabilities.groupby('token').mean()\n",
        "corpus_tokens_probabilities = corpus_tokens_probabilities.reset_index()\n",
        "corpus_tokens_probabilities.columns = ['token','probability']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu51jQbo9QE6",
        "colab_type": "text"
      },
      "source": [
        "Get vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72BAU9Jm9PwE",
        "colab_type": "code",
        "outputId": "1a4ef491-cceb-4fe7-9f10-18ffac71e652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vectors = []\n",
        "for token in tqdm(corpus_tokens_probabilities['token'],total=corpus_tokens_probabilities.shape[0]):\n",
        "    vectors.append(model.wv[token])\n",
        "corpus_tokens_probabilities['vector'] = vectors"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65533/65533 [00:03<00:00, 20827.02it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y9rI72IxZal",
        "colab_type": "text"
      },
      "source": [
        "Calculate weighted average vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WwJtQqABN8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = 1e-3\n",
        "embedding_size = 50\n",
        "\n",
        "doc_set = []\n",
        "for doc in tqdm(corpus['abstracts'].values.tolist(),total=len(corpus['abstracts'].values.tolist())):\n",
        "    vs = np.zeros(embedding_size)  # add all word2vec values into one vector for the sentence\n",
        "    doc_length = len(doc.split())\n",
        "    print(doc.split())\n",
        "    for word in doc.split():\n",
        "        a_value = a / (a + corpus_tokens_probabilities[corpus_tokens_probabilities['token']==word]['probability'].values.tolist()[0])  # smooth inverse frequency, SIF\n",
        "        vs = np.add(vs, np.multiply(a_value, corpus_tokens_probabilities[corpus_tokens_probabilities['token']==word]['vector'].values.tolist()[0]))  # vs += sif * word_vector\n",
        "\n",
        "    vs = np.divide(vs, doc_length)  # weighted average\n",
        "    doc_set.append(vs)  # add to our existing re-calculated set of sentences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L6EqReXxgPX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_m00tKKkKNG",
        "colab_type": "text"
      },
      "source": [
        "# Train on a large Scopus corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz7pfl9d6R9o",
        "colab_type": "text"
      },
      "source": [
        "#### Load Corpus Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuO0J8oxkKWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_corpus = pd.read_csv('drive/My Drive/Data/corpus/AI ALL/1900-2019 corpus sentences abstract-title')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYHEYxwpEgCi",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocess and prepare corpus for FastText training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkqrGJ3cEgnX",
        "colab_type": "code",
        "outputId": "0b5441c0-1785-4a1e-83e7-6b6c995d31b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentences = []\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "with open('drive/My Drive/Data/corpus/AI ALL/1900-2019 corpus sentences abstract-title further processed.csv', 'w') as f:\n",
        "    for index,row in tqdm(sentence_corpus.iterrows(),total=sentence_corpus.shape[0]):\n",
        "        sentence = row['sentence']\n",
        "        sentence = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",sentence)\n",
        "        sentence = word_tokenize(sentence)\n",
        "        sentence = [word for word in sentence if not word in punkts] \n",
        "        sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
        "        # sentences.append(sentence)\n",
        "        f.write(\"%s\\n\" % ' '.join(sentence))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 13307816/13307816 [1:54:08<00:00, 1943.10it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQHwYepMdKFc",
        "colab_type": "text"
      },
      "source": [
        "#### Save sentences to disk for future use -- Not needed anymore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJZ0UwdhdOlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame([' '.join(words) for words in sentences],columns=['sentences']).to_csv(\n",
        "    'drive/My Drive/Data/corpus/AI ALL/1900-2019 corpus sentences abstract-title further processed.csv',\n",
        "    header=True,index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j6H5hsfu8p0",
        "colab_type": "text"
      },
      "source": [
        "#### Load pre-processed sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLHxqzOPvCdV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9171ddac-0f5d-4dfe-8bc9-99e563071495"
      },
      "source": [
        "sentence_corpus = pd.read_csv('drive/My Drive/Data/corpus/AI ALL/1900-2019 corpus sentences abstract-title further processed.csv',delimiter=\";;;\")\n",
        "sentence_corpus.columns = [\"sentences\"]\n",
        "sentences = []\n",
        "sentence_corpus = sentence_corpus.fillna('')\n",
        "for index,row in tqdm(sentence_corpus.iterrows(),total=sentence_corpus.shape[0]):\n",
        "    sentences.append(row['sentences'].split(' '))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 13230009/13230009 [28:30<00:00, 7736.00it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA83SDYjwfrO",
        "colab_type": "code",
        "outputId": "62a9447a-3b75-40ef-bd4a-2910968b284f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "sentence_corpus.head(10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>circuit sequential circuit or vlsi chip realiz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the design of efficient hardware is a fundamen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>because of the large cost for the physical con...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>for these purpose data structure for boolean f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the corresponding state of the art data struct...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>efficient algorithm for the operation on obdds...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>a generalized data structure called graph-driv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>the new data structure allows for many importa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>efficient algorithm for the operation on graph...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>consider a finite graph g v e</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           sentences\n",
              "0  circuit sequential circuit or vlsi chip realiz...\n",
              "1  the design of efficient hardware is a fundamen...\n",
              "2  because of the large cost for the physical con...\n",
              "3  for these purpose data structure for boolean f...\n",
              "4  the corresponding state of the art data struct...\n",
              "5  efficient algorithm for the operation on obdds...\n",
              "6  a generalized data structure called graph-driv...\n",
              "7  the new data structure allows for many importa...\n",
              "8  efficient algorithm for the operation on graph...\n",
              "9                      consider a finite graph g v e"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-9A4hbD7xwV",
        "colab_type": "text"
      },
      "source": [
        "### Train Fasttext - Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYfn-ZQfhed1",
        "colab_type": "text"
      },
      "source": [
        "#### Load a model to continue training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bnu7_aMhu-A",
        "colab_type": "text"
      },
      "source": [
        "* If want to continue training, run this section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7eHrQg_g2ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load(gensim_model_address)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s07ymd4xhtWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.build_vocab(sentences, update=True)\n",
        "model.train(sentences, total_examples=len(sentences), epochs=model.epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9LWfxerh2DI",
        "colab_type": "text"
      },
      "source": [
        "* Otherwise run this section\n",
        "\n",
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsDmFFxp7yJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = fasttext_gensim(min_n=3, max_n=6, size=15, window=5, min_count=1, seed = 50)\n",
        "model.build_vocab(sentences=sentences)\n",
        "model.train(sentences=sentences, total_examples=len(sentences), epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqV_noKRcD4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fname = \"drive/My Drive/Data/Models/fasttext-scopus-2.2-million_docs-gensim 15D.model\"\n",
        "model.save(fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3vrtHdqHy-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = fasttext_gensim(min_n=3, max_n=6, size=50, window=5, min_count=1, seed = 50)\n",
        "model.build_vocab(sentences=sentences)\n",
        "model.train(sentences=sentences, total_examples=len(sentences), epochs=10)\n",
        "fname = \"drive/My Drive/Data/Models/fasttext-scopus-2.2-million_docs-gensim 50D.model\"\n",
        "model.save(fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g75Pt__ZH4RZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = fasttext_gensim(min_n=3, max_n=6, size=100, window=5, min_count=1, seed = 50)\n",
        "model.build_vocab(sentences=sentences)\n",
        "model.train(sentences=sentences, total_examples=len(sentences), epochs=10)\n",
        "fname = \"drive/My Drive/Data/Models/fasttext-scopus-2.2-million_docs-gensim 50D.model\"\n",
        "model.save(fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnTG0yUGiqxw",
        "colab_type": "text"
      },
      "source": [
        "#### Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVJoU_sLiw2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "similarities = model.wv.most_similar(positive=['logic','fuzzy','expert'],negative=['deep','neural','network','cnn','ann'])\n",
        "most_similar = similarities[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaKS52YSDcup",
        "colab_type": "code",
        "outputId": "c4257bf2-6134-4a85-df40-d7696a731c57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "most_similar"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('mam-rnn', 0.9763791561126709)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkokDJdHjEFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "not_matching = model.wv.doesnt_match(\"human computer interface tree\".split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fOp26djDYk7",
        "colab_type": "code",
        "outputId": "3fca5ff0-ac5a-464e-ddf8-4662385b6c82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "not_matching"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tree'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7RJl89QjMhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sim_score = model.wv.similarity('computer', 'human')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtlAXvqWDWo4",
        "colab_type": "code",
        "outputId": "639a0234-75f9-4fec-c4ee-09d607516050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sim_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7571839"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWLLBs8UBRwy",
        "colab_type": "code",
        "outputId": "836cc23e-4ba9-4dfd-dfaa-21f5dd09f25a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print(model.wv['artificial intelligence'])\n",
        "print(model.wv['artificial'])\n",
        "print(model.wv['intelligence'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.2783480e+00 -4.2018552e+00  7.1276689e-01  4.2023015e+00\n",
            " -5.0359420e-03  4.4385982e+00  6.2421050e+00 -8.9032326e+00\n",
            "  1.7556003e+00  1.3425230e+00  9.4295764e-01 -4.4485557e-01\n",
            " -5.8648558e+00  2.6428668e+00 -1.2076639e+00]\n",
            "[  3.7072854   -3.616749     1.3040072    0.234361    -2.753659\n",
            "   7.528801    14.293305   -14.688236     5.3885765    6.496681\n",
            "   1.9917868    2.855616    -0.05153261   7.8660994   -2.22459   ]\n",
            "[  0.28606984  -6.971052    -0.9232919   11.48035      0.2561571\n",
            "   4.084776     2.4220266   -8.616226     0.94255084  -2.2498865\n",
            "   1.7112938   -3.370861   -12.577294    -1.1608386   -0.04991044]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sSZfjEMUyMc",
        "colab_type": "text"
      },
      "source": [
        "### Train Fasttext - Facebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK4N7jUqU1CJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_joined = ' '.join(sentences)\n",
        "model = fasttext.train_unsupervised(sentences_joined, \"cbow\", minn=2, maxn=5, dim=50, epoch=10,lr=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLNquk0BaywP",
        "colab_type": "text"
      },
      "source": [
        "#### Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i6detxZVMUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o13SRSefVFDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.get_word_vector(\"the\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FvMFASYVNzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.get_nearest_neighbors('asparagus')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH9Wdf5WV5F8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.get_analogies(\"intelligence\", \"math\", \"fuzzy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaDnem0pWcNw",
        "colab_type": "text"
      },
      "source": [
        "#### Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wjWeyw-Wbpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_model(\"drive/My Drive/Data/fasttext-scopus_wos-merged-310k_docs-facebook.ftz\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}