{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fasttext_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "r_m00tKKkKNG"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahandv/science_science/blob/master/FastText_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B43KQKxsZqK2"
      },
      "source": [
        "# FASTTEXT EMBEDDING\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyakRMAwbUq2"
      },
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8klZfiWyEt98"
      },
      "source": [
        "Local OR Colab?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsEwsoJSEtKx"
      },
      "source": [
        "# datapath = '/mnt/16A4A9BCA4A99EAD/GoogleDrive/Data/' # Local\n",
        "datapath = 'drive/My Drive/Data/' # Remote"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIPuXPla6BKh"
      },
      "source": [
        "### Clone Project Git Repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-YQSyFe6Bgv",
        "outputId": "cfb840a0-83ff-4ea5-c074-d879ecc53732"
      },
      "source": [
        "!rm -rf 'science_science'\n",
        "username = \"sahandv\"#@param {type:\"string\"}\n",
        "# password = \"\"#@param {type:\"string\"} \n",
        "token = \"\"#@param {type:\"string\"}\n",
        "\n",
        "!git clone https://$username:$token@github.com/$username/science_science.git\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'science_science'...\n",
            "remote: Enumerating objects: 2198, done.\u001b[K\n",
            "remote: Counting objects: 100% (862/862), done.\u001b[K\n",
            "remote: Compressing objects: 100% (741/741), done.\u001b[K\n",
            "remote: Total 2198 (delta 554), reused 415 (delta 114), pack-reused 1336\u001b[K\n",
            "Receiving objects: 100% (2198/2198), 169.00 MiB | 32.23 MiB/s, done.\n",
            "Resolving deltas: 100% (1353/1353), done.\n",
            "Checking out files: 100% (218/218), done.\n",
            "sample_data  science_science\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8crXh0Ek1FA"
      },
      "source": [
        "### Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulliMrpjkKAd",
        "outputId": "6cbdd7b3-2dc3-4b89-9c04-2a6fccb4ec93"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhV7OflrnOhI"
      },
      "source": [
        "# Check files!\n",
        "!ls 'drive/My Drive/Data-Permenant/FastText-crawl-300d-2M-subword'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiBuMInGmypx"
      },
      "source": [
        "### Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OwQMHvQmy3A",
        "outputId": "568e4083-18ac-4ae4-96c1-18acf617618d"
      },
      "source": [
        "!pip install fasttext\n",
        "!pip install -r 'science_science/requirements.txt'\n",
        "!pip install gensim==3.8.1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 68 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.7.1-py2.py3-none-any.whl (200 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3094708 sha256=9eea29f2a8cec8fa18d47c384a4690c8c1278239b1fcf8a69b42c7a6c8dac528\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.7.1\n",
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow_gpu-2.6.0-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 458.3 MB 10 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 2)) (4.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 3)) (4.62.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 4)) (1.1.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 5)) (3.2.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 6)) (2.2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 7)) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 9)) (0.11.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 10)) (1.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 11)) (3.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 12)) (7.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 13)) (0.8.9)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 14)) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 15)) (2.6.3)\n",
            "Collecting netgraph\n",
            "  Downloading netgraph-4.0.5.tar.gz (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: yellowbrick in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 17)) (0.9.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 18)) (4.4.1)\n",
            "Collecting stellargraph\n",
            "  Downloading stellargraph-1.2.1-py3-none-any.whl (435 kB)\n",
            "\u001b[K     |████████████████████████████████| 435 kB 57.7 MB/s \n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 44.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 21)) (1.3.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from -r science_science/requirements.txt (line 22)) (0.10.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (2.6.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (3.17.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.40.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (2.6.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (5.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (2.6.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu->-r science_science/requirements.txt (line 1)) (0.37.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.5.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements.txt (line 2)) (2.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements.txt (line 2)) (5.2.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements.txt (line 2)) (0.1.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements.txt (line 2)) (0.16.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements.txt (line 2)) (21.2.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->-r science_science/requirements.txt (line 2)) (0.3.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r science_science/requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r science_science/requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r science_science/requirements.txt (line 6)) (0.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow-gpu->-r science_science/requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r science_science/requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r science_science/requirements.txt (line 11)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r science_science/requirements.txt (line 11)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r science_science/requirements.txt (line 11)) (1.3.2)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->-r science_science/requirements.txt (line 14)) (5.2.1)\n",
            "Collecting rectangle-packer\n",
            "  Downloading rectangle_packer-2.0.1-cp37-cp37m-manylinux2010_x86_64.whl (246 kB)\n",
            "\u001b[K     |████████████████████████████████| 246 kB 63.6 MB/s \n",
            "\u001b[?25hCollecting grandalf\n",
            "  Downloading grandalf-0.7-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 563 kB/s \n",
            "\u001b[?25hRequirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->-r science_science/requirements.txt (line 18)) (1.3.3)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from stellargraph->-r science_science/requirements.txt (line 19)) (2.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->-r science_science/requirements.txt (line 2)) (1.53.0)\n",
            "Building wheels for collected packages: netgraph\n",
            "  Building wheel for netgraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for netgraph: filename=netgraph-4.0.5-py3-none-any.whl size=72596 sha256=17274e052ef8e1d55b82ab6c61a3163a1dee4374692c84cebc2665a494049fd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/b1/fc/b83b257e7318cf4b8bde770dc0f5bdd2e8c79a2fcc27bd6f8a\n",
            "Successfully built netgraph\n",
            "Installing collected packages: rectangle-packer, grandalf, tokenizers, tensorflow-gpu, stellargraph, netgraph\n",
            "Successfully installed grandalf-0.7 netgraph-4.0.5 rectangle-packer-2.0.1 stellargraph-1.2.1 tensorflow-gpu-2.6.0 tokenizers-0.10.3\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 69 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (5.2.1)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9l5YuBnkKFB"
      },
      "source": [
        "### Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKZ3hKboHU8y",
        "outputId": "9588e133-a430-47ab-a17d-d995f6900933"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data  science_science\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4sdVZl-kKI3"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import json\n",
        "import re\n",
        "\n",
        "# import fasttext\n",
        "import gensim\n",
        "from gensim.models import FastText as fasttext_gensim\n",
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# from sciosci.assets import text_assets as kw\n",
        "# from sciosci.assets import generic_assets as sci\n",
        "# from sciosci.assets import advanced_assets as aa\n",
        "\n",
        "from science_science.sciosci.assets import text_assets as kw\n",
        "from science_science.sciosci.assets import generic_assets as sci\n",
        "from science_science.sciosci.assets import advanced_assets as aa"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu17lpjOg7F2"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "draRfEpf8A5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1aa867e-ed99-4252-a4a9-59b01b9ac317"
      },
      "source": [
        "stops = ['a','an','we','result','however','yet','since','previously','although','propose','proposed','e_g','method',\n",
        "         'published_elsevier','b','v','problem','paper','approach','within','with','by','via','way','t','case','issue','level','area','system',\n",
        "         'work','discussed','seen','put','usually','take','make','author','versus','enables','result','research','design','based']\n",
        "punkts = [' ','','(',')','[',']','{','}','.',',','!','?','<','>','-','_',':',';','\\\\','/','|','&','%',\"'s\",\"`s\",'#','$','@']\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = list(set(stopwords.words(\"english\")))+stops+punkts\n",
        "np.random.seed(50)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLK4cDhVkKbs"
      },
      "source": [
        "# Get embeddings from a pre-trained model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhKPivNafWBf"
      },
      "source": [
        "### Load Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQEzlLSQ6g0E"
      },
      "source": [
        "period = '2017-2018'\n",
        "percentile = 97"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6XYOAVzkKsR"
      },
      "source": [
        "#### Option A - Load corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkWEF6DBkKzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50a7e4a-f051-4525-b8a8-74a39313bac1"
      },
      "source": [
        "# directory = datapath+'Corpus/cora-classify/cora/clean/with citations/'\n",
        "# directory = datapath+'Corpus/Dimensions AI unlimited citations/clean/'\n",
        "directory = datapath+'Corpus/Dimensions All/clean/'\n",
        "\n",
        "# file_name = 'cora deflemm'#corpus abstract-title - with n-grams'\n",
        "file_name = 'data with abstract'#corpus abstract-title - with n-grams'\n",
        "corpus = pd.read_csv(directory+file_name)\n",
        "keywords = [item.lower().replace(';;;',' ') for sublist in corpus['DE-terms'].values.tolist() for item in sublist.split()]\n",
        "ids = corpus['id']\n",
        "gc.collect()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "610"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG1-O4D26irL"
      },
      "source": [
        "#### Option B - Load keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7qJSz6l6mVt"
      },
      "source": [
        "directory = datapath+'LDA/'\n",
        "file_name = period+' top_90-percentile_keywords_terms.csv'\n",
        "corpus = pd.read_csv(directory+file_name)\n",
        "corpus = corpus.fillna('this_is_null')\n",
        "corpus_tokens = []\n",
        "for idx,row in tqdm(corpus.iterrows(),total=corpus.shape[0]):\n",
        "    for token in row.values.tolist():\n",
        "        if token != 'this_is_null': \n",
        "            corpus_tokens.append(token) \n",
        "del corpus\n",
        "print(\"\\nNumber of unique tokens:\",len(corpus_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YXZPTZrKjhr"
      },
      "source": [
        "#### Option C - Load author keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq2HreIeKqP0"
      },
      "source": [
        "directory = datapath+'Author keywords - 29 Oct 2019/'\n",
        "file_name = period+' keyword frequency'\n",
        "corpus = pd.read_csv(directory+file_name,names=['keyword','frequency'])\n",
        "corpus = corpus.fillna('this_is_null')\n",
        "threshold = np.percentile(corpus['frequency'].values,percentile)\n",
        "corpus = corpus[corpus['frequency']>threshold]\n",
        "\n",
        "corpus_tokens = []\n",
        "for idx,row in tqdm(corpus.iterrows(),total=corpus.shape[0]):\n",
        "    if row['keyword'] != 'this_is_null': \n",
        "        corpus_tokens.append(row['keyword']) \n",
        "print(\"\\nNumber of unique tokens:\",len(corpus_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6UjbqehEOFA"
      },
      "source": [
        "#### Option D - Load author keywords and lemmatze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJvpKLFvENUj"
      },
      "source": [
        "file_name = 'Corpus/Taxonomy/CSO.3.3-taxonomy-US.csv'\n",
        "corpus = pd.read_csv(datapath+file_name)#,names=['keyword','frequency'])\n",
        "keywords = [kw.string_pre_processing(x,stemming_method='None',lemmatization='DEF',stop_word_removal=True,stop_words_extra=stops,verbose=False,download_nltk=False) for x in tqdm(corpus['keywords'].values.tolist())]\n",
        "keywords = [keyword for keyword in tqdm(keywords) if len(keyword)>2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqlZHLo1e40r"
      },
      "source": [
        "## Facebook Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeC8SkXPkKjx"
      },
      "source": [
        "#### Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjSQ5qVnS7CQ",
        "outputId": "f0a7666f-db80-4850-c2ab-507ad91a677c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz'\n",
        "# https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.vec.zip"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-22 02:25:50--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4503593528 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.en.300.bin.gz’\n",
            "\n",
            "cc.en.300.bin.gz    100%[===================>]   4.19G  10.8MB/s    in 6m 39s  \n",
            "\n",
            "2021-09-22 02:32:30 (10.8 MB/s) - ‘cc.en.300.bin.gz’ saved [4503593528/4503593528]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klvGhIVlTK37",
        "outputId": "924e62db-eb8a-4c8c-95b5-3182e88643d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!gunzip 'cc.en.300.bin.gz'\n",
        "!ls"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cc.en.300.bin  sample_data\twiki-news-300d-1M-subword.vec\n",
            "drive\t       science_science\twiki-news-300d-1M-subword.vec.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv3ACHBtexEI"
      },
      "source": [
        "fb_model_address = datapath+'/FastText-crawl-300d-2M-subword/crawl-300d-2M-subword.bin'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6K74cHMkKnn"
      },
      "source": [
        "model = fasttext.load_model(fb_model_address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2648m7e8kk0n"
      },
      "source": [
        "#### Get embeddings\n",
        "\n",
        "*   Save to dictionary and json\n",
        "*   This takes much less space on disk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mq87DiZxVvY"
      },
      "source": [
        "##### No n-gram handle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiJcbZqHkkLl"
      },
      "source": [
        "# Save in a dict\n",
        "output_dict = {}\n",
        "comment_embedding = ''\n",
        "for token in tqdm(corpus_tokens[:],total=len(corpus_tokens[:])):\n",
        "    output_dict[token] = str(model.get_word_vector(token))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOMUto5AxVDy"
      },
      "source": [
        "##### Manual n-gram handle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4UJAF3exSLm"
      },
      "source": [
        "# Save in a dict\n",
        "comment_embedding = 'average_manual '\n",
        "output_dict = {}\n",
        "for token in tqdm(corpus_tokens[:],total=len(corpus_tokens[:])):\n",
        "    token_split = token.split(' ')\n",
        "    if len(token_split) > 0:\n",
        "        tmp_vector_grams = []\n",
        "        for item in token_split:\n",
        "            tmp_vector_grams.append(model.get_word_vector(item))\n",
        "        output_dict[token] = str(list(np.array(tmp_vector_grams).mean(axis=0)))\n",
        "    else:\n",
        "        output_dict[token] = str(model.get_word_vector(item))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8nMUTU0xXo8"
      },
      "source": [
        "##### Save to disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJk-lqjemSky"
      },
      "source": [
        "# Save embeddings to disk\n",
        "with open(directory+'vectors/100D/FastText vector '+comment_embedding+period+'.json', 'w') as json_file:\n",
        "    json.dump(output_dict, json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXkwEnj4lnJi"
      },
      "source": [
        "#### Get embeddings (alternative) : \n",
        "\n",
        "*   save to a list instead of a dicktionary and csv\n",
        "*   Will have many redundant words in it and will take lots of disk space\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GndJ3Q1OlnSy"
      },
      "source": [
        "# Save in a list\n",
        "batches = 1000\n",
        "batch_size = len(corpus_tokens)/batches\n",
        "\n",
        "for step in tqdm(range(batches),total=batches):\n",
        "    batch_tokens = corpus_tokens[int(step*batch_size):int((step+1)*batch_size)]\n",
        "    corpus_vectors = [model.get_word_vector(x) for x in batch_tokens]\n",
        "    corpus_vectors = pd.DataFrame(corpus_vectors)\n",
        "    corpus_vectors['tokens'] = batch_tokens\n",
        "\n",
        "    # Save embeddings to disk\n",
        "    with open(directory+'vector '+period,'a') as f:\n",
        "        corpus_vectors.to_csv(f,index=False,header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Xcww-Mfdaj"
      },
      "source": [
        "## Gensim Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPd9g4E4fgTi"
      },
      "source": [
        "#### Load model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06depR4llpmB"
      },
      "source": [
        "Load gensim model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_UYlrcCfgxw"
      },
      "source": [
        "gensim_model_address = datapath+'Corpus/Dimensions All/models/Fasttext/FastText100D-dim-scopus.model'\n",
        "model_AI = fasttext_gensim.load(gensim_model_address)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx1x_twuQCbQ"
      },
      "source": [
        "gensim_model_address = datapath+\"Corpus/cora-classify/cora/models/fasttext cora corpus 128d5w.model\"\n",
        "model = fasttext_gensim.load(gensim_model_address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q18gRbrW3mvO"
      },
      "source": [
        "gensim_model_address = 'cc.en.300.bin'\n",
        "model = gensim.models.fasttext.load_facebook_model(gensim_model_address)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhKlmoX6gPE-"
      },
      "source": [
        "Simple Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7oeEkj_gNLg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "980c009f-f3ae-4a8f-d09d-60540aa49b89"
      },
      "source": [
        "print('intelligence' in model.wv.vocab)\n",
        "print(model_AI.similarity(\"anns\", \"ann\"))\n",
        "print(model.most_similar(['eye','vision','processing'], ['computer']))\n",
        "print(model.wmdistance(['stop', 'word', 'removed', 'tokens', 'of', 'sentence 1'], ['stop word removed tokens of sentence 2']))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "0.7752938\n",
            "[('vision.This', 0.5317180156707764), ('acuity', 0.5243748426437378), ('vison', 0.5201269388198853), ('vision.The', 0.5188624262809753), ('vision.It', 0.5166184902191162), ('vision.What', 0.5062639713287354), ('vision-', 0.5017036199569702), ('vision.A', 0.4983903169631958), ('binocularity', 0.4980439841747284), ('eye-', 0.4922565221786499)]\n",
            "1.2832011783002162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Gdwsz_glA7",
        "outputId": "38acf839-8862-4cba-fda0-90b62f255e32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(model.most_similar(['math','vector','statistic'], ['psychology']))\n",
        "print(model_AI.most_similar(['math','vector','statistic'], ['psychology']))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('dot-product', 0.5507299900054932), ('vectors', 0.5373679399490356), ('arithmetic', 0.506456732749939), ('arithmatic', 0.5047929883003235), ('scatterplot', 0.49137625098228455), ('calculation', 0.4910370111465454), ('log-scale', 0.48583289980888367), ('real-number', 0.4845273494720459), ('non-vector', 0.48128536343574524), ('calulator', 0.4809798002243042)]\n",
            "[('vectorizer', 0.727868914604187), ('vectorized', 0.6785148978233337), ('ldf', 0.6675050854682922), ('subvector', 0.6655511856079102), ('vectored', 0.6642667055130005), ('vectorial', 0.6612221002578735), ('vectorize', 0.6548691987991333), ('eigenvector', 0.6539524793624878), ('bitvector', 0.6513363122940063), ('rametrix', 0.6496775150299072)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCuEFwIMbND_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ebf03bf-916b-4244-9d7e-db85d47fae7f"
      },
      "source": [
        "# get distance of two words\n",
        "from scipy import spatial,sparse,sign\n",
        "vec_a = model.wv['artificial intelligence']\n",
        "vec_b = model.wv['learning']\n",
        "distance_tmp = spatial.distance.cosine(vec_a, vec_b)\n",
        "similarity_tmp = 1 - distance_tmp\n",
        "similarity_tmp"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.255439817905426"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqUSjLRy_isH"
      },
      "source": [
        "(model.wv['artificial']+model.wv['intelligence'])/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5516KiWbelyF"
      },
      "source": [
        "#### Compare vectors to ACM categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AxNRKUidrCk"
      },
      "source": [
        "AI_categories = [\n",
        "              'Artificial Intelligence Applications - Expert Systems',\n",
        "              'Automatic Programming',\n",
        "              'Deduction - Theorem Proving',\n",
        "              'Knowledge Representation Formalisms - Knowledge Representation Methods',\n",
        "              'Programming Languages - Software',\n",
        "              'Learning',\n",
        "              'Natural Language Processing',\n",
        "              'Problem Solving - Control Methods - Search',\n",
        "              'Robotics',\n",
        "              'Vision - Scene Understanding',\n",
        "              'Distributed Artificial Intelligence',\n",
        "              'ARTIFICIAL INTELLIGENCE'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QeWKP-k1qpy"
      },
      "source": [
        "categories = [\n",
        "              'Natural language processing - Information extraction - Machine translation - Discourse, dialogue  pragmatics - Natural language generation - Speech recognition - Lexical semantics - Phonology / morphology',\n",
        "              'Knowledge representation reasoning - Description logics - Semantic networks Nonmonotonic default reasoning  belief revision - Probabilistic reasoning - Vagueness fuzzy logic - Causal reasoning  diagnostics - Temporal reasoning - Cognitive robotics - Ontology engineering - Logic programming answer set programming - Spatial  physical reasoning - Reasoning about belief  knowledge',\n",
        "              'Planning  scheduling - Planning for deterministic actions - Planning under uncertainty - Multi-agent planning - Planning  abstraction  generalization - Robotic planning - Evolutionary robotics',\n",
        "              'Search methodologies - Heuristic function construction - Discrete space search - Continuous space search - Randomized search - Game tree search - Abstraction  micro-operators - Search with partial observations - ',\n",
        "              'Control methods - Robotic planning - Evolutionary robotics - Computational control theory - Motion path planning',\n",
        "              'Philosophical theoretical foundations artificial intelligence - Cognitive science - Theory mind',\n",
        "              'Distributed artificial intelligence - Multi agent systems - Intelligent agents - Mobile agents - Cooperation  coordination',\n",
        "              'Computer vision - Biometrics - Scene understanding - Activity recognition  understanding - Video summarization - Visual content based indexing  retrieval - Visual inspection - Vision for robotics - Scene anomaly detection - Image  video acquisition - Camera calibration - Epipolar geometry - Computational photography - Hyperspectral imaging - Motion capture - 3D imaging - Active vision - Image representations - Shape representations - Appearance  texture - Hierarchical representations - Computer vision problems - Interest point  salient region detections - Image segmentation  - Video segmentation - Shape inference - Object detection - Object recognition - Object identification - Tracking - Reconstruction - Matching',\n",
        "              \n",
        "              'Learning paradigms - Supervised learning - Ranking - Learning to rank - Supervised learning  classification - Supervised learning  regression - Structured outputs - Cost sensitive learning - Unsupervised learning - Cluster analysis - Anomaly detection - Mixture modeling - Topic modeling - Source separation - Motif discovery - Dimensionality reduction  manifold learning - Reinforcement learning - Sequential decision making - Inverse reinforcement learning - Apprenticeship learning - Multi-agent reinforcement learning - Adversarial learning - Multi-task learning - Transfer learning - Lifelong machine learning - Learning under covariate shift',\n",
        "              'Learning settings - Batch learning - Online learning settings - Learning from demonstrations - Learning from critiques - Learning from implicit feedback - Active learning settings - Semi supervised learning settings',\n",
        "              'Machine learning approaches - Classification  regression trees - Kernel methods - Support vector machines - Gaussian processes - Neural networks - Logical  relational learning - Inductive logic learning - Statistical relational learning - Learning in probabilistic graphical models - Maximum likelihood modeling - Maximum entropy modeling - Maximum a posteriori modeling - Mixture models - Latent variable models - Bayesian network models - Learning linear models - Perceptron algorithm - Factorization methods - Non-negative matrix factorization - Factor analysis - Principal component analysis - Canonical correlation analysis - Latent Dirichlet allocation - Rule learning - Instance-based learning - Markov decision processes -  Markov decision processes - Stochastic games - Learning latent representations - Deep belief networks - Bio inspired approaches - Artificial life - Evolvable hardware - Genetic algorithms - Genetic programming - Evolutionary robotics - Generative  developmental approaches',\n",
        "              'Machine learning algorithms - Dynamic programming Markov decision processes - Value iteration - Q learning - Policy iteration - Temporal difference learning - Approximate dynamic programming methods - Ensemble methods - Boosting - Bagging - Spectral methods - Feature selection - Regularization',\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rtaQ7KbfQ_y"
      },
      "source": [
        "AI_vectors = []\n",
        "labels = []\n",
        "for item in categories:\n",
        "    vector_tmp = []\n",
        "    label = item.split('-')[0]\n",
        "    for phrase in item.split('-'):\n",
        "        phrase = phrase.lower().strip()\n",
        "        # print(phrase)\n",
        "        vector_tmp.append(model.wv[phrase])\n",
        "    # print('---')\n",
        "    AI_vectors.append(list(np.array(vector_tmp).mean(axis=0)))\n",
        "    labels.append(label)\n",
        "print(AI_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brCodssfgEgt"
      },
      "source": [
        "pd.DataFrame(AI_vectors).to_csv(datapath+'FastText doc clusters - SIP/50D/classification/ACM_classifications_vectors')\n",
        "pd.DataFrame(labels,columns=['label']).to_csv(datapath+'FastText doc clusters - SIP/50D/classification/ACM_classifications_labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMAHWCgX6kEz"
      },
      "source": [
        "### Get Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LPLO5z4cER3"
      },
      "source": [
        "##### No n-gram handle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP8pYSjf6kiR"
      },
      "source": [
        "# Save in a dict\n",
        "comment_embedding = ''\n",
        "output_dict = {}\n",
        "for token in tqdm(corpus_tokens[:],total=len(corpus_tokens[:])):\n",
        "    output_dict[token] = str(model.wv[token])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGLF7jepcI83"
      },
      "source": [
        "##### Manual n-gram handle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8OC3uOovyM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb95059-eeda-4f49-a829-510b18d537fd"
      },
      "source": [
        "# Save in a dict\n",
        "comment_embedding = 'average_manual '\n",
        "sentence_corpus = pd.read_csv(datapath+\"Corpus/Taxonomy/AI kw mergedAI kw merged preprocessed for similarity measure\")\n",
        "corpus_tokens = sentence_corpus[pd.notna(sentence_corpus['sentence'])]['sentence'].values.tolist()\n",
        "output_dict = {}\n",
        "for token in tqdm(corpus_tokens,total=len(corpus_tokens)):\n",
        "    token_split = token.split(' ')\n",
        "    if len(token_split) > 0:\n",
        "        tmp_vector_grams = []\n",
        "        for item in token_split:\n",
        "            tmp_vector_grams.append(model.wv[item])\n",
        "        output_dict[token] = str(list(np.array(tmp_vector_grams).mean(axis=0)))\n",
        "    else:\n",
        "        output_dict[token] = str(model.wv[token])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971871/971871 [01:50<00:00, 8789.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq52ru_cKBuK"
      },
      "source": [
        "##### Manual n-gram handle for keywords - for Option D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8kzOvxDJ_Fr"
      },
      "source": [
        "vectors = []\n",
        "for keyword in tqdm(keywords,total=len(keywords)):\n",
        "    tokens = keyword.split(' ')\n",
        "    keyword_vec = []\n",
        "    if len(tokens)>1:\n",
        "        for token in tokens:\n",
        "            keyword_vec.append(model_AI.wv['neural'])\n",
        "        keyword_vec = np.array(keyword_vec).mean(axis=0)\n",
        "    else:\n",
        "        keyword_vec = model_AI.wv['neural']\n",
        "    vectors.append(keyword_vec)\n",
        "vectors = pd.DataFrame(vectors)\n",
        "vectors['keywords'] = keywords\n",
        "vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk6CJa6pcQC-"
      },
      "source": [
        "##### Save to disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pMtN1sRSCPl"
      },
      "source": [
        "vectors.to_csv(datapath+\"Corpus/Taxonomy/CSO.3.3-taxonomy-US-vectors.csv\",index=False)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4wRDXMX6tBH"
      },
      "source": [
        "# Save embeddings to disk\n",
        "with open(datapath+\"Corpus/Taxonomy/AI kw mergedAI kw merged preprocessed for similarity measure embedding.json\", 'w') as json_file:\n",
        "    json.dump(output_dict, json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohPXzf8mKUxQ",
        "outputId": "978f7b84-38da-4b31-e3e6-0c6636a2147c"
      },
      "source": [
        "print(directory)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive/My Drive/Data/Corpus/Dimensions AI unlimited citations/clean/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yYjO-JQBLE3"
      },
      "source": [
        "### Get Doc Embeddings (SIF)\n",
        "\n",
        "It is not recommended to perform this on cloud, as it is not process intesive, yet takes too long depending on the doc-count. It might take over 30 hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp1fP9z4xUGX"
      },
      "source": [
        "Make a probability dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buSIQtoWVnOz"
      },
      "source": [
        "corpus_tokens_s = pd.Series(corpus_tokens)\n",
        "corpus_tokens_probabilities = (corpus_tokens_s.groupby(corpus_tokens_s).transform('count') / len(corpus_tokens_s)).values\n",
        "corpus_tokens_probabilities = pd.DataFrame(corpus_tokens_probabilities)\n",
        "corpus_tokens_probabilities['tokens'] = corpus_tokens_s\n",
        "corpus_tokens_probabilities.columns = ['probability','token']\n",
        "corpus_tokens_probabilities = corpus_tokens_probabilities.groupby('token').mean()\n",
        "corpus_tokens_probabilities = corpus_tokens_probabilities.reset_index()\n",
        "corpus_tokens_probabilities.columns = ['token','probability']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu51jQbo9QE6"
      },
      "source": [
        "Get vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72BAU9Jm9PwE",
        "outputId": "427361b5-2231-40ab-b819-c11aa6d61144"
      },
      "source": [
        "vectors = []\n",
        "for token in tqdm(corpus_tokens_probabilities['token'],total=corpus_tokens_probabilities.shape[0]):\n",
        "    vectors.append(model.wv[token])\n",
        "corpus_tokens_probabilities['vector'] = vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 41443/41443 [00:01<00:00, 29117.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y9rI72IxZal"
      },
      "source": [
        "Calculate weighted average vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WwJtQqABN8w",
        "outputId": "1e3eab7e-f796-4978-df21-eedf6e9f4cfd"
      },
      "source": [
        "a = 1e-3\n",
        "embedding_size = 128\n",
        "\n",
        "doc_set = []\n",
        "for doc in tqdm(corpus['abstracts'].values.tolist(),total=len(corpus['abstracts'].values.tolist())):\n",
        "    vs = np.zeros(embedding_size)  # add all word2vec values into one vector for the sentence\n",
        "    doc_length = len(doc.split())\n",
        "#     print(doc.split())\n",
        "    for word in doc.lower().split():\n",
        "        a_value = a / (a + corpus_tokens_probabilities[corpus_tokens_probabilities['token']==word]['probability'].values.tolist()[0])  # smooth inverse frequency, SIF\n",
        "        vs = np.add(vs, np.multiply(a_value, corpus_tokens_probabilities[corpus_tokens_probabilities['token']==word]['vector'].values.tolist()[0]))  # vs += sif * word_vector\n",
        "\n",
        "    vs = np.divide(vs, doc_length)  # weighted average\n",
        "    doc_set.append(vs)  # add to our existing re-calculated set of sentences\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 23144/23144 [2:24:09<00:00,  2.68it/s]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3p67dBhcyDE"
      },
      "source": [
        "pd.DataFrame(doc_set).to_csv(datapath+'Corpus/cora-classify/cora/embeddings/FastText SIF cora corpus',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3C8IQiLgBJJ"
      },
      "source": [
        "##### Wikipedia embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKLjavgtgBeJ"
      },
      "source": [
        "import os\n",
        "import time\n",
        "from stat import S_ISREG, ST_CTIME, ST_MODE\n",
        "\n",
        "print(\"\\nSearching for Wiki texts...\\n\")\n",
        "dir_path = datapath+'Corpus/AI Wiki Classifications/applications/clean/'\n",
        "data = (os.path.join(dir_path, fn) for fn in os.listdir(dir_path))\n",
        "data = ((os.stat(path), path) for path in data)\n",
        "data = ((stat[ST_CTIME], path) for stat, path in data if S_ISREG(stat[ST_MODE]))\n",
        "\n",
        "names = []\n",
        "files = []\n",
        "for cdate, path in sorted(data):\n",
        "    print('   - ', time.ctime(cdate), os.path.basename(path),int(os.path.getsize(path)/1000000),'MB')\n",
        "    files.append(path)\n",
        "    names.append(os.path.basename(path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFMUH3SfgF2h"
      },
      "source": [
        "embedding_size = 50\n",
        "all = []\n",
        "for file_index,file in enumerate(files):\n",
        "    print('\\n',file,'\\n')\n",
        "    corpus = pd.read_csv(file)\n",
        "    corpus_tokens = [item.lower() for sublist in corpus['sentence'].values.tolist() if pd.notnull(sublist) for item in sublist.split()]\n",
        "    gc.collect()\n",
        "    \n",
        "    corpus_tokens_s = pd.Series(corpus_tokens)\n",
        "    corpus_tokens_probabilities = (corpus_tokens_s.groupby(corpus_tokens_s).transform('count') / len(corpus_tokens_s)).values\n",
        "    corpus_tokens_probabilities = pd.DataFrame(corpus_tokens_probabilities)\n",
        "    corpus_tokens_probabilities['tokens'] = corpus_tokens_s\n",
        "    corpus_tokens_probabilities.columns = ['probability','token']\n",
        "    corpus_tokens_probabilities = corpus_tokens_probabilities.groupby('token').mean()\n",
        "    corpus_tokens_probabilities = corpus_tokens_probabilities.reset_index()\n",
        "    corpus_tokens_probabilities.columns = ['token','probability']\n",
        "\n",
        "    vectors = []\n",
        "    for token in tqdm(corpus_tokens_probabilities['token'],total=corpus_tokens_probabilities.shape[0]):\n",
        "        vectors.append(model.wv[token])\n",
        "    corpus_tokens_probabilities['vector'] = vectors\n",
        "\n",
        "    a = 1e-3\n",
        "\n",
        "    doc_set = []\n",
        "    for doc in tqdm(corpus['sentence'].values.tolist(),total=len(corpus['sentence'].values.tolist())):\n",
        "        vs = np.zeros(embedding_size)  # add all word2vec values into one vector for the sentence\n",
        "        if pd.notnull(doc):\n",
        "            doc_length = len(doc.split())\n",
        "        #     print(doc.split())\n",
        "            for word in doc.lower().split():\n",
        "                a_value = a / (a + corpus_tokens_probabilities[corpus_tokens_probabilities['token']==word]['probability'].values.tolist()[0])  # smooth inverse frequency, SIF\n",
        "                vs = np.add(vs, np.multiply(a_value, corpus_tokens_probabilities[corpus_tokens_probabilities['token']==word]['vector'].values.tolist()[0]))  # vs += sif * word_vector\n",
        "\n",
        "            vs = np.divide(vs, doc_length)  # weighted average\n",
        "            doc_set.append(vs)  # add to our existing re-calculated set of sentences\n",
        "\n",
        "    pd.DataFrame(doc_set).to_csv(datapath+'Corpus/AI Wiki Classifications/applications/clean/vectors/'+names[file_index],index=False)\n",
        "\n",
        "    all.append(pd.DataFrame(doc_set).mean(axis=0))\n",
        "\n",
        "all_df = pd.DataFrame(all)\n",
        "all_df['clusters'] = names\n",
        "all_df.to_csv(datapath+'Corpus/AI Wiki Classifications/applications/clean/vectors/all',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBNrZ_OccyDF"
      },
      "source": [
        "### Get Doc Embedding (averaging)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6jh2g6xcyDF",
        "outputId": "7dccb7cc-47eb-4a3b-89e4-6487292e3802"
      },
      "source": [
        "doc_vectors = []\n",
        "for doc in tqdm(corpus['abstracts']):\n",
        "    tokens = doc.split()\n",
        "    doc_vectors.append(np.array([model.wv[token] for token in tokens]).mean(axis=0))\n",
        "pd.DataFrame(doc_vectors).to_csv(datapath+'Corpus/KPRIS/embeddings/FastText Avg large corpus',index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19710/19710 [00:09<00:00, 2073.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_m00tKKkKNG"
      },
      "source": [
        "# Train on a large Scopus corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz7pfl9d6R9o"
      },
      "source": [
        "#### Load Corpus Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuO0J8oxkKWz"
      },
      "source": [
        "sentence_corpus = pd.read_csv(datapath+'Corpus/patent_wos_training_very_large')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYHEYxwpEgCi"
      },
      "source": [
        "#### Preprocess and prepare corpus for FastText training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkqrGJ3cEgnX"
      },
      "source": [
        "sentences = []\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "with open(datapath+'corpus/AI ALL/1900-2019 corpus sentences abstract-title further processed.csv', 'w') as f:\n",
        "    for index,row in tqdm(sentence_corpus.iterrows(),total=sentence_corpus.shape[0]):\n",
        "        sentence = row['sentence']\n",
        "        sentence = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",sentence)\n",
        "        sentence = word_tokenize(sentence)\n",
        "        sentence = [word for word in sentence if not word in punkts] \n",
        "        sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
        "        # sentences.append(sentence)\n",
        "        f.write(\"%s\\n\" % ' '.join(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQHwYepMdKFc"
      },
      "source": [
        "#### Save sentences to disk for future use -- Not needed anymore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJZ0UwdhdOlp"
      },
      "source": [
        "pd.DataFrame([' '.join(words) for words in sentences],columns=['sentences']).to_csv(\n",
        "    datapath+'corpus/AI ALL/1900-2019 corpus sentences abstract-title further processed.csv',\n",
        "    header=True,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j6H5hsfu8p0"
      },
      "source": [
        "#### Load pre-processed sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh9_K4LmkvsH"
      },
      "source": [
        "sentence_corpus = pd.read_csv(datapath+'Corpus/Dimensions AI unlimited citations/clean/corpus sentences abstract-title')\n",
        "sentence_corpus_scopus = pd.read_csv(datapath+'Corpus/Scopus new/clean/corpus sentences abstract-title')\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki6Wngdvkxyl"
      },
      "source": [
        "sentence_corpus = sentence_corpus.append(sentence_corpus_scopus)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLHxqzOPvCdV",
        "outputId": "8d34fa8a-93f7-43db-e35a-c3b5e93dda11"
      },
      "source": [
        "sentence_corpus[['sentence']]\n",
        "# sentence_corpus.columns = [\"sentence\"]\n",
        "sentences = []\n",
        "sentence_corpus = sentence_corpus.fillna('')\n",
        "for index,row in tqdm(sentence_corpus.iterrows(),total=sentence_corpus.shape[0]):\n",
        "    sentences.append(row['sentence'].split(' '))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6413332/6413332 [09:28<00:00, 11289.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "UA83SDYjwfrO",
        "outputId": "39b39cbc-af64-41af-9d0f-7b7927dcce63"
      },
      "source": [
        "sentence_corpus.head(10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>computer simulation thinking</td>\n",
              "      <td>1960.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>nub simulation problem involves use similar ty...</td>\n",
              "      <td>1960.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>determination elastic modulus tin single cryst...</td>\n",
              "      <td>1960.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>apparatus described measurement frequency reso...</td>\n",
              "      <td>1960.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>value obtained used determine elastic modulus ...</td>\n",
              "      <td>1960.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>equation relating elastic modulus frequency re...</td>\n",
              "      <td>1960.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>value elastic modulus c given unit cm dyn</td>\n",
              "      <td>1960.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "      <td>diagnostic process men automaton</td>\n",
              "      <td>1961.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>diagnostic skill involves recognizing symptom ...</td>\n",
              "      <td>1961.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2</td>\n",
              "      <td>talent involved diagnosis characteristic searc...</td>\n",
              "      <td>1961.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   article_index                                           sentence    year\n",
              "0              0                       computer simulation thinking  1960.0\n",
              "1              0  nub simulation problem involves use similar ty...  1960.0\n",
              "2              1  determination elastic modulus tin single cryst...  1960.0\n",
              "3              1  apparatus described measurement frequency reso...  1960.0\n",
              "4              1  value obtained used determine elastic modulus ...  1960.0\n",
              "5              1  equation relating elastic modulus frequency re...  1960.0\n",
              "6              1          value elastic modulus c given unit cm dyn  1960.0\n",
              "7              2                   diagnostic process men automaton  1961.0\n",
              "8              2  diagnostic skill involves recognizing symptom ...  1961.0\n",
              "9              2  talent involved diagnosis characteristic searc...  1961.0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoJhFXrrcyDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1a7bbcc-f666-4800-b234-cf5707cd8a9f"
      },
      "source": [
        "sentences = [[t for t in s if len(t)>1] for s in sentences if len(s)>1]\n",
        "sentences[:5]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['computer', 'simulation', 'thinking'],\n",
              " ['nub',\n",
              "  'simulation',\n",
              "  'problem',\n",
              "  'involves',\n",
              "  'use',\n",
              "  'similar',\n",
              "  'type',\n",
              "  'program',\n",
              "  'instruction',\n",
              "  'machine',\n",
              "  'order',\n",
              "  'reproduce',\n",
              "  'step',\n",
              "  'individual',\n",
              "  'go',\n",
              "  'thinking',\n",
              "  'solution',\n",
              "  'difficult',\n",
              "  'problem',\n",
              "  'simulation',\n",
              "  'method',\n",
              "  'tremendous',\n",
              "  'role',\n",
              "  'sharpening',\n",
              "  'formulation',\n",
              "  'concerning',\n",
              "  'mental',\n",
              "  'process',\n",
              "  'phenomenon',\n",
              "  'simulation',\n",
              "  'human',\n",
              "  'response',\n",
              "  'overwhelming',\n",
              "  'advantage',\n",
              "  'understanding',\n",
              "  'behavioral',\n",
              "  'phenomenon',\n",
              "  'similar',\n",
              "  'method',\n",
              "  'science',\n",
              "  'research',\n",
              "  'simulation',\n",
              "  'complex',\n",
              "  'psychological',\n",
              "  'process',\n",
              "  'yielding',\n",
              "  'result',\n",
              "  'increasing',\n",
              "  'importance',\n",
              "  'psyc',\n",
              "  'abstract',\n",
              "  'cm'],\n",
              " ['determination',\n",
              "  'elastic',\n",
              "  'modulus',\n",
              "  'tin',\n",
              "  'single',\n",
              "  'crystal',\n",
              "  'variation',\n",
              "  'temperature'],\n",
              " ['apparatus',\n",
              "  'described',\n",
              "  'measurement',\n",
              "  'frequency',\n",
              "  'resonance',\n",
              "  'longitudinal',\n",
              "  'torsional',\n",
              "  'vibration',\n",
              "  'circular',\n",
              "  'cylindrical',\n",
              "  'rod',\n",
              "  'temperature',\n",
              "  'range'],\n",
              " ['value',\n",
              "  'obtained',\n",
              "  'used',\n",
              "  'determine',\n",
              "  'elastic',\n",
              "  'modulus',\n",
              "  'single',\n",
              "  'crystal',\n",
              "  'tin',\n",
              "  'range',\n",
              "  'temperature']]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-9A4hbD7xwV"
      },
      "source": [
        "### Train Fasttext - Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYfn-ZQfhed1"
      },
      "source": [
        "#### Load a model to continue training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bnu7_aMhu-A"
      },
      "source": [
        "* If want to continue training, run this section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7eHrQg_g2ej"
      },
      "source": [
        "model = load(gensim_model_address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s07ymd4xhtWh"
      },
      "source": [
        "model.build_vocab(sentences, update=True)\n",
        "model.train(sentences, total_examples=len(sentences), epochs=model.epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9LWfxerh2DI"
      },
      "source": [
        "* Otherwise run this section\n",
        "\n",
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsDmFFxp7yJa"
      },
      "source": [
        "model = fasttext_gensim(min_n=3, max_n=6, size=15, window=5, min_count=1, seed = 50)\n",
        "model.build_vocab(sentences=sentences)\n",
        "model.train(sentences=sentences, total_examples=len(sentences), epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqV_noKRcD4b"
      },
      "source": [
        "fname = \"datapath+Models/fasttext-scopus-2.2-million_docs-gensim 15D.model\"\n",
        "model.save(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3vrtHdqHy-K"
      },
      "source": [
        "model = fasttext_gensim(min_n=3, max_n=6, size=50, window=5, min_count=1, seed = 50)\n",
        "model.build_vocab(sentences=sentences)\n",
        "model.train(sentences=sentences, total_examples=len(sentences), epochs=10)\n",
        "fname = datapath+\"Models/fasttext-scopus-2.2-million_docs-gensim 50D.model\"\n",
        "model.save(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g75Pt__ZH4RZ"
      },
      "source": [
        "model = fasttext_gensim(min_n=3, max_n=12, size=100, window=5, min_count=5, seed = 50)\n",
        "model.build_vocab(sentences=sentences)\n",
        "model.train(sentences=sentences, total_examples=len(sentences), epochs=10)#,threads=15)\n",
        "fname = datapath+\"Corpus/Dimensions AI unlimited citations/FastText100D-dim-scopus.model\"\n",
        "model.save(fname)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnTG0yUGiqxw"
      },
      "source": [
        "#### Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVJoU_sLiw2E"
      },
      "source": [
        "similarities = model.wv.most_similar(positive=['logic','fuzzy','expert'],negative=['deep','neural','network','cnn','ann'])\n",
        "most_similar = similarities[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaKS52YSDcup"
      },
      "source": [
        "most_similar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkokDJdHjEFz"
      },
      "source": [
        "not_matching = model.wv.doesnt_match(\"human computer interface tree\".split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fOp26djDYk7"
      },
      "source": [
        "not_matching"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7RJl89QjMhl"
      },
      "source": [
        "sim_score = model.wv.similarity('computer', 'human')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtlAXvqWDWo4"
      },
      "source": [
        "sim_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWLLBs8UBRwy"
      },
      "source": [
        "print(model.wv['artificial intelligence'])\n",
        "print(model.wv['artificial'])\n",
        "print(model.wv['intelligence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sSZfjEMUyMc"
      },
      "source": [
        "### Train Fasttext - Facebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK4N7jUqU1CJ"
      },
      "source": [
        "sentences_joined = ' '.join(sentences)\n",
        "model = fasttext.train_unsupervised(sentences_joined, \"cbow\", minn=2, maxn=5, dim=50, epoch=10,lr=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLNquk0BaywP"
      },
      "source": [
        "#### Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i6detxZVMUQ"
      },
      "source": [
        "model.words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o13SRSefVFDD"
      },
      "source": [
        "model.get_word_vector(\"the\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FvMFASYVNzs"
      },
      "source": [
        "model.get_nearest_neighbors('asparagus')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH9Wdf5WV5F8"
      },
      "source": [
        "model.get_analogies(\"intelligence\", \"math\", \"fuzzy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaDnem0pWcNw"
      },
      "source": [
        "#### Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wjWeyw-Wbpi"
      },
      "source": [
        "model.save_model(datapath+\"fasttext-scopus_wos-merged-310k_docs-facebook.ftz\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}